{"id":"af76e194-4424-444b-8d94-5b07b015187d","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'court_cases' table from the CSV file located at C:/legal_data/court_cases.csv.\", \"Load the 'lawyers' table from the Excel file located at C:/legal_data/lawyers.xlsx.\", \"Filter the 'court_cases' table to include only cases from the year 2022.\", \"Join the filtered 'court_cases' table with the 'lawyers' table on the 'lawyer_id' column.\", \"Aggregate the joined table to find the total number of cases handled by each lawyer.\", \"Sort the aggregated data by the total number of cases in descending order.\", \"Calculate a new column 'case_success_rate' as the ratio of 'successful_cases' to 'total_cases'.\", \"Filter the data to include only lawyers with a 'case_success_rate' greater than 0.75.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"court_cases\", \"location\": \"C:/legal_data/court_cases.csv\", \"sheet_name\": null, \"label\": \"Court Cases Data\", \"description\": \"This table contains information about court cases, including case ID, lawyer ID, case status, and year.\", \"columns\": [{\"column\": \"case_id\", \"column_name\": \"CASE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each court case.\"}, {\"column\": \"lawyer_id\", \"column_name\": \"LAWYER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}, {\"column\": \"case_status\", \"column_name\": \"CASE_STATUS\", \"column_type\": \"xsd:string\", \"column_description\": \"Status of the case, e.g., 'successful', 'unsuccessful'.\"}, {\"column\": \"year\", \"column_name\": \"YEAR\", \"column_type\": \"xsd:integer\", \"column_description\": \"Year the case was handled.\"}]}, \"table_id2\": {\"name\": \"lawyers\", \"location\": \"C:/legal_data/lawyers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Lawyers Information\", \"description\": \"This table contains information about lawyers, including lawyer ID and name.\", \"columns\": [{\"column\": \"lawyer_id\", \"column_name\": \"LAWYER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"lawyer_name\", \"column_name\": \"LAWYER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"legal_firms\", \"location\": \"C:/legal_data/legal_firms.xlsx\", \"sheet_name\": \"Firms\", \"label\": \"Legal Firms Data\", \"description\": \"This table contains information about legal firms, including firm ID and name.\", \"columns\": [{\"column\": \"firm_id\", \"column_name\": \"FIRM_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each legal firm.\"}, {\"column\": \"firm_name\", \"column_name\": \"FIRM_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the legal firm.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/court_cases.csv\", \"type\": \"csv\"}, \"output\": \"court_cases\", \"comment\": \"Loading the court cases data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/lawyers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"lawyers\", \"comment\": \"Loading the lawyers data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"court_cases\", \"query\": \"YEAR == 2022\"}, \"output\": \"court_cases_2022\", \"comment\": \"Filtering court cases to include only those from the year 2022.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"court_cases_2022\", \"lawyers\"], \"joinOn\": [\"LAWYER_ID\"], \"joinType\": \"inner\"}, \"output\": \"cases_with_lawyers\", \"comment\": \"Joining filtered court cases with lawyers data on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"cases_with_lawyers\", \"groupBy\": \"LAWYER_ID\", \"aggregations\": [{\"column\": \"CASE_ID\", \"name\": \"total_cases\", \"function\": \"count\"}]}, \"output\": \"lawyer_case_counts\", \"comment\": \"Aggregating data to find the total number of cases handled by each lawyer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"lawyer_case_counts\", \"sortBy\": \"total_cases\", \"order\": \"desc\"}, \"output\": \"sorted_lawyer_case_counts\", \"comment\": \"Sorting the aggregated data by total number of cases in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_lawyer_case_counts\", \"columnName\": \"case_success_rate\", \"formula\": \"successful_cases / total_cases\"}, \"output\": \"lawyer_case_success_rate\", \"comment\": \"Calculating a new column 'case_success_rate' as the ratio of 'successful_cases' to 'total_cases'.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"lawyer_case_success_rate\", \"query\": \"case_success_rate > 0.75\"}, \"output\": \"high_success_lawyers\", \"comment\": \"Filtering the data to include only lawyers with a 'case_success_rate' greater than 0.75.\"}]"}]}
{"id":"e230a8a7-6fae-41ec-a190-7764bd669afe","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Filter the sales data to include only transactions where the total revenue is greater than 1000.\", \"Join the filtered sales data with the customer data on the customer_id column.\", \"Aggregate the joined data by region to calculate the total sales revenue for each region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_2023\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data 2023\", \"description\": \"This table contains sales transactions for the year 2023, including details such as transaction ID, customer ID, product ID, and total revenue.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"CustomerInfo\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID, name, and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_data\", \"location\": \"C:/data/products.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"This table contains product information including product ID, name, and category.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_2023\", \"comment\": \"Loading the sales data for 2023 from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_2023\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data based on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to compute total revenue.\"}]"}]}
{"id":"78cbd19c-04ff-4a91-aa3f-8d9e27e95a2a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/financial_transactions.csv'.\", \"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\", \"Load the 'exchange_rates' table from the Excel file located at 'C:/data/exchange_rates.xlsx'.\", \"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column using an inner join.\", \"Calculate a new column 'amount_in_usd' in the joined table by multiplying 'amount' with 'exchange_rate'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transactions with details such as transaction ID, amount, currency code, and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code of the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the transaction.\"}]}, \"table_id2\": {\"name\": \"exchange_rates\", \"location\": \"C:/data/exchange_rates.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Exchange Rates\", \"description\": \"A table containing exchange rates for various currencies against USD.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code.\"}, {\"column\": \"col_2\", \"column_name\": \"exchange_rate\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The exchange rate of the currency against USD.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"A table with data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/exchange_rates.xlsx\", \"type\": \"xlsx\"}, \"output\": \"exchange_rates\", \"comment\": \"Loading the exchange rates dataset from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"exchange_rates\"], \"joinOn\": [\"currency_code\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with exchange rates on currency code using an inner join.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"amount_in_usd\", \"formula\": \"amount * exchange_rate\"}, \"output\": \"transactions_in_usd\", \"comment\": \"Calculating the amount in USD by multiplying the transaction amount with the exchange rate.\"}]"}]}
{"id":"adb3e5ac-5fd2-4188-bb40-47b0ae4658bf","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee records table from the HR database.\", \"Filter the employee records to include only those with a salary above $50,000.\", \"Join the filtered employee records with the department details table on the department ID.\", \"Aggregate the joined data to calculate the average salary per department.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_records\", \"location\": \"C:/HR_Database/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"Contains detailed records of employees including their salary, department ID, and other personal information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The salary of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_id2\": {\"name\": \"department_details\", \"location\": \"C:/HR_Database/department_details.csv\", \"sheet_name\": null, \"label\": \"Department Details\", \"description\": \"Contains information about each department, including department ID and department name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the department.\"}]}, \"table_id3\": {\"name\": \"project_assignments\", \"location\": \"C:/HR_Database/project_assignments.csv\", \"sheet_name\": null, \"label\": \"Project Assignments\", \"description\": \"Lists the projects assigned to each employee, not relevant for salary analysis.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"project_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each project.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR_Database/employee_records.csv\", \"type\": \"csv\"}, \"output\": \"employee_records\", \"comment\": \"Loading the employee records from the HR database.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_records\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employee records to include only those with a salary above $50,000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR_Database/department_details.csv\", \"type\": \"csv\"}, \"output\": \"department_details\", \"comment\": \"Loading the department details from the HR database.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_salary_employees\", \"department_details\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_employee_department\", \"comment\": \"Joining filtered employee records with department details on department ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_employee_department\", \"groupBy\": \"department_name\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"average_salary_per_department\", \"comment\": \"Calculating the average salary per department.\"}]"}]}
{"id":"8bc319b8-1c3f-446a-9645-4c209b2fe916","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Filter the sales data to include only records from the year 2023.\", \"Aggregate the sales data by month to calculate the total sales for each month.\", \"Sort the aggregated data by month in ascending order.\", \"Identify any missing values in the monthly sales data and fill them using linear interpolation.\", \"Set the 'month' column as the index for the time series data.\", \"Apply a rolling mean transformation with a window size of 3 to smooth the sales data.\", \"Forecast the next 6 months of sales using the Holt-Winters method with a seasonal period of 12.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"A dataset containing sales records with columns for date, product, and sales amount.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"product\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the product sold.\"}, {\"column\": \"col_3\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of sales in currency.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"A dataset containing customer information with columns for customer ID, name, and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"The region where the customer is located.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`date`.dt.year == 2023\"}, \"output\": \"sales_data_2023\", \"comment\": \"Filtering sales data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_data_2023\", \"groupBy\": \"date.dt.to_period('M')\", \"aggregations\": [{\"column\": \"sales_amount\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"monthly_sales\", \"comment\": \"Aggregating sales data by month to calculate total sales for each month.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_sales\", \"sortBy\": \"date\", \"order\": \"asc\"}, \"output\": \"sorted_monthly_sales\", \"comment\": \"Sorting the aggregated data by month in ascending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_monthly_sales\", \"transformations\": [{\"columns\": [\"total_sales\"], \"operation\": \"interpolate\", \"parameters\": {\"method\": \"linear\"}}]}, \"output\": \"interpolated_sales\", \"comment\": \"Filling missing values in the monthly sales data using linear interpolation.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"interpolated_sales\", \"transformations\": [{\"columns\": [\"date\"], \"operation\": \"set_index\"}]}, \"output\": \"time_series_sales\", \"comment\": \"Setting the 'month' column as the index for the time series data.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"time_series_sales\", \"transformations\": [{\"columns\": [\"total_sales\"], \"operation\": \"rolling_mean\", \"parameters\": {\"window\": 3}}]}, \"output\": \"smoothed_sales\", \"comment\": \"Applying a rolling mean transformation with a window size of 3 to smooth the sales data.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"smoothed_sales\", \"columns\": [\"total_sales\"], \"forecastParameters\": {\"forecastPeriod\": 6, \"frequency\": \"MS\", \"seasonal_periods\": 12, \"algorithm\": \"holt_winters\", \"dateColumn\": \"date\"}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the next 6 months of sales using the Holt-Winters method with a seasonal period of 12.\"}]"}]}
{"id":"793ade45-c0dc-4be5-a84d-ef8f76dc6ff4","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\", \"Filter the employee data to include only those with a salary greater than $50,000.\", \"Load the department data from the Excel file located at C:/data/department_data.xlsx.\", \"Join the filtered employee data with the department data on the department_id column.\", \"Aggregate the joined data to calculate the average salary per department.\", \"Sort the aggregated data by average salary in descending order.\"]\n\n                ### Context:\n                {\"table_1\": {\"name\": \"employee_data\", \"location\": \"C:/data/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"Contains information about employees including their salaries and department IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The salary of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_2\": {\"name\": \"department_data\", \"location\": \"C:/data/department_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Department Data\", \"description\": \"Contains information about departments including department IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the department.\"}]}, \"table_3\": {\"name\": \"project_data\", \"location\": \"C:/data/project_data.xlsx\", \"sheet_name\": \"Projects\", \"label\": \"Project Data\", \"description\": \"Contains information about projects including project IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"project_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each project.\"}, {\"column\": \"col_2\", \"column_name\": \"project_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the project.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/employee_data.csv\", \"type\": \"csv\"}, \"output\": \"employee_data\", \"comment\": \"Loading the employee data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_data\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employees with a salary greater than $50,000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/department_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"department_data\", \"comment\": \"Loading the department data from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_salary_employees\", \"department_data\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"employee_department_data\", \"comment\": \"Joining filtered employee data with department data on department_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"employee_department_data\", \"groupBy\": \"department_id\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"average_salary_per_department\", \"comment\": \"Calculating the average salary per department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_salary_per_department\", \"sortBy\": \"average_salary\", \"order\": \"desc\"}, \"output\": \"sorted_average_salary\", \"comment\": \"Sorting the aggregated data by average salary in descending order.\"}]"}]}
{"id":"0ddeb3a4-810f-4f8b-aafd-74ed60fee0d9","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'case_records' which contains details of legal cases.\", \"Filter the 'case_records' table to include only cases filed after January 1, 2020.\", \"Join the filtered 'case_records' with the 'lawyer_info' table on the lawyer_id column.\", \"Aggregate the joined table to find the total number of cases handled by each lawyer.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"case_records\", \"location\": \"C:/data/legal/case_records.csv\", \"sheet_name\": null, \"label\": \"Case Records\", \"description\": \"A table containing details of legal cases including case ID, filing date, and lawyer ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each case.\"}, {\"column\": \"col_2\", \"column_name\": \"filing_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the case was filed.\"}, {\"column\": \"col_3\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}]}, \"table_id2\": {\"name\": \"lawyer_info\", \"location\": \"C:/data/legal/lawyer_info.csv\", \"sheet_name\": null, \"label\": \"Lawyer Information\", \"description\": \"A table containing information about lawyers including lawyer ID and name.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"court_schedule\", \"location\": \"C:/data/legal/court_schedule.csv\", \"sheet_name\": null, \"label\": \"Court Schedule\", \"description\": \"A table containing court schedules, not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"schedule_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each schedule.\"}, {\"column\": \"col_2\", \"column_name\": \"court_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the court session.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/case_records.csv\", \"type\": \"csv\"}, \"output\": \"case_records\", \"comment\": \"Loading the table containing details of legal cases.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"case_records\", \"query\": \"`filing_date` > '2020-01-01'\"}, \"output\": \"filtered_case_records\", \"comment\": \"Filtering cases to include only those filed after January 1, 2020.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/legal/lawyer_info.csv\", \"type\": \"csv\"}, \"output\": \"lawyer_info\", \"comment\": \"Loading the table containing information about lawyers.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_case_records\", \"lawyer_info\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered case records with lawyer information based on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_table\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"total_cases\", \"function\": \"count\"}]}, \"output\": \"lawyer_case_count\", \"comment\": \"Aggregating the joined table to find the total number of cases handled by each lawyer.\"}]"}]}
{"id":"53033b9b-748e-4479-bf3c-2d739323b98e","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability scores.\", \"Load the table 'product_data' which contains details about products and their associated suppliers.\", \"Filter the supplier data to include only those with a sustainability score above 80.\", \"Join the filtered supplier data with the product data on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products per supplier.\", \"Sort the aggregated data by the total number of products in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains details about suppliers, including their sustainability scores.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"supplier_name\", \"column_name\": \"SUPPLIER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}, {\"column\": \"sustainability_score\", \"column_name\": \"SUSTAINABILITY_SCORE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability score of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Information\", \"description\": \"Contains details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"product_name\", \"column_name\": \"PRODUCT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Information\", \"description\": \"Contains logistics details which are not relevant to the current workflow.\", \"columns\": [{\"column\": \"logistics_id\", \"column_name\": \"LOGISTICS_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each logistics entry.\"}, {\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transport used for logistics.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading the supplier data which includes sustainability scores.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading the product data which includes supplier associations.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_SCORE > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with sustainability scores above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate the total number of products per supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_products\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the aggregated data by total number of products in descending order.\"}]"}]}
{"id":"5d0cc63b-0811-4642-a874-aa75edac6d0a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee data from the CSV file located at C:/data/employee_data.csv.\", \"Filter the employee data to include only those with a salary greater than 50000.\", \"Join the filtered employee data with the department data on the department_id column.\", \"Aggregate the joined data to calculate the average salary per department.\", \"Sort the aggregated data by average salary in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_data\", \"location\": \"C:/data/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"Contains information about employees including their salaries and department IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Salary of the employee.\"}, {\"column\": \"col_4\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_id2\": {\"name\": \"department_data\", \"location\": \"C:/data/department_data.csv\", \"sheet_name\": null, \"label\": \"Department Data\", \"description\": \"Contains information about departments including department IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the department.\"}]}, \"table_id3\": {\"name\": \"project_data\", \"location\": \"C:/data/project_data.csv\", \"sheet_name\": null, \"label\": \"Project Data\", \"description\": \"Contains information about projects including project IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"project_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each project.\"}, {\"column\": \"col_2\", \"column_name\": \"project_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the project.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/employee_data.csv\", \"type\": \"csv\"}, \"output\": \"employee_data\", \"comment\": \"Loading the employee data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_data\", \"query\": \"salary > 50000\"}, \"output\": \"filtered_employee_data\", \"comment\": \"Filtering employees with a salary greater than 50000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/department_data.csv\", \"type\": \"csv\"}, \"output\": \"department_data\", \"comment\": \"Loading the department data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_employee_data\", \"department_data\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered employee data with department data on department_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"department_name\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Calculating the average salary per department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_salary\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting the aggregated data by average salary in descending order.\"}]"}]}
{"id":"4d8c8c5b-a17b-4313-bbf1-5c0d5e2bab13","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer purchase history from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or 5.\", \"Join the filtered feedback data with the purchase history on customer ID.\", \"Calculate the total purchase amount for each customer.\", \"Sort the customers by total purchase amount in descending order.\", \"Identify the top 10 customers based on total purchase amount.\", \"Aggregate the feedback data to find the average rating for each product.\", \"Filter the aggregated data to include only products with an average rating above 4.5.\", \"Generate a report of the top 10 customers and high-rated products.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"Contains customer feedback including ratings and comments.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"rating\", \"column_type\": \"xsd:integer\", \"column_description\": \"Customer rating for the product, ranging from 1 to 5.\"}, {\"column\": \"column_3_id\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Customer's feedback comment.\"}]}, \"table_id2\": {\"name\": \"purchase_history\", \"location\": \"C:/data/purchase_history.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Purchase History\", \"description\": \"Records of customer purchases including product details and amounts.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"purchase_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount spent by the customer on the purchase.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.xlsx\", \"sheet_name\": \"Products\", \"label\": \"Product Catalog\", \"description\": \"List of products available for purchase with details.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/purchase_history.xlsx\", \"type\": \"xlsx\"}, \"output\": \"purchase_history\", \"comment\": \"Loading customer purchase history from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback to include only ratings of 4 or 5.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"positive_feedback\", \"purchase_history\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"feedback_with_purchases\", \"comment\": \"Joining filtered feedback with purchase history on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"feedback_with_purchases\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"purchase_amount\", \"name\": \"total_purchase\", \"function\": \"sum\"}]}, \"output\": \"customer_total_purchases\", \"comment\": \"Calculating total purchase amount for each customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"customer_total_purchases\", \"sortBy\": \"total_purchase\", \"order\": \"desc\"}, \"output\": \"sorted_customers\", \"comment\": \"Sorting customers by total purchase amount in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_customers\", \"query\": \"index < 10\"}, \"output\": \"top_10_customers\", \"comment\": \"Identifying the top 10 customers based on total purchase amount.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"positive_feedback\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"product_average_ratings\", \"comment\": \"Aggregating feedback to find average rating for each product.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"product_average_ratings\", \"query\": \"average_rating > 4.5\"}, \"output\": \"high_rated_products\", \"comment\": \"Filtering products with an average rating above 4.5.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"top_10_customers\", \"high_rated_products\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"customer_product_report\", \"comment\": \"Generating a report of the top 10 customers and high-rated products.\"}]"}]}
{"id":"650b8989-0848-44b7-a38d-e8d9b3a5c527","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'shipment_data' which contains details of all shipments.\", \"Filter the shipment data to include only shipments with a weight greater than 500 kg.\", \"Join the filtered shipment data with the 'vehicle_data' table on the vehicle_id column.\", \"Aggregate the joined data to calculate the total shipment weight for each vehicle type.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"This table contains details of all shipments including shipment ID, weight, and vehicle ID.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"column_2_id\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"column_3_id\", \"column_name\": \"vehicle_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the vehicle used for the shipment.\"}]}, \"table_id2\": {\"name\": \"vehicle_data\", \"location\": \"C:/data/logistics/vehicle_data.csv\", \"sheet_name\": null, \"label\": \"Vehicle Data\", \"description\": \"This table contains details about vehicles including vehicle ID and type.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"vehicle_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each vehicle.\"}, {\"column\": \"column_2_id\", \"column_name\": \"vehicle_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of the vehicle.\"}]}, \"table_id3\": {\"name\": \"driver_data\", \"location\": \"C:/data/logistics/driver_data.csv\", \"sheet_name\": null, \"label\": \"Driver Data\", \"description\": \"This table contains information about drivers including driver ID and name.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"driver_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each driver.\"}, {\"column\": \"column_2_id\", \"column_name\": \"driver_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the driver.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 500\"}, \"output\": \"filtered_shipment_data\", \"comment\": \"Filtering shipments to include only those with weight greater than 500 kg.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/vehicle_data.csv\", \"type\": \"csv\"}, \"output\": \"vehicle_data\", \"comment\": \"Loading the vehicle data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipment_data\", \"vehicle_data\"], \"joinOn\": [\"vehicle_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with vehicle data on vehicle_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"vehicle_type\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_shipment_weight\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate total shipment weight for each vehicle type.\"}]"}]}
{"id":"22432e36-826b-4d53-8853-fac5cdafc409","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'production_schedule' which contains the manufacturing schedule for the upcoming month.\", \"Filter the 'production_schedule' to include only entries where the 'status' is 'pending'.\", \"Join the filtered 'production_schedule' with the 'inventory' table on 'product_id' to get inventory details.\", \"Aggregate the joined table to calculate the total quantity required for each product.\", \"Sort the aggregated data by 'total_quantity' in descending order to prioritize production.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the manufacturing schedule for the upcoming month, including product IDs, quantities, and status.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"quantity\", \"column_type\": \"xsd:integer\", \"column_description\": \"The quantity of the product to be manufactured.\"}, {\"column\": \"column_3_id\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"The current status of the production order, e.g., pending, completed.\"}]}, \"table_id2\": {\"name\": \"inventory\", \"location\": \"C:/data/manufacturing/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory Details\", \"description\": \"This table contains current inventory levels for each product, identified by product ID.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"current_stock\", \"column_type\": \"xsd:integer\", \"column_description\": \"The current stock level of the product.\"}]}, \"table_id3\": {\"name\": \"employee_schedule\", \"location\": \"C:/data/manufacturing/employee_schedule.csv\", \"sheet_name\": null, \"label\": \"Employee Schedule\", \"description\": \"This table contains the work schedule for employees, including shifts and assigned tasks.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"column_2_id\", \"column_name\": \"shift\", \"column_type\": \"xsd:string\", \"column_description\": \"The shift assigned to the employee.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the manufacturing schedule for the upcoming month from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"status == 'pending'\"}, \"output\": \"pending_production_schedule\", \"comment\": \"Filtering the production schedule to include only pending entries.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"pending_production_schedule\", \"inventory\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"production_with_inventory\", \"comment\": \"Joining the filtered production schedule with inventory details based on product_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"production_with_inventory\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"quantity\", \"name\": \"total_quantity\", \"function\": \"sum\"}]}, \"output\": \"aggregated_production\", \"comment\": \"Aggregating the joined table to calculate the total quantity required for each product.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_production\", \"sortBy\": \"total_quantity\", \"order\": \"desc\"}, \"output\": \"prioritized_production\", \"comment\": \"Sorting the aggregated data by total quantity in descending order to prioritize production.\"}]"}]}
{"id":"a36dc0cd-c4d2-461b-9328-bf3c684b7489","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Calculate the total energy consumption for each building.\", \"Sort the buildings by total energy consumption in descending order.\", \"Identify the top 5 buildings with the highest energy consumption.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"building_energy_data\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption Data\", \"description\": \"This table contains energy consumption data for various buildings, including timestamps and energy usage values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_2\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_3\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"building_info\", \"location\": \"C:/data/building_info.xlsx\", \"sheet_name\": \"Info\", \"label\": \"Building Information\", \"description\": \"This table contains additional information about the buildings, such as location and size.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_B\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"The location of the building.\"}, {\"column\": \"col_C\", \"column_name\": \"size\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The size of the building in square meters.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_X\", \"column_name\": \"random_id\", \"column_type\": \"xsd:string\", \"column_description\": \"A random identifier.\"}, {\"column\": \"col_Y\", \"column_name\": \"random_value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"A random value.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"building_energy_data\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"building_energy_data\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"filtered_energy_data\", \"comment\": \"Filtering the data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_data\", \"groupBy\": \"building_id\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy_consumption\", \"function\": \"sum\"}]}, \"output\": \"total_energy_by_building\", \"comment\": \"Calculating the total energy consumption for each building.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"total_energy_by_building\", \"sortBy\": \"total_energy_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_energy_data\", \"comment\": \"Sorting the buildings by total energy consumption in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_energy_data\", \"query\": \"index < 5\"}, \"output\": \"top_5_buildings\", \"comment\": \"Identifying the top 5 buildings with the highest energy consumption.\"}]"}]}
{"id":"11a17411-0365-49d0-ad5a-d76ed769b783","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file located at 'C:/data/finance/transactions.csv'.\", \"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\", \"Load the 'customer_info' table from the Excel file located at 'C:/data/finance/customers.xlsx'.\", \"Join the filtered transactions table with the 'customer_info' table on the 'customer_id' column using an inner join.\", \"Aggregate the joined table by 'customer_region' to calculate the total transaction amount for each region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/finance/transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"This table contains detailed records of financial transactions including transaction ID, customer ID, amount, and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The monetary amount of the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the transaction occurred.\"}]}, \"table_id2\": {\"name\": \"customer_info\", \"location\": \"C:/data/finance/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Information\", \"description\": \"This table contains customer details such as customer ID, name, and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"customer_region\", \"column_type\": \"xsd:string\", \"column_description\": \"The region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"irrelevant_financial_data\", \"location\": \"C:/data/finance/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Financial Data\", \"description\": \"This table contains financial data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_id\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant identifier.\"}, {\"column\": \"col_B\", \"column_name\": \"irrelevant_value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"An irrelevant financial value.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_info\", \"comment\": \"Loading the customer information dataset from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"customer_info\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with customer information on customer_id using an inner join.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_region\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_transaction_amount\", \"function\": \"sum\"}]}, \"output\": \"regional_transaction_totals\", \"comment\": \"Aggregating the joined data by customer region to calculate total transaction amounts.\"}]"}]}
{"id":"eae80765-0e2f-4561-b8a8-35f8de0a08f7","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Aggregate the energy consumption data by month to calculate total usage.\", \"Calculate the average daily energy consumption for each month.\", \"Apply a rolling mean transformation with a window of 3 months to smooth the monthly data.\", \"Forecast the energy consumption for the next 6 months using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains energy consumption records with timestamps and usage values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"energy_prices\", \"location\": \"C:/data/energy_prices.csv\", \"sheet_name\": null, \"label\": \"Energy Prices Data\", \"description\": \"This table contains historical energy prices with timestamps.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"timestamp >= '2023-01-01' and timestamp < '2024-01-01'\"}, \"output\": \"energy_2023\", \"comment\": \"Filtering the data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"energy_2023\", \"groupBy\": \"timestamp.dt.to_period('M')\", \"aggregations\": [{\"column\": \"usage\", \"name\": \"total_usage\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_usage\", \"comment\": \"Aggregating the energy consumption data by month to calculate total usage.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"monthly_energy_usage\", \"columnName\": \"average_daily_usage\", \"formula\": \"total_usage / 30\"}, \"output\": \"monthly_energy_with_avg\", \"comment\": \"Calculating the average daily energy consumption for each month.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"monthly_energy_with_avg\", \"transformations\": [{\"columns\": [\"total_usage\"], \"operation\": \"rolling_mean\", \"parameters\": {\"window\": 3}}]}, \"output\": \"smoothed_monthly_energy\", \"comment\": \"Applying a rolling mean transformation with a window of 3 months to smooth the monthly data.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"smoothed_monthly_energy\", \"columns\": [\"total_usage\"], \"forecastParameters\": {\"forecastPeriod\": 6, \"frequency\": \"MS\", \"dateColumn\": \"timestamp\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"confidenceInterval\": 95}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting the energy consumption for the next 6 months using the Holt-Winters method.\"}]"}]}
{"id":"5fd80dfa-b8cb-4c43-a244-135a5eed64dc","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file.\", \"Filter the data to include only records where consumption is greater than 100 kWh.\", \"Aggregate the filtered data to calculate the total consumption per month.\", \"Sort the aggregated data by total consumption in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains energy consumption data with columns for date, consumption in kWh, and other related metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}, {\"column\": \"col_3\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"The location where the energy consumption was recorded.\"}]}, \"table_id2\": {\"name\": \"energy_prices\", \"location\": \"C:/data/energy_prices.csv\", \"sheet_name\": null, \"label\": \"Energy Prices Data\", \"description\": \"This table contains historical energy prices with columns for date, price per kWh, and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price_per_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"The region where the energy price was recorded.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"consumption_kWh > 100\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering data to include only records where consumption is greater than 100 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_consumption\", \"groupBy\": \"date\", \"aggregations\": [{\"column\": \"consumption_kWh\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_consumption\", \"comment\": \"Aggregating filtered data to calculate total consumption per month.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_consumption\", \"sortBy\": \"total_monthly_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_monthly_consumption\", \"comment\": \"Sorting aggregated data by total consumption in descending order.\"}]"}]}
{"id":"816e716a-486f-4286-b16e-fb48f75f56c4","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Filter the sales data to include only records where the total sales amount is greater than 1000.\", \"Load the customer data from the Excel file located at C:/data/customers.xlsx.\", \"Join the filtered sales data with the customer data on the customer_id column using an inner join.\", \"Aggregate the joined data by region to calculate the total sales amount for each region.\", \"Sort the aggregated data by total sales amount in descending order.\", \"Calculate a new column in the sorted data for profit margin by subtracting total cost from total sales.\", \"Apply a log transformation to the profit margin column to stabilize variance.\", \"Perform a time-series forecast on the total sales column using the Holt-Winters algorithm with a forecast period of 12 months.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales records including sales amount, customer ID, and other relevant details.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"total_sales\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total sales amount for each transaction.\"}, {\"column\": \"column_2_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and region.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"The region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_data\", \"location\": \"C:/data/products.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"This table contains product details including product ID and category.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"total_sales > 1000\"}, \"output\": \"filtered_sales_data\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales_data\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_sales\", \"name\": \"total_sales_by_region\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_sales_by_region\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_aggregated_data\", \"columnName\": \"profit_margin\", \"formula\": \"total_sales - total_cost\"}, \"output\": \"data_with_profit_margin\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"data_with_profit_margin\", \"transformations\": [{\"columns\": [\"profit_margin\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_data\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"transformed_data\", \"columns\": [\"total_sales\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12}}, \"output\": \"forecast_results\"}]"}]}
{"id":"fbdaf8ca-67f2-49b4-b577-ba73e9e2c082","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table which contains information about suppliers and their sustainability scores.\", \"Load the 'product_data' table which contains details about products and their associated suppliers.\", \"Filter the 'supplier_data' to include only suppliers with a sustainability score above 80.\", \"Join the filtered 'supplier_data' with 'product_data' on the supplier ID to get products from sustainable suppliers.\", \"Aggregate the joined data to calculate the total number of sustainable products per category.\", \"Load the 'transport_data' table which contains information about transportation modes and emissions.\", \"Filter the 'transport_data' to include only transportation modes with emissions below 50 CO2 per unit.\", \"Join the sustainable products data with filtered 'transport_data' on transportation mode to get sustainable logistics options.\", \"Calculate a new column 'total_sustainability_score' by summing the sustainability score and inverse of emissions.\", \"Sort the final data by 'total_sustainability_score' in descending order to prioritize the most sustainable options.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers and their sustainability scores.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_score\", \"column_name\": \"SUSTAINABILITY_SCORE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability score of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"Details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}, {\"column\": \"category\", \"column_name\": \"CATEGORY\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product.\"}]}, \"table_id3\": {\"name\": \"transport_data\", \"location\": \"C:/data/transport_data.csv\", \"sheet_name\": null, \"label\": \"Transport Data\", \"description\": \"Information about transportation modes and emissions.\", \"columns\": [{\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transportation.\"}, {\"column\": \"emissions\", \"column_name\": \"EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions in CO2 per unit for the transportation mode.\"}]}, \"table_id4\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains unrelated data not used in this workflow.\", \"columns\": [{\"column\": \"irrelevant_column\", \"column_name\": \"IRRELEVANT_COLUMN\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant column not used in this workflow.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data containing sustainability scores.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data with associated suppliers.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_SCORE > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with sustainability scores above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"sustainable_products\", \"comment\": \"Joining filtered suppliers with products to get sustainable products.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sustainable_products\", \"groupBy\": \"CATEGORY\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_sustainable_products\", \"function\": \"count\"}]}, \"output\": \"sustainable_products_per_category\", \"comment\": \"Aggregating to find total sustainable products per category.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/transport_data.csv\", \"type\": \"csv\"}, \"output\": \"transport_data\", \"comment\": \"Loading transport data with emissions information.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"transport_data\", \"query\": \"EMISSIONS < 50\"}, \"output\": \"filtered_transport_data\", \"comment\": \"Filtering transportation modes with emissions below 50 CO2 per unit.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"sustainable_products_per_category\", \"filtered_transport_data\"], \"joinOn\": [\"TRANSPORT_MODE\"], \"joinType\": \"inner\"}, \"output\": \"sustainable_logistics\", \"comment\": \"Joining sustainable products with transport data for logistics options.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sustainable_logistics\", \"columnName\": \"total_sustainability_score\", \"formula\": \"SUSTAINABILITY_SCORE + (1 / EMISSIONS)\"}, \"output\": \"sustainability_score_calculated\", \"comment\": \"Calculating total sustainability score by combining scores and inverse emissions.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"sustainability_score_calculated\", \"sortBy\": \"total_sustainability_score\", \"order\": \"desc\"}, \"output\": \"sorted_sustainable_options\", \"comment\": \"Sorting data by total sustainability score to prioritize options.\"}]"}]}
{"id":"f029e71a-be73-4402-9561-8c77916a1592","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee records from the CSV file located at C:/data/employee_records.csv.\", \"Load the department information from the Excel file located at C:/data/department_info.xlsx.\", \"Filter the employee records to include only those with a salary greater than 50,000.\", \"Join the filtered employee records with the department information on the department_id column.\", \"Calculate a new column 'annual_bonus' as 10% of the salary for each employee.\", \"Aggregate the data to find the total salary and total annual bonus for each department.\", \"Sort the aggregated data by total salary in descending order.\", \"Apply a standardization transformation to the total salary and total annual bonus columns.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_records\", \"location\": \"C:/data/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"Contains detailed records of employees including their salaries and department IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Annual salary of the employee.\"}, {\"column\": \"col_4\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_id2\": {\"name\": \"department_info\", \"location\": \"C:/data/department_info.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Department Information\", \"description\": \"Contains information about each department including department IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the department.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/employee_records.csv\", \"type\": \"csv\"}, \"output\": \"employee_records\", \"comment\": \"Loading employee records from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/department_info.xlsx\", \"type\": \"xlsx\"}, \"output\": \"department_info\", \"comment\": \"Loading department information from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_records\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employees with a salary greater than 50,000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_salary_employees\", \"department_info\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"employee_department_data\", \"comment\": \"Joining filtered employees with department information on department_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"employee_department_data\", \"columnName\": \"annual_bonus\", \"formula\": \"salary * 0.10\"}, \"output\": \"employee_with_bonus\", \"comment\": \"Calculating annual bonus as 10% of the salary.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"employee_with_bonus\", \"groupBy\": \"department_id\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"total_salary\", \"function\": \"sum\"}, {\"column\": \"annual_bonus\", \"name\": \"total_annual_bonus\", \"function\": \"sum\"}]}, \"output\": \"department_aggregates\", \"comment\": \"Aggregating total salary and annual bonus by department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"department_aggregates\", \"sortBy\": \"total_salary\", \"order\": \"desc\"}, \"output\": \"sorted_department_aggregates\", \"comment\": \"Sorting departments by total salary in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_department_aggregates\", \"transformations\": [{\"columns\": [\"total_salary\", \"total_annual_bonus\"], \"operation\": \"standardize\"}]}, \"output\": \"standardized_department_aggregates\", \"comment\": \"Standardizing total salary and total annual bonus columns.\"}]"}]}
{"id":"b665b800-49d2-4248-bc46-0062c9dc6b52","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer data from the CSV file located at C:/data/marketing/customers.csv.\", \"Filter the customer data to include only those with an annual spending greater than $5000.\", \"Join the filtered customer data with the campaign data on the customer_id column.\", \"Aggregate the joined data to calculate the total spending per campaign.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customers\", \"location\": \"C:/data/marketing/customers.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"Contains customer information including spending habits.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"annual_spending\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total annual spending of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}]}, \"table_id2\": {\"name\": \"campaigns\", \"location\": \"C:/data/marketing/campaigns.csv\", \"sheet_name\": null, \"label\": \"Campaign Data\", \"description\": \"Contains information about marketing campaigns.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier linking customer to campaign.\"}, {\"column\": \"col_3\", \"column_name\": \"campaign_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the marketing campaign.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/marketing/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant identifier.\"}, {\"column\": \"col_2\", \"column_name\": \"irrelevant_info\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant information.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/customers.csv\", \"type\": \"csv\"}, \"output\": \"customers\", \"comment\": \"Loading customer data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customers\", \"query\": \"annual_spending > 5000\"}, \"output\": \"high_spending_customers\", \"comment\": \"Filtering customers with annual spending greater than $5000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/campaigns.csv\", \"type\": \"csv\"}, \"output\": \"campaigns\", \"comment\": \"Loading campaign data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_spending_customers\", \"campaigns\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"customer_campaign_data\", \"comment\": \"Joining filtered customer data with campaign data on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"customer_campaign_data\", \"groupBy\": \"campaign_id\", \"aggregations\": [{\"column\": \"annual_spending\", \"name\": \"total_spending\", \"function\": \"sum\"}]}, \"output\": \"campaign_spending_summary\", \"comment\": \"Aggregating data to calculate total spending per campaign.\"}]"}]}
{"id":"8d75c095-37a5-4a62-89e0-5e73491606f9","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'case_details' from the legal database.\", \"Load the table 'client_info' from the legal database.\", \"Filter the 'case_details' table to include only cases with a status of 'Open'.\", \"Join the filtered 'case_details' table with the 'client_info' table on 'client_id'.\", \"Aggregate the joined table to count the number of open cases per client.\", \"Sort the aggregated data by the number of open cases in descending order.\", \"Calculate a derived column 'case_ratio' as the ratio of open cases to total cases for each client.\", \"Filter the final table to include only clients with a 'case_ratio' greater than 0.5.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"case_details\", \"location\": \"C:/legal_data/case_details.csv\", \"sheet_name\": null, \"label\": \"Case Details\", \"description\": \"Contains information about legal cases, including status and client ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each case.\"}, {\"column\": \"col_2\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"Current status of the case (e.g., Open, Closed).\"}, {\"column\": \"col_3\", \"column_name\": \"client_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the client associated with the case.\"}]}, \"table_id2\": {\"name\": \"client_info\", \"location\": \"C:/legal_data/client_info.csv\", \"sheet_name\": null, \"label\": \"Client Information\", \"description\": \"Contains personal and contact information for clients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"client_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each client.\"}, {\"column\": \"col_2\", \"column_name\": \"client_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the client.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_number\", \"column_type\": \"xsd:string\", \"column_description\": \"Contact number of the client.\"}]}, \"table_id3\": {\"name\": \"lawyer_info\", \"location\": \"C:/legal_data/lawyer_info.csv\", \"sheet_name\": null, \"label\": \"Lawyer Information\", \"description\": \"Contains details about lawyers, not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"col_2\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/case_details.csv\", \"type\": \"csv\"}, \"output\": \"case_details\", \"comment\": \"Loading the case details dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/client_info.csv\", \"type\": \"csv\"}, \"output\": \"client_info\", \"comment\": \"Loading the client information dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"case_details\", \"query\": \"status == 'Open'\"}, \"output\": \"open_cases\", \"comment\": \"Filtering case details to include only open cases.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"open_cases\", \"client_info\"], \"joinOn\": [\"client_id\"], \"joinType\": \"inner\"}, \"output\": \"open_cases_with_clients\", \"comment\": \"Joining open cases with client information based on client_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"open_cases_with_clients\", \"groupBy\": \"client_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"open_case_count\", \"function\": \"count\"}]}, \"output\": \"client_case_counts\", \"comment\": \"Aggregating data to count the number of open cases per client.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"client_case_counts\", \"sortBy\": \"open_case_count\", \"order\": \"desc\"}, \"output\": \"sorted_client_case_counts\", \"comment\": \"Sorting clients by the number of open cases in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_client_case_counts\", \"columnName\": \"case_ratio\", \"formula\": \"open_case_count / total_cases\"}, \"output\": \"client_case_ratios\", \"comment\": \"Calculating the ratio of open cases to total cases for each client.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"client_case_ratios\", \"query\": \"case_ratio > 0.5\"}, \"output\": \"high_ratio_clients\", \"comment\": \"Filtering clients with a case ratio greater than 0.5.\"}]"}]}
{"id":"bd7fedae-10db-41bb-98a1-8d07d0c9884c","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\", \"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\", \"Aggregate the filtered data to calculate the total weight of shipments by destination.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"This table contains information about shipments including weight, destination, and other logistics details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_3\", \"column_name\": \"destination\", \"column_type\": \"xsd:string\", \"column_description\": \"Destination of the shipment.\"}]}, \"table_id2\": {\"name\": \"transport_modes\", \"location\": \"C:/data/logistics/transport_modes.csv\", \"sheet_name\": null, \"label\": \"Transport Modes\", \"description\": \"This table contains information about different modes of transport used in logistics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"mode_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transport mode.\"}, {\"column\": \"col_2\", \"column_name\": \"mode_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the transport mode.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"`weight` > 1000\"}, \"output\": \"heavy_shipments\", \"comment\": \"Filtering shipments to include only those with a weight greater than 1000 kg.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"heavy_shipments\", \"groupBy\": \"destination\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_weight\", \"function\": \"sum\"}]}, \"output\": \"total_weight_by_destination\", \"comment\": \"Aggregating the filtered shipments to calculate the total weight by destination.\"}]"}]}
{"id":"67ca8973-e5ff-47ef-9523-39700d6bdf46","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file located at C:/data/energy_consumption.csv.\", \"Load the weather data from the Excel file located at C:/data/weather_data.xlsx.\", \"Filter the energy consumption data to include only records from the year 2023.\", \"Join the filtered energy consumption data with the weather data on the date column.\", \"Calculate a new column 'energy_efficiency' by dividing 'energy_usage' by 'temperature'.\", \"Aggregate the joined data by month to calculate the total energy usage and average temperature.\", \"Sort the aggregated data by total energy usage in descending order.\", \"Forecast the next 30 days of energy usage using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains energy consumption data with columns for date, energy usage, and other relevant metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy used on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"other_metric\", \"column_type\": \"xsd:string\", \"column_description\": \"Other relevant metrics for energy consumption.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Weather Data\", \"description\": \"This table contains weather data with columns for date, temperature, and other weather metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The temperature recorded on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The humidity level recorded on the given date.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the energy management workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column for this workflow.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`date`.dt.year == 2023\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering energy consumption data to include only records from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_energy_consumption\", \"weather_data\"], \"joinOn\": [\"date\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered energy consumption data with weather data on the date column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"energy_efficiency\", \"formula\": \"`energy_usage` / `temperature`\"}, \"output\": \"data_with_efficiency\", \"comment\": \"Calculating energy efficiency by dividing energy usage by temperature.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_efficiency\", \"groupBy\": \"date.dt.to_period('M')\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy_usage\", \"function\": \"sum\"}, {\"column\": \"temperature\", \"name\": \"average_temperature\", \"function\": \"mean\"}]}, \"output\": \"monthly_aggregated_data\", \"comment\": \"Aggregating data by month to calculate total energy usage and average temperature.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_aggregated_data\", \"sortBy\": \"total_energy_usage\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total energy usage in descending order.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"energy_consumption\", \"columns\": [\"energy_usage\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7}}, \"output\": \"energy_forecast\", \"comment\": \"Forecasting the next 30 days of energy usage using the Holt-Winters method.\"}]"}]}
{"id":"0ff47f57-6683-464e-840c-c84cdadaa0fd","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'shipments' table containing shipment details.\", \"Load the 'delivery_times' table with delivery time records.\", \"Filter the 'shipments' table to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered shipments with the 'delivery_times' table on the 'shipment_id' column.\", \"Calculate a new column 'delivery_delay' by subtracting 'expected_delivery_date' from 'actual_delivery_date'.\", \"Aggregate the joined table to find the average delivery delay per 'destination_city'.\", \"Sort the aggregated data by 'average_delay' in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipments\", \"location\": \"C:/data/logistics/shipments.csv\", \"sheet_name\": null, \"label\": \"Shipment Details\", \"description\": \"Contains details of all shipments including shipment_id, weight, and destination.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"column_2_id\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"column_3_id\", \"column_name\": \"destination_city\", \"column_type\": \"xsd:string\", \"column_description\": \"City where the shipment is to be delivered.\"}]}, \"table_id2\": {\"name\": \"delivery_times\", \"location\": \"C:/data/logistics/delivery_times.csv\", \"sheet_name\": null, \"label\": \"Delivery Times\", \"description\": \"Records of delivery times including shipment_id, expected and actual delivery dates.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"column_2_id\", \"column_name\": \"expected_delivery_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date the shipment is expected to be delivered.\"}, {\"column\": \"column_3_id\", \"column_name\": \"actual_delivery_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date the shipment was actually delivered.\"}]}, \"table_id3\": {\"name\": \"warehouse_inventory\", \"location\": \"C:/data/logistics/warehouse_inventory.csv\", \"sheet_name\": null, \"label\": \"Warehouse Inventory\", \"description\": \"Details of current inventory in the warehouse, not relevant for this workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"item_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each inventory item.\"}, {\"column\": \"column_2_id\", \"column_name\": \"quantity\", \"column_type\": \"xsd:integer\", \"column_description\": \"Quantity of the item in stock.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipments.csv\", \"type\": \"csv\"}, \"output\": \"shipments\", \"comment\": \"Loading the shipments dataset containing shipment details.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/delivery_times.csv\", \"type\": \"csv\"}, \"output\": \"delivery_times\", \"comment\": \"Loading the delivery times dataset with delivery records.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipments\", \"query\": \"weight > 1000\"}, \"output\": \"heavy_shipments\", \"comment\": \"Filtering shipments to include only those with weight greater than 1000 kg.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"heavy_shipments\", \"delivery_times\"], \"joinOn\": [\"shipment_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_shipments\", \"comment\": \"Joining filtered shipments with delivery times on shipment_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_shipments\", \"columnName\": \"delivery_delay\", \"formula\": \"`actual_delivery_date` - `expected_delivery_date`\"}, \"output\": \"shipments_with_delay\", \"comment\": \"Calculating delivery delay by subtracting expected delivery date from actual delivery date.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"shipments_with_delay\", \"groupBy\": \"destination_city\", \"aggregations\": [{\"column\": \"delivery_delay\", \"name\": \"average_delay\", \"function\": \"mean\"}]}, \"output\": \"average_delay_per_city\", \"comment\": \"Aggregating data to find average delivery delay per destination city.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_delay_per_city\", \"sortBy\": \"average_delay\", \"order\": \"desc\"}, \"output\": \"sorted_delays\", \"comment\": \"Sorting the aggregated data by average delay in descending order.\"}]"}]}
{"id":"f3292fef-35b7-4037-b730-2ff3aec00dcd","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the financial transactions table from the CSV file.\", \"Filter the transactions to include only those with an amount greater than 1000.\", \"Calculate the total amount for each category.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/finance/transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing all financial transactions including date, amount, and category.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category of the transaction, such as groceries, utilities, etc.\"}]}, \"table_id2\": {\"name\": \"budget_plan\", \"location\": \"C:/data/finance/budget_plan.xlsx\", \"sheet_name\": \"Plan\", \"label\": \"Budget Plan\", \"description\": \"A table outlining the planned budget for different categories.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"The category for which the budget is planned.\"}, {\"column\": \"col_2\", \"column_name\": \"planned_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The planned budget amount for the category.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"`amount` > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_transactions\", \"groupBy\": \"category\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_amount\", \"function\": \"sum\"}]}, \"output\": \"category_totals\", \"comment\": \"Calculating the total amount for each category.\"}]"}]}
{"id":"d38e2148-1bab-465d-b2f0-5233791ccdf0","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supply_chain_data' table from the CSV file located at 'C:/data/supply_chain.csv'.\", \"Filter the 'supply_chain_data' table to include only records where the 'carbon_emission' is less than 50.\", \"Aggregate the filtered data by 'supplier_region' to calculate the total 'carbon_emission' for each region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supply_chain_data\", \"location\": \"C:/data/supply_chain.csv\", \"sheet_name\": null, \"label\": \"Supply Chain Data\", \"description\": \"This table contains data on supply chain operations, including supplier regions and carbon emissions.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"supplier_region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the supplier is located.\"}, {\"column\": \"column_3_id\", \"column_name\": \"carbon_emission\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Carbon emissions reported by the supplier.\"}]}, \"table_id2\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains unrelated data not relevant to the supply chain sustainability analysis.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"unrelated_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An unrelated column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supply_chain.csv\", \"type\": \"csv\"}, \"output\": \"supply_chain_data\", \"comment\": \"Loading the supply chain data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supply_chain_data\", \"query\": \"carbon_emission < 50\"}, \"output\": \"filtered_supply_chain_data\", \"comment\": \"Filtering supply chain data to include only records with carbon emissions less than 50.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_supply_chain_data\", \"groupBy\": \"supplier_region\", \"aggregations\": [{\"column\": \"carbon_emission\", \"name\": \"total_carbon_emission\", \"function\": \"sum\"}]}, \"output\": \"regional_carbon_emission\", \"comment\": \"Aggregating filtered data by supplier region to calculate total carbon emissions for each region.\"}]"}]}
{"id":"14d76152-efaf-47e5-98c5-6e031caaec2f","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Load the market analysis data from the Excel sheet.\", \"Filter the market analysis data to include only entries from the last year.\", \"Join the product specifications with the filtered market analysis data on the product ID.\", \"Calculate a new column for potential revenue by multiplying the estimated sales by the product price.\", \"Aggregate the joined data by product category to find the total potential revenue.\", \"Sort the aggregated data by total potential revenue in descending order.\", \"Filter the sorted data to include only the top 5 product categories.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"Contains detailed specifications for each product, including product ID, name, and price.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}, \"table_id2\": {\"name\": \"market_analysis\", \"location\": \"C:/data/market_analysis.xlsx\", \"sheet_name\": \"2023\", \"label\": \"Market Analysis\", \"description\": \"Market analysis data including estimated sales and market trends for various products.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"estimated_sales\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Estimated sales figures for the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"year\", \"column_type\": \"xsd:integer\", \"column_description\": \"Year of the market analysis data.\"}]}, \"table_id3\": {\"name\": \"historical_sales\", \"location\": \"C:/data/historical_sales.csv\", \"sheet_name\": null, \"label\": \"Historical Sales\", \"description\": \"Contains historical sales data for various products, not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"sales\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Historical sales figures for the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"year\", \"column_type\": \"xsd:integer\", \"column_description\": \"Year of the sales data.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading product specifications from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/market_analysis.xlsx\", \"type\": \"xlsx\"}, \"output\": \"market_analysis\", \"comment\": \"Loading market analysis data from an Excel sheet.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"market_analysis\", \"query\": \"year == 2023\"}, \"output\": \"filtered_market_analysis\", \"comment\": \"Filtering market analysis data to include only entries from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"product_specifications\", \"filtered_market_analysis\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining product specifications with filtered market analysis data on product ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"potential_revenue\", \"formula\": \"estimated_sales * price\"}, \"output\": \"data_with_revenue\", \"comment\": \"Calculating potential revenue by multiplying estimated sales by product price.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_revenue\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"potential_revenue\", \"name\": \"total_potential_revenue\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by product ID to find total potential revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_potential_revenue\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by total potential revenue in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_data\", \"query\": \"index < 5\"}, \"output\": \"top_categories\", \"comment\": \"Filtering sorted data to include only the top 5 product categories.\"}]"}]}
{"id":"bedb6011-d9f0-45e2-9825-f2843caa069d","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Filter the sales data to include only records from the year 2023.\", \"Apply a time series transformation to set the 'date' column as the index.\", \"Forecast the sales revenue for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales records including date, revenue, and other sales-related metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales record.\"}, {\"column\": \"col_2\", \"column_name\": \"revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The revenue generated on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The identifier for the product sold.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"Contains customer information including customer ID, name, and contact details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_number\", \"column_type\": \"xsd:string\", \"column_description\": \"Contact number of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"}, \"output\": \"sales_data_2023\", \"comment\": \"Filtering sales data to include only records from the year 2023.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data_2023\", \"transformations\": [{\"columns\": [\"date\"], \"operation\": \"set_index\", \"parameters\": {}}]}, \"output\": \"sales_data_2023_indexed\", \"comment\": \"Setting the 'date' column as the index for time series analysis.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data_2023_indexed\", \"columns\": [\"revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting the sales revenue for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"1b7ec2a5-2451-4799-9b45-79c577cbcedf","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Load the customer data from the Excel file located at C:/data/customer_data.xlsx.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with customer data on the customer_id column using an inner join.\", \"Aggregate the joined data by region to calculate the total sales revenue.\", \"Sort the aggregated data by total sales in descending order.\", \"Calculate a new column 'profit_margin' by subtracting total cost from total revenue in the sales data.\", \"Apply a rolling mean transformation with a window size of 7 on the total revenue column.\", \"Set the date column as the index for the sales data.\", \"Forecast the future sales revenue for the next 30 days using the Holt-Winters method with a seasonal period of 7.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including total revenue, total cost, and transaction dates.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost incurred for the transaction.\"}, {\"column\": \"col_5\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/inventory_data.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains information about inventory levels and product details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"stock_level\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current stock level of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to calculate total sales revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting aggregated data by total sales in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_data\", \"columnName\": \"profit_margin\", \"formula\": \"`total_revenue` - `total_cost`\"}, \"output\": \"sales_data_with_profit\", \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data\", \"transformations\": [{\"columns\": [\"total_revenue\"], \"operation\": \"rolling_mean\", \"parameters\": {\"window\": 7}}]}, \"output\": \"sales_data_rolling_mean\", \"comment\": \"Applying a rolling mean transformation with a window size of 7 on total revenue.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data\", \"transformations\": [{\"columns\": [\"date\"], \"operation\": \"set_index\"}]}, \"output\": \"sales_data_indexed\", \"comment\": \"Setting the date column as the index for the sales data.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data_indexed\", \"columns\": [\"total_revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"dateColumn\": \"date\"}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting future sales revenue for the next 30 days using Holt-Winters method.\"}]"}]}
{"id":"49b53404-d4bc-4124-976a-2fae20cf6470","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Load the customer data from the Excel file located at C:/data/customers.xlsx.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with the customer data on the customer_id column using an inner join.\", \"Aggregate the joined data by region to calculate the total sales revenue.\", \"Sort the aggregated data by total sales in descending order.\", \"Calculate a new column for profit margin by subtracting total cost from total revenue.\", \"Apply a logarithmic transformation to the profit margin column to stabilize variance.\", \"Forecast the future sales revenue for the next 30 days using the Holt-Winters method with a seasonal period of 7 days.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including total revenue, total cost, and customer IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost incurred for the transaction.\"}, {\"column\": \"col_5\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory details including product IDs and stock levels.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"stock_level\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current stock level of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to calculate total sales revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting aggregated data by total sales in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_with_customers\", \"columnName\": \"profit_margin\", \"formula\": \"`total_revenue` - `total_cost`\"}, \"output\": \"sales_with_profit\", \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sales_with_profit\", \"transformations\": [{\"columns\": [\"profit_margin\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_sales\", \"comment\": \"Applying a logarithmic transformation to the profit margin column.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"transformed_sales\", \"columns\": [\"total_revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"transaction_date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting future sales revenue for the next 30 days using Holt-Winters method.\"}]"}]}
{"id":"e1d396b0-f9d6-44b0-87c0-f12851625545","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer details data from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or higher.\", \"Join the filtered feedback data with the customer details data on the customer ID.\", \"Aggregate the joined data to calculate the average feedback rating per customer.\", \"Sort the aggregated data by average feedback rating in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"Contains customer feedback including ratings and comments.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer feedback rating on a scale of 1 to 5.\"}, {\"column\": \"column_3_id\", \"column_name\": \"comments\", \"column_type\": \"xsd:string\", \"column_description\": \"Additional comments provided by the customer.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details Data\", \"description\": \"Contains detailed information about customers including their contact details.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"email\", \"column_type\": \"xsd:string\", \"column_description\": \"Email address of the customer.\"}]}, \"table_id3\": {\"name\": \"product_sales\", \"location\": \"C:/data/product_sales.csv\", \"sheet_name\": null, \"label\": \"Product Sales Data\", \"description\": \"Contains sales data for various products, not relevant to customer feedback analysis.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"quantity_sold\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Number of units sold.\"}, {\"column\": \"column_3_id\", \"column_name\": \"sale_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the sale.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading customer details data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback data to include only ratings of 4 or higher.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"positive_feedback\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered feedback with customer details on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Calculating average feedback rating per customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_rating\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by average feedback rating in descending order.\"}]"}]}
{"id":"4cf70b07-3616-4bb7-ae36-1ae84b468de0","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file.\", \"Load the customer transaction data from the Excel file.\", \"Filter the feedback data to include only feedback with a rating of 4 or higher.\", \"Join the filtered feedback data with the transaction data on the customer ID.\", \"Calculate a new column in the joined table for the total transaction amount by multiplying quantity and price.\", \"Aggregate the joined table to find the average transaction amount per customer.\", \"Sort the aggregated data by average transaction amount in descending order.\", \"Identify the top 10 customers based on the average transaction amount.\", \"Load the customer demographic data from another Excel sheet.\", \"Join the top 10 customers data with the demographic data to enrich the customer profiles.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"Contains customer feedback with ratings and comments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating from 1 to 5.\"}, {\"column\": \"col_3\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Customer feedback comments.\"}]}, \"table_id2\": {\"name\": \"customer_transactions\", \"location\": \"C:/data/customer_transactions.xlsx\", \"sheet_name\": \"Transactions\", \"label\": \"Customer Transaction Data\", \"description\": \"Contains transaction details including quantity and price.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of items purchased.\"}, {\"column\": \"col_3\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price per item.\"}]}, \"table_id3\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.xlsx\", \"sheet_name\": \"Demographics\", \"label\": \"Customer Demographic Data\", \"description\": \"Contains demographic information about customers.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"Location of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_transactions.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_transactions\", \"comment\": \"Loading customer transaction data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback data to include only feedback with a rating of 4 or higher.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"positive_feedback\", \"customer_transactions\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered feedback data with transaction data on customer ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"total_transaction_amount\", \"formula\": \"quantity * price\"}, \"output\": \"joined_data_with_total\", \"comment\": \"Calculating total transaction amount by multiplying quantity and price.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_total\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"total_transaction_amount\", \"name\": \"average_transaction_amount\", \"function\": \"mean\"}]}, \"output\": \"average_transaction_per_customer\", \"comment\": \"Aggregating data to find the average transaction amount per customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_transaction_per_customer\", \"sortBy\": \"average_transaction_amount\", \"order\": \"desc\"}, \"output\": \"sorted_customers\", \"comment\": \"Sorting data by average transaction amount in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_customers\", \"query\": \"index < 10\"}, \"output\": \"top_10_customers\", \"comment\": \"Identifying the top 10 customers based on average transaction amount.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographic data from an Excel sheet.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"top_10_customers\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"enriched_customer_profiles\", \"comment\": \"Joining top 10 customers data with demographic data to enrich customer profiles.\"}]"}]}
{"id":"4566747a-5df6-4bff-a554-63e7503893ec","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at 'C:/data/sales_data.csv'.\", \"Filter the sales data to include only records from the year 2023.\", \"Perform a time-series forecast on the 'sales_amount' column for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains historical sales data including sales amount and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales record.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of sales made on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The identifier for the product sold.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and demographics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from the specified CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"}, \"output\": \"sales_data_2023\", \"comment\": \"Filtering the sales data to include only records from the year 2023.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data_2023\", \"columns\": [\"sales_amount\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"sales_forecast\", \"comment\": \"Performing a time-series forecast on the 'sales_amount' column for the next 30 days using the Holt-Winters method.\"}]"}]}
{"id":"3a23a516-2e15-4e30-8d88-25e34bb72470","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with the customer data on the customer_id column.\", \"Aggregate the joined data by region to calculate the total sales revenue for each region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including total revenue and customer information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer details including region and customer_id.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"This table contains product details including product_id and price.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`total_revenue` > 1000\"}, \"output\": \"filtered_sales_data\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_data.csv\", \"type\": \"csv\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales_data\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data based on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to compute total revenue.\"}]"}]}
{"id":"4c7f224d-896f-45a9-b5be-3501baa18f4a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics data from the CSV file.\", \"Load the purchase history data from the Excel file.\", \"Filter the purchase history to include only transactions from the last year.\", \"Join the filtered purchase history with customer demographics based on customer ID.\", \"Calculate the total spending for each customer.\", \"Segment customers into high, medium, and low spenders based on total spending.\", \"Aggregate the data to find the average spending in each segment.\", \"Sort the segments by average spending in descending order.\", \"Save the sorted segments to a new CSV file for further analysis.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information about customers, including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"purchase_history\", \"location\": \"C:/data/purchase_history.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Purchase History\", \"description\": \"Records of customer purchases, including transaction dates and amounts.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier linking the transaction to a customer.\"}, {\"column\": \"col_3\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the transaction occurred.\"}, {\"column\": \"col_4\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount spent in the transaction.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"data_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for irrelevant data.\"}, {\"column\": \"col_2\", \"column_name\": \"info\", \"column_type\": \"xsd:string\", \"column_description\": \"Additional information.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/purchase_history.xlsx\", \"type\": \"xlsx\"}, \"output\": \"purchase_history\", \"comment\": \"Loading purchase history data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"purchase_history\", \"query\": \"`transaction_date` >= '2022-01-01'\"}, \"output\": \"filtered_purchase_history\", \"comment\": \"Filtering purchase history to include only transactions from the last year.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_purchase_history\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered purchase history with customer demographics based on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_spending\", \"function\": \"sum\"}]}, \"output\": \"customer_spending\", \"comment\": \"Calculating the total spending for each customer.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"customer_spending\", \"transformations\": [{\"columns\": [\"total_spending\"], \"operation\": \"min_max_scale\", \"parameters\": {\"range\": [0, 1]}}]}, \"output\": \"scaled_spending\", \"comment\": \"Scaling total spending to segment customers into high, medium, and low spenders.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"scaled_spending\", \"groupBy\": \"spending_segment\", \"aggregations\": [{\"column\": \"total_spending\", \"name\": \"average_spending\", \"function\": \"mean\"}]}, \"output\": \"segment_averages\", \"comment\": \"Aggregating data to find the average spending in each segment.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"segment_averages\", \"sortBy\": \"average_spending\", \"order\": \"desc\"}, \"output\": \"sorted_segments\", \"comment\": \"Sorting the segments by average spending in descending order.\"}]"}]}
{"id":"8eac8eb4-34d2-49ac-926e-eeb9f0daf28c","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table which contains information about suppliers and their sustainability ratings.\", \"Load the 'product_data' table which contains details about products and their associated suppliers.\", \"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\", \"Join the filtered supplier data with the 'product_data' on the supplier ID.\", \"Aggregate the joined data to calculate the total number of sustainable products per supplier.\", \"Sort the aggregated data by the total number of sustainable products in descending order.\", \"Calculate a new column 'sustainability_score' by multiplying the sustainability rating by the number of sustainable products.\", \"Filter the data to include only suppliers with a 'sustainability_score' above 500.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers, including their sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability rating of the supplier, ranging from 0 to 100.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"Contains details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}, \"table_id3\": {\"name\": \"transport_data\", \"location\": \"C:/data/transport_data.csv\", \"sheet_name\": null, \"label\": \"Transport Data\", \"description\": \"Contains information about transportation methods and emissions.\", \"columns\": [{\"column\": \"transport_id\", \"column_name\": \"TRANSPORT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transport method.\"}, {\"column\": \"emissions\", \"column_name\": \"EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions associated with the transport method.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data containing sustainability ratings.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data with associated suppliers.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_sustainable_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate total sustainable products per supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_sustainable_products\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting data by total number of sustainable products in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_data\", \"columnName\": \"sustainability_score\", \"formula\": \"SUSTAINABILITY_RATING * total_sustainable_products\"}, \"output\": \"data_with_score\", \"comment\": \"Calculating sustainability score by multiplying rating by number of sustainable products.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"data_with_score\", \"query\": \"sustainability_score > 500\"}, \"output\": \"high_score_suppliers\", \"comment\": \"Filtering suppliers with a sustainability score above 500.\"}]"}]}
{"id":"304dc9ac-d7ee-4bb6-9ce0-992aa1faf830","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Load the market research data from the Excel sheet.\", \"Filter the market research data to include only entries from the last year.\", \"Join the product specifications with the filtered market research data on the product ID.\", \"Calculate the potential revenue by multiplying the estimated sales volume by the product price.\", \"Sort the resulting table by potential revenue in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"Contains detailed specifications of each product including product ID, name, and price.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}, \"table_id2\": {\"name\": \"market_research\", \"location\": \"C:/data/market_research.xlsx\", \"sheet_name\": \"2023_data\", \"label\": \"Market Research Data\", \"description\": \"Contains market research data including product ID, estimated sales volume, and date of research.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"estimated_sales_volume\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Estimated sales volume for the product.\"}, {\"column\": \"col_3\", \"column_name\": \"research_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the market research was conducted.\"}]}, \"table_id3\": {\"name\": \"historical_sales\", \"location\": \"C:/data/historical_sales.csv\", \"sheet_name\": null, \"label\": \"Historical Sales Data\", \"description\": \"Contains historical sales data for various products, not relevant for this workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_volume\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Recorded sales volume for the product.\"}, {\"column\": \"col_3\", \"column_name\": \"sales_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the sales record.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading the product specifications from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/market_research.xlsx\", \"type\": \"xlsx\"}, \"output\": \"market_research\", \"comment\": \"Loading the market research data from an Excel sheet.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"market_research\", \"query\": \"`research_date` >= '2022-01-01'\"}, \"output\": \"filtered_market_research\", \"comment\": \"Filtering market research data to include only entries from the last year.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"product_specifications\", \"filtered_market_research\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining product specifications with filtered market research data on product ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"potential_revenue\", \"formula\": \"`estimated_sales_volume` * `price`\"}, \"output\": \"data_with_revenue\", \"comment\": \"Calculating potential revenue by multiplying estimated sales volume by product price.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"data_with_revenue\", \"sortBy\": \"potential_revenue\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the resulting table by potential revenue in descending order.\"}]"}]}
{"id":"0c7b7e76-9403-4300-b0a5-e30fc92f9aad","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table containing supplier details and sustainability scores.\", \"Load the 'product_data' table with product information and associated supplier IDs.\", \"Filter the 'supplier_data' table to include only suppliers with a sustainability score above 80.\", \"Join the filtered supplier data with the 'product_data' table on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each supplier.\", \"Sort the aggregated data by the total number of products in descending order.\", \"Calculate a new column 'sustainability_impact' by multiplying the sustainability score by the total number of products.\", \"Filter the data to include only suppliers with a 'sustainability_impact' greater than 5000.\", \"Join the filtered data with 'logistics_data' to include transportation emissions.\", \"Aggregate the final data to calculate the average transportation emissions per supplier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains details about suppliers including their sustainability scores.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_score\", \"column_name\": \"SUSTAINABILITY_SCORE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Score representing the sustainability of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"Contains product information and associated supplier IDs.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Data\", \"description\": \"Contains data on transportation emissions for each supplier.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier.\"}, {\"column\": \"transport_emissions\", \"column_name\": \"TRANSPORT_EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions from transportation for each supplier.\"}]}, \"table_id4\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"irrelevant_column\", \"column_name\": \"IRRELEVANT_COLUMN\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data containing sustainability scores.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data with supplier IDs.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_SCORE > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with sustainability scores above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_supplier_product_data\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_supplier_product_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_supplier_data\", \"comment\": \"Aggregating data to calculate total number of products per supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_supplier_data\", \"sortBy\": \"total_products\", \"order\": \"desc\"}, \"output\": \"sorted_supplier_data\", \"comment\": \"Sorting suppliers by total number of products in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_supplier_data\", \"columnName\": \"sustainability_impact\", \"formula\": \"SUSTAINABILITY_SCORE * total_products\"}, \"output\": \"supplier_data_with_impact\", \"comment\": \"Calculating sustainability impact by multiplying score by total products.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data_with_impact\", \"query\": \"sustainability_impact > 5000\"}, \"output\": \"high_impact_suppliers\", \"comment\": \"Filtering suppliers with sustainability impact greater than 5000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics_data.csv\", \"type\": \"csv\"}, \"output\": \"logistics_data\", \"comment\": \"Loading logistics data to include transportation emissions.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_impact_suppliers\", \"logistics_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"final_supplier_data\", \"comment\": \"Joining high impact suppliers with logistics data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"final_supplier_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"TRANSPORT_EMISSIONS\", \"name\": \"average_emissions\", \"function\": \"mean\"}]}, \"output\": \"average_emissions_per_supplier\", \"comment\": \"Calculating average transportation emissions per supplier.\"}]"}]}
{"id":"59fcdec7-79e3-40b0-a424-65679d8a696f","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\", \"Filter the employee data to include only those with a salary greater than 50,000.\", \"Join the filtered employee data with the department data on the department_id column.\", \"Aggregate the joined data to calculate the average salary per department.\", \"Sort the aggregated data by average salary in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_data\", \"location\": \"C:/data/hr/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"Contains information about employees including their salaries and department IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Salary of the employee.\"}, {\"column\": \"col_4\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_id2\": {\"name\": \"department_data\", \"location\": \"C:/data/hr/department_data.csv\", \"sheet_name\": null, \"label\": \"Department Data\", \"description\": \"Contains information about departments including their IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the department.\"}]}, \"table_id3\": {\"name\": \"project_data\", \"location\": \"C:/data/hr/project_data.csv\", \"sheet_name\": null, \"label\": \"Project Data\", \"description\": \"Contains information about projects including project IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"project_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each project.\"}, {\"column\": \"col_2\", \"column_name\": \"project_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the project.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/hr/employee_data.csv\", \"type\": \"csv\"}, \"output\": \"employee_data\", \"comment\": \"Loading the employee data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_data\", \"query\": \"salary > 50000\"}, \"output\": \"filtered_employee_data\", \"comment\": \"Filtering employees with a salary greater than 50,000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/hr/department_data.csv\", \"type\": \"csv\"}, \"output\": \"department_data\", \"comment\": \"Loading the department data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_employee_data\", \"department_data\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered employee data with department data on department_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"department_name\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Calculating the average salary per department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_salary\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting the aggregated data by average salary in descending order.\"}]"}]}
{"id":"ed49b4d1-4869-48bb-990f-198a5453103a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table which contains information about suppliers and their sustainability ratings.\", \"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\", \"Aggregate the filtered data to calculate the average sustainability rating by region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supply_chain/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"This table contains information about suppliers, including their sustainability ratings and regional data.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"column_2_id\", \"column_name\": \"sustainability_rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability rating of the supplier on a scale from 0 to 100.\"}, {\"column\": \"column_3_id\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the supplier is located.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/supply_chain/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"This table contains information about products and their suppliers.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supply_chain/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading the supplier data table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"sustainability_rating > 80\"}, \"output\": \"filtered_suppliers\", \"comment\": \"Filtering suppliers with a sustainability rating above 80.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_suppliers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"sustainability_rating\", \"name\": \"average_sustainability_rating\", \"function\": \"mean\"}]}, \"output\": \"average_sustainability_by_region\", \"comment\": \"Calculating the average sustainability rating by region.\"}]"}]}
{"id":"54f6c6bb-ee9a-4e97-8fe9-71a8b3d6e016","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'shipment_data' table from the CSV file.\", \"Load the 'warehouse_data' table from the Excel file.\", \"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column.\", \"Aggregate the joined data to calculate the total shipment weight per warehouse.\", \"Sort the aggregated data by total shipment weight in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"Contains information about shipments including weight, destination, and warehouse ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_3\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the warehouse associated with the shipment.\"}]}, \"table_id2\": {\"name\": \"warehouse_data\", \"location\": \"C:/data/logistics/warehouse_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Warehouse Data\", \"description\": \"Contains information about warehouses including their location and capacity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each warehouse.\"}, {\"column\": \"col_2\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"Location of the warehouse.\"}, {\"column\": \"col_3\", \"column_name\": \"capacity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Maximum capacity of the warehouse in kilograms.\"}]}, \"table_id3\": {\"name\": \"driver_data\", \"location\": \"C:/data/logistics/driver_data.csv\", \"sheet_name\": null, \"label\": \"Driver Data\", \"description\": \"Contains information about drivers including their ID and assigned routes.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"driver_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each driver.\"}, {\"column\": \"col_2\", \"column_name\": \"route\", \"column_type\": \"xsd:string\", \"column_description\": \"Assigned route for the driver.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading shipment data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/warehouse_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"warehouse_data\", \"comment\": \"Loading warehouse data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 1000\"}, \"output\": \"filtered_shipment_data\", \"comment\": \"Filtering shipments to include only those with a weight greater than 1000 kg.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipment_data\", \"warehouse_data\"], \"joinOn\": [\"warehouse_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with warehouse data on warehouse_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"warehouse_id\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_shipment_weight\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate total shipment weight per warehouse.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_shipment_weight\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total shipment weight in descending order.\"}]"}]}
{"id":"bd1775fc-6cc5-4e4b-8203-bf036a3820f7","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales.csv.\", \"Load the customer data from the Excel file located at C:/data/customers.xlsx.\", \"Filter the sales data to include only transactions with a total revenue greater than 1000.\", \"Join the filtered sales data with the customer data on the customer_id column using an inner join.\", \"Calculate a new column in the joined table called profit_margin by subtracting total_cost from total_revenue.\", \"Aggregate the data by region to calculate the total sales and average profit margin.\", \"Sort the aggregated data by total sales in descending order.\", \"Forecast the total sales for the next 30 days using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales\", \"location\": \"C:/data/sales.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including total revenue and total cost.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"column_2_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"column_3_id\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}, {\"column\": \"column_4_id\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost incurred for the transaction.\"}, {\"column\": \"column_5_id\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the transaction took place.\"}, {\"column\": \"column_6_id\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the transaction.\"}]}, \"table_id2\": {\"name\": \"customers\", \"location\": \"C:/data/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and name.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}]}, \"table_id3\": {\"name\": \"inventory\", \"location\": \"C:/data/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory details including product ID and stock levels.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"stock_level\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current stock level of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"total_revenue > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data on customer_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sales_with_customers\", \"columnName\": \"profit_margin\", \"formula\": \"total_revenue - total_cost\"}, \"output\": \"sales_with_profit\", \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_profit\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}, {\"column\": \"profit_margin\", \"name\": \"average_profit_margin\", \"function\": \"mean\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating data by region to calculate total sales and average profit margin.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting aggregated data by total sales in descending order.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data\", \"columns\": [\"total_revenue\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"sales_forecast\", \"comment\": \"Forecasting total sales for the next 30 days using Holt-Winters method.\"}]"}]}
{"id":"955fc62f-b9fd-40e4-a93c-52d4ad10fbce","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Filter the data to include only records where energy usage exceeds 1000 kWh.\", \"Aggregate the filtered data to calculate the total energy consumption per building.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption\", \"description\": \"This table contains energy consumption data for various buildings, including timestamps and energy usage in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_2\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"Timestamp of the energy consumption record.\"}, {\"column\": \"col_3\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Energy usage in kilowatt-hours (kWh).\"}]}, \"table_id2\": {\"name\": \"building_info\", \"location\": \"C:/data/building_info.csv\", \"sheet_name\": null, \"label\": \"Building Information\", \"description\": \"Contains information about buildings, such as address and type.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_2\", \"column_name\": \"address\", \"column_type\": \"xsd:string\", \"column_description\": \"Address of the building.\"}, {\"column\": \"col_3\", \"column_name\": \"building_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of the building, e.g., residential, commercial.\"}]}, \"table_id3\": {\"name\": \"maintenance_schedule\", \"location\": \"C:/data/maintenance_schedule.csv\", \"sheet_name\": null, \"label\": \"Maintenance Schedule\", \"description\": \"Schedule of maintenance activities for buildings.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_2\", \"column_name\": \"maintenance_date\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"Date of the scheduled maintenance.\"}, {\"column\": \"col_3\", \"column_name\": \"activity\", \"column_type\": \"xsd:string\", \"column_description\": \"Description of the maintenance activity.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"energy_usage > 1000\"}, \"output\": \"high_usage_data\", \"comment\": \"Filtering the data to include only records where energy usage exceeds 1000 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"high_usage_data\", \"groupBy\": \"building_id\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy_consumption\", \"function\": \"sum\"}]}, \"output\": \"total_energy_per_building\", \"comment\": \"Aggregating the filtered data to calculate the total energy consumption per building.\"}]"}]}
{"id":"5159791d-130a-4deb-b70a-178323a5fc79","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Apply a time series transformation to set the 'date' column as the index.\", \"Perform sales forecasting using the Holt-Winters algorithm for the next 30 days.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains historical sales data with columns for date, sales amount, and other relevant metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the sales record.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total sales amount for the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The identifier for the product sold.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"Contains customer information including customer ID, name, and contact details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_number\", \"column_type\": \"xsd:string\", \"column_description\": \"The contact number of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"applyTimeSeriesTransformations\", \"parameters\": {\"table\": \"sales_data\", \"transformations\": [{\"columns\": [\"date\"], \"operation\": \"set_index\"}]}, \"output\": \"sales_data_indexed\", \"comment\": \"Setting the 'date' column as the index for time series analysis.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"sales_data_indexed\", \"columns\": [\"sales_amount\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"sales_forecast\", \"comment\": \"Performing sales forecasting using the Holt-Winters algorithm for the next 30 days.\"}]"}]}
{"id":"9ab1ab62-46ec-43d8-8831-8dc678760efe","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains information about the materials used in production.\", \"Load the table 'production_schedule' which contains the schedule for manufacturing.\", \"Filter the 'raw_materials' table to include only materials with a stock level greater than 100.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Calculate a new column 'required_materials' by multiplying 'quantity_needed' by 'production_days' in the joined table.\", \"Aggregate the joined table to find the total 'required_materials' for each 'material_id'.\", \"Sort the aggregated table by 'required_materials' in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials\", \"description\": \"Table containing information about raw materials including their stock levels and IDs.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"stock_level\", \"column_name\": \"STOCK_LEVEL\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"Table containing the production schedule including material requirements and production days.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"quantity_needed\", \"column_name\": \"QUANTITY_NEEDED\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of material needed for production.\"}, {\"column\": \"production_days\", \"column_name\": \"PRODUCTION_DAYS\", \"column_type\": \"xsd:integer\", \"column_description\": \"Number of days the material is needed for production.\"}]}, \"table_id3\": {\"name\": \"employee_data\", \"location\": \"C:/data/manufacturing/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"Table containing information about employees, not relevant to the current workflow.\", \"columns\": [{\"column\": \"employee_id\", \"column_name\": \"EMPLOYEE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"name\", \"column_name\": \"NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"STOCK_LEVEL > 100\"}, \"output\": \"filtered_raw_materials\", \"comment\": \"Filtering raw materials to include only those with stock levels greater than 100.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_raw_materials\", \"production_schedule\"], \"joinOn\": [\"MATERIAL_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered raw materials with production schedule on material_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_table\", \"columnName\": \"required_materials\", \"formula\": \"QUANTITY_NEEDED * PRODUCTION_DAYS\"}, \"output\": \"joined_table_with_requirements\", \"comment\": \"Calculating required materials by multiplying quantity needed by production days.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_table_with_requirements\", \"groupBy\": \"MATERIAL_ID\", \"aggregations\": [{\"column\": \"required_materials\", \"name\": \"total_required_materials\", \"function\": \"sum\"}]}, \"output\": \"aggregated_materials\", \"comment\": \"Aggregating to find total required materials for each material_id.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_materials\", \"sortBy\": \"total_required_materials\", \"order\": \"desc\"}, \"output\": \"sorted_materials\", \"comment\": \"Sorting the aggregated materials by total required materials in descending order.\"}]"}]}
{"id":"9ed3306a-5488-4e48-9c1f-4b3308310d79","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file.\", \"Load the 'exchange_rates' table from the Excel file.\", \"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\", \"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\", \"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\", \"Aggregate the data by 'transaction_type' to find the total 'amount_in_usd' for each type.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"Table containing details of financial transactions including transaction amount and currency code.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"transaction_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"Currency code of the transaction amount.\"}, {\"column\": \"col_4\", \"column_name\": \"transaction_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of the transaction.\"}]}, \"table_id2\": {\"name\": \"exchange_rates\", \"location\": \"C:/data/exchange_rates.xlsx\", \"sheet_name\": \"Rates\", \"label\": \"Exchange Rates\", \"description\": \"Table containing exchange rates for various currencies against USD.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"Currency code for which the exchange rate is provided.\"}, {\"column\": \"col_2\", \"column_name\": \"exchange_rate\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Exchange rate of the currency against USD.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"A table containing data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/exchange_rates.xlsx\", \"type\": \"xlsx\"}, \"output\": \"exchange_rates\", \"comment\": \"Loading the exchange rates dataset from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"transaction_amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"exchange_rates\"], \"joinOn\": [\"currency_code\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with exchange rates on currency code.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"amount_in_usd\", \"formula\": \"transaction_amount * exchange_rate\"}, \"output\": \"transactions_with_usd\", \"comment\": \"Calculating amount in USD by multiplying transaction amount with exchange rate.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"transactions_with_usd\", \"groupBy\": \"transaction_type\", \"aggregations\": [{\"column\": \"amount_in_usd\", \"name\": \"total_amount_usd\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by transaction type to find the total amount in USD for each type.\"}]"}]}
{"id":"2187973d-6121-4ee2-9941-620f8c1fe059","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the production schedule table from the CSV file.\", \"Filter the production schedule to include only high-priority orders.\", \"Join the filtered production schedule with the inventory table on the product ID.\", \"Aggregate the joined table to calculate the total quantity needed for each product.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the production schedule with details about order priorities and product IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"order_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each order.\"}, {\"column\": \"col_2\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product to be manufactured.\"}, {\"column\": \"col_3\", \"column_name\": \"priority\", \"column_type\": \"xsd:string\", \"column_description\": \"Priority level of the order.\"}]}, \"table_id2\": {\"name\": \"inventory\", \"location\": \"C:/data/manufacturing/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory\", \"description\": \"This table contains current inventory levels for each product.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product in inventory.\"}, {\"column\": \"col_2\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current quantity of the product in inventory.\"}]}, \"table_id3\": {\"name\": \"employee_records\", \"location\": \"C:/data/manufacturing/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"Contains records of employees working in the manufacturing unit.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"priority == 'high'\"}, \"output\": \"high_priority_orders\", \"comment\": \"Filtering the production schedule to include only high-priority orders.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/inventory.csv\", \"type\": \"csv\"}, \"output\": \"inventory\", \"comment\": \"Loading the inventory data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_priority_orders\", \"inventory\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining the filtered production schedule with the inventory table on product ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"quantity\", \"name\": \"total_quantity_needed\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating the joined table to calculate the total quantity needed for each product.\"}]"}]}
{"id":"9f604cf7-b2b2-4d0a-a29a-702cc45912ce","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_data.csv.\", \"Filter the sales data to include only transactions where the total revenue is greater than 1000.\", \"Load the customer data from the Excel file located at C:/data/customer_data.xlsx.\", \"Join the filtered sales data with the customer data on the customer_id column.\", \"Aggregate the joined data by region to calculate the total sales revenue for each region.\", \"Sort the aggregated data by total sales revenue in descending order.\", \"Calculate a new column in the sorted data to determine the profit margin by subtracting total cost from total revenue.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions including revenue, cost, and customer ID.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"column_2_id\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue for the transaction.\"}, {\"column\": \"column_3_id\", \"column_name\": \"total_cost\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total cost for the transaction.\"}, {\"column\": \"column_4_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer associated with the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Data\", \"description\": \"This table contains customer information including customer ID and region.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}]}, \"table_id3\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"This table contains product information including product ID and price.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.csv\", \"type\": \"csv\"}, \"output\": \"sales_data\", \"comment\": \"Loading the sales data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"total_revenue > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data based on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"regional_sales\", \"comment\": \"Aggregating sales data by region to compute total revenue.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_sales\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_regional_sales\", \"comment\": \"Sorting the aggregated data by total sales revenue in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_regional_sales\", \"columnName\": \"profit_margin\", \"formula\": \"total_revenue - total_cost\"}, \"output\": \"sales_with_profit_margin\", \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"}]"}]}
{"id":"f42e5dad-0520-47be-a24c-7ce72812a6fd","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback data from the CSV file located at C:/data/customer_feedback.csv.\", \"Filter the feedback data to include only entries where the satisfaction score is below 3.\", \"Aggregate the filtered data to count the number of negative feedbacks per product.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"This table contains customer feedback with satisfaction scores and product information.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"satisfaction_score\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer satisfaction score ranging from 1 to 5.\"}, {\"column\": \"column_4_id\", \"column_name\": \"feedback_text\", \"column_type\": \"xsd:string\", \"column_description\": \"Textual feedback provided by the customer.\"}]}, \"table_id2\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.xlsx\", \"sheet_name\": \"Products\", \"label\": \"Product Catalog\", \"description\": \"This table contains information about products available in the catalog.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading the customer feedback data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"satisfaction_score < 3\"}, \"output\": \"negative_feedback\", \"comment\": \"Filtering feedback data to include only entries with satisfaction scores below 3.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"negative_feedback\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"satisfaction_score\", \"name\": \"negative_feedback_count\", \"function\": \"count\"}]}, \"output\": \"negative_feedback_summary\", \"comment\": \"Aggregating the filtered data to count the number of negative feedbacks per product.\"}]"}]}
{"id":"d03fa531-9896-4274-92a5-5fbfb1ce0bb4","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'supplier_data' table from the CSV file located at C:/data/supplier_data.csv.\", \"Load the 'emissions_data' table from the Excel file located at C:/data/emissions_data.xlsx.\", \"Filter the 'supplier_data' table to include only suppliers with a sustainability score above 80.\", \"Join the filtered 'supplier_data' with the 'emissions_data' on the 'supplier_id' column using an inner join.\", \"Calculate a new column 'total_emissions' in the joined table by summing 'emissions_scope1', 'emissions_scope2', and 'emissions_scope3'.\", \"Aggregate the data by 'region' to calculate the total emissions for each region.\", \"Sort the aggregated data by 'total_emissions' in descending order.\", \"Apply a log transformation to the 'total_emissions' column to stabilize variance.\", \"Filter the transformed data to include only regions with total emissions above a threshold of 1000.\", \"Forecast future emissions for the next 12 months using the 'holt_winters' algorithm on the 'total_emissions' column.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers including their sustainability scores.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"col_2\", \"column_name\": \"sustainability_score\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Score representing the sustainability performance of the supplier.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the supplier is located.\"}]}, \"table_id2\": {\"name\": \"emissions_data\", \"location\": \"C:/data/emissions_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Emissions Data\", \"description\": \"Contains emissions data for each supplier across different scopes.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"supplier_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"col_2\", \"column_name\": \"emissions_scope1\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions data for scope 1.\"}, {\"column\": \"col_3\", \"column_name\": \"emissions_scope2\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions data for scope 2.\"}, {\"column\": \"col_4\", \"column_name\": \"emissions_scope3\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Emissions data for scope 3.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"random_column\", \"column_type\": \"xsd:string\", \"column_description\": \"A random column not used in the workflow.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/emissions_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"emissions_data\", \"comment\": \"Loading emissions data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"sustainability_score > 80\"}, \"output\": \"filtered_supplier_data\", \"comment\": \"Filtering suppliers with a sustainability score above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_supplier_data\", \"emissions_data\"], \"joinOn\": [\"supplier_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with emissions data on supplier_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"total_emissions\", \"formula\": \"emissions_scope1 + emissions_scope2 + emissions_scope3\"}, \"output\": \"data_with_total_emissions\", \"comment\": \"Calculating total emissions by summing emissions across all scopes.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_total_emissions\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"total_emissions\", \"name\": \"total_emissions\", \"function\": \"sum\"}]}, \"output\": \"regional_emissions\", \"comment\": \"Aggregating total emissions by region.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"regional_emissions\", \"sortBy\": \"total_emissions\", \"order\": \"desc\"}, \"output\": \"sorted_regional_emissions\", \"comment\": \"Sorting regions by total emissions in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_regional_emissions\", \"transformations\": [{\"columns\": [\"total_emissions\"], \"operation\": \"log_transform\"}]}, \"output\": \"transformed_emissions\", \"comment\": \"Applying log transformation to total emissions.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"transformed_emissions\", \"query\": \"total_emissions > 1000\"}, \"output\": \"high_emission_regions\", \"comment\": \"Filtering regions with total emissions above 1000.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"high_emission_regions\", \"columns\": [\"total_emissions\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"dateColumn\": \"region\"}}, \"output\": \"emissions_forecast\", \"comment\": \"Forecasting future emissions for the next 12 months using Holt-Winters.\"}]"}]}

{"id":"90ce8a6b-0372-4a67-88ab-0f8331c83d9c","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the raw materials data from the CSV file.\", \"Load the production schedule from the Excel file.\", \"Filter the raw materials data to include only materials with a stock level below the reorder threshold.\", \"Join the filtered raw materials data with the production schedule to identify urgent material needs.\", \"Aggregate the joined data to calculate the total quantity needed for each material.\", \"Sort the aggregated data by material priority in descending order.\", \"Calculate the cost of ordering additional materials based on the unit price and quantity needed.\", \"Apply a transformation to standardize the cost data for comparison.\", \"Forecast the future material needs using the production schedule and historical consumption data.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials Inventory\", \"description\": \"Contains information about raw materials, including stock levels and reorder thresholds.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"Material ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each raw material.\"}, {\"column\": \"stock_level\", \"column_name\": \"Stock Level\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the material.\"}, {\"column\": \"reorder_threshold\", \"column_name\": \"Reorder Threshold\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Stock level at which the material should be reordered.\"}, {\"column\": \"unit_price\", \"column_name\": \"Unit Price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price per unit of the material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Production Schedule\", \"description\": \"Details the production schedule, including material requirements and priorities.\", \"columns\": [{\"column\": \"schedule_id\", \"column_name\": \"Schedule ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each production schedule entry.\"}, {\"column\": \"material_id\", \"column_name\": \"Material ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the material required in the schedule.\"}, {\"column\": \"quantity_needed\", \"column_name\": \"Quantity Needed\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount of material required for production.\"}, {\"column\": \"priority\", \"column_name\": \"Priority\", \"column_type\": \"xsd:integer\", \"column_description\": \"Priority level of the production schedule.\"}]}, \"table_id3\": {\"name\": \"historical_consumption\", \"location\": \"C:/data/manufacturing/historical_consumption.csv\", \"sheet_name\": null, \"label\": \"Historical Material Consumption\", \"description\": \"Records past consumption of materials over time.\", \"columns\": [{\"column\": \"date\", \"column_name\": \"Date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of material consumption.\"}, {\"column\": \"material_id\", \"column_name\": \"Material ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the consumed material.\"}, {\"column\": \"quantity_consumed\", \"column_name\": \"Quantity Consumed\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount of material consumed on the given date.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading raw materials data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"type\": \"xlsx\"}, \"output\": \"production_schedule\", \"comment\": \"Loading production schedule data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"`Stock Level` < `Reorder Threshold`\"}, \"output\": \"low_stock_materials\", \"comment\": \"Filtering raw materials to include only those with stock levels below the reorder threshold.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"low_stock_materials\", \"production_schedule\"], \"joinOn\": [\"Material ID\"], \"joinType\": \"inner\"}, \"output\": \"urgent_material_needs\", \"comment\": \"Joining filtered raw materials with the production schedule to identify urgent material needs.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"urgent_material_needs\", \"groupBy\": \"Material ID\", \"aggregations\": [{\"column\": \"Quantity Needed\", \"name\": \"Total Quantity Needed\", \"function\": \"sum\"}]}, \"output\": \"aggregated_material_needs\", \"comment\": \"Aggregating data to calculate the total quantity needed for each material.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_material_needs\", \"sortBy\": \"Priority\", \"order\": \"desc\"}, \"output\": \"sorted_material_needs\", \"comment\": \"Sorting aggregated data by material priority in descending order.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"sorted_material_needs\", \"columnName\": \"Order Cost\", \"formula\": \"`Total Quantity Needed` * `Unit Price`\"}, \"output\": \"material_costs\", \"comment\": \"Calculating the cost of ordering additional materials based on unit price and quantity needed.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"material_costs\", \"transformations\": [{\"columns\": [\"`Order Cost`\"], \"operation\": \"standardize\", \"parameters\": {}}]}, \"output\": \"standardized_costs\", \"comment\": \"Standardizing the cost data for comparison.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"historical_consumption\", \"columns\": [\"Quantity Consumed\"], \"forecastParameters\": {\"forecastPeriod\": 30, \"frequency\": \"D\", \"dateColumn\": \"Date\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 7, \"confidenceInterval\": 95}}, \"output\": \"future_material_needs\", \"comment\": \"Forecasting future material needs using the production schedule and historical consumption data.\"}]"}]}
{"id":"be490a56-eca7-4f04-9e50-7c2737c422fe","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee data from the CSV file located at C:/data/hr/employee_data.csv.\", \"Load the department data from the Excel file located at C:/data/hr/department_data.xlsx.\", \"Filter the employee data to include only those with a salary greater than 50,000.\", \"Join the filtered employee data with the department data on the department_id column.\", \"Calculate a new column 'annual_bonus' as 10% of the salary for each employee.\", \"Aggregate the joined data to find the total salary and average bonus by department.\", \"Sort the aggregated data by total salary in descending order.\", \"Identify employees who have been with the company for more than 5 years.\", \"Apply a transformation to standardize the salary column.\", \"Forecast the headcount for each department for the next 12 months using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_data\", \"location\": \"C:/data/hr/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"Contains information about employees including their ID, name, department, salary, and years with the company.\", \"columns\": [{\"column\": \"emp_id\", \"column_name\": \"EMPLOYEE_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"name\", \"column_name\": \"NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"department_id\", \"column_name\": \"DEPARTMENT_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Identifier for the department the employee belongs to.\"}, {\"column\": \"salary\", \"column_name\": \"SALARY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Annual salary of the employee.\"}, {\"column\": \"years_with_company\", \"column_name\": \"YEARS_WITH_COMPANY\", \"column_type\": \"xsd:integer\", \"column_description\": \"Number of years the employee has been with the company.\"}]}, \"table_id2\": {\"name\": \"department_data\", \"location\": \"C:/data/hr/department_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Department Data\", \"description\": \"Contains information about departments including department ID and name.\", \"columns\": [{\"column\": \"dept_id\", \"column_name\": \"DEPARTMENT_ID\", \"column_type\": \"xsd:integer\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"dept_name\", \"column_name\": \"DEPARTMENT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the department.\"}]}, \"table_id3\": {\"name\": \"not_relevant_table\", \"location\": \"C:/data/hr/irrelevant_data.xlsx\", \"sheet_name\": \"Irrelevant\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_x\", \"column_name\": \"COLUMN_X\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/hr/employee_data.csv\", \"type\": \"csv\"}, \"output\": \"employee_data\", \"comment\": \"Loading employee data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/hr/department_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"department_data\", \"comment\": \"Loading department data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_data\", \"query\": \"`SALARY` > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employees with a salary greater than 50,000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_salary_employees\", \"department_data\"], \"joinOn\": [\"DEPARTMENT_ID\"], \"joinType\": \"inner\"}, \"output\": \"employee_department_data\", \"comment\": \"Joining filtered employee data with department data on department_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"employee_department_data\", \"columnName\": \"annual_bonus\", \"formula\": \"`SALARY` * 0.10\"}, \"output\": \"employee_data_with_bonus\", \"comment\": \"Calculating annual bonus as 10% of the salary.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"employee_data_with_bonus\", \"groupBy\": \"DEPARTMENT_ID\", \"aggregations\": [{\"column\": \"SALARY\", \"name\": \"total_salary\", \"function\": \"sum\"}, {\"column\": \"annual_bonus\", \"name\": \"average_bonus\", \"function\": \"mean\"}]}, \"output\": \"aggregated_department_data\", \"comment\": \"Aggregating data to find total salary and average bonus by department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_department_data\", \"sortBy\": \"total_salary\", \"order\": \"desc\"}, \"output\": \"sorted_department_data\", \"comment\": \"Sorting departments by total salary in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_data\", \"query\": \"`YEARS_WITH_COMPANY` > 5\"}, \"output\": \"long_term_employees\", \"comment\": \"Identifying employees who have been with the company for more than 5 years.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"employee_data\", \"transformations\": [{\"columns\": [\"SALARY\"], \"operation\": \"standardize\"}]}, \"output\": \"standardized_employee_data\", \"comment\": \"Standardizing the salary column.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"employee_data\", \"columns\": [\"EMPLOYEE_ID\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"dateColumn\": \"EMPLOYEE_ID\"}}, \"output\": \"headcount_forecast\", \"comment\": \"Forecasting headcount for each department for the next 12 months using Holt-Winters method.\"}]"}]}
{"id":"9111c376-7177-48a7-aedc-58cb4c2dc074","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Load the weather data from the Excel file.\", \"Filter the energy consumption data to include only records from the last year.\", \"Join the filtered energy consumption data with the weather data on the date column.\", \"Calculate a new column for energy efficiency by dividing total energy consumption by the number of occupants.\", \"Aggregate the joined data by month to calculate the average energy efficiency.\", \"Sort the aggregated data by energy efficiency in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption\", \"description\": \"This table contains the energy consumption data for the building, including total energy used and number of occupants.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"total_energy\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total energy consumed in kilowatt-hours.\"}, {\"column\": \"col_3\", \"column_name\": \"occupants\", \"column_type\": \"xsd:integer\", \"column_description\": \"The number of occupants in the building.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.xlsx\", \"sheet_name\": \"Weather\", \"label\": \"Weather Data\", \"description\": \"This table contains weather data including temperature and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average temperature on the given date.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average humidity on the given date.\"}]}, \"table_id3\": {\"name\": \"maintenance_schedule\", \"location\": \"C:/data/maintenance_schedule.csv\", \"sheet_name\": null, \"label\": \"Maintenance Schedule\", \"description\": \"This table contains the maintenance schedule for the building, including dates and tasks.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"maintenance_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the scheduled maintenance.\"}, {\"column\": \"col_2\", \"column_name\": \"task\", \"column_type\": \"xsd:string\", \"column_description\": \"The maintenance task to be performed.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`date` >= '2022-01-01'\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering the energy consumption data to include only records from the last year.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_energy_consumption\", \"weather_data\"], \"joinOn\": [\"date\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining the filtered energy consumption data with the weather data on the date column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"energy_efficiency\", \"formula\": \"`total_energy` / `occupants`\"}, \"output\": \"data_with_efficiency\", \"comment\": \"Calculating a new column for energy efficiency by dividing total energy consumption by the number of occupants.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_efficiency\", \"groupBy\": \"date\", \"aggregations\": [{\"column\": \"energy_efficiency\", \"name\": \"avg_energy_efficiency\", \"function\": \"mean\"}]}, \"output\": \"monthly_efficiency\", \"comment\": \"Aggregating the joined data by month to calculate the average energy efficiency.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_efficiency\", \"sortBy\": \"avg_energy_efficiency\", \"order\": \"desc\"}, \"output\": \"sorted_efficiency\", \"comment\": \"Sorting the aggregated data by energy efficiency in descending order.\"}]"}]}
{"id":"83a3e6ce-6909-4563-8d65-6d159ab49b4e","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee records table from the HR database.\", \"Filter the employee records to include only those with a salary greater than $50,000.\", \"Sort the filtered employee records by their hire date in ascending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_records\", \"location\": \"C:/HR_data/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"A table containing detailed records of employees including their salary, hire date, and other personal information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The annual salary of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"hire_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the employee was hired.\"}]}, \"table_id2\": {\"name\": \"department_info\", \"location\": \"C:/HR_data/department_info.csv\", \"sheet_name\": null, \"label\": \"Department Information\", \"description\": \"A table containing information about various departments within the company.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the department.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR_data/employee_records.csv\", \"type\": \"csv\"}, \"output\": \"employee_records\", \"comment\": \"Loading the employee records table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_records\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employee records to include only those with a salary greater than $50,000.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"high_salary_employees\", \"sortBy\": \"hire_date\", \"order\": \"asc\"}, \"output\": \"sorted_high_salary_employees\", \"comment\": \"Sorting the filtered employee records by their hire date in ascending order.\"}]"}]}
{"id":"c766a692-7468-4aee-a852-e9ec72111be9","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'production_schedule' which contains the manufacturing schedule for the upcoming month.\", \"Filter the production schedule to include only high-priority orders.\", \"Aggregate the filtered schedule to calculate the total production time required for each product.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the manufacturing schedule for the upcoming month, including order details and priority levels.\", \"columns\": [{\"column\": \"order_id\", \"column_name\": \"ORDER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each order.\"}, {\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product being manufactured.\"}, {\"column\": \"priority\", \"column_name\": \"PRIORITY\", \"column_type\": \"xsd:string\", \"column_description\": \"Priority level of the order (e.g., high, medium, low).\"}, {\"column\": \"production_time\", \"column_name\": \"PRODUCTION_TIME\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Estimated production time in hours for the order.\"}]}, \"table_id2\": {\"name\": \"inventory_status\", \"location\": \"C:/data/manufacturing/inventory_status.xlsx\", \"sheet_name\": \"Current\", \"label\": \"Current Inventory Status\", \"description\": \"This table provides the current status of inventory items, including quantities and reorder levels.\", \"columns\": [{\"column\": \"item_id\", \"column_name\": \"ITEM_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each inventory item.\"}, {\"column\": \"quantity\", \"column_name\": \"QUANTITY\", \"column_type\": \"xsd:integer\", \"column_description\": \"Current quantity of the item in stock.\"}, {\"column\": \"reorder_level\", \"column_name\": \"REORDER_LEVEL\", \"column_type\": \"xsd:integer\", \"column_description\": \"The stock level at which the item should be reordered.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the manufacturing schedule for the upcoming month from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"PRIORITY == 'high'\"}, \"output\": \"high_priority_orders\", \"comment\": \"Filtering the production schedule to include only high-priority orders.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"high_priority_orders\", \"groupBy\": \"PRODUCT_ID\", \"aggregations\": [{\"column\": \"PRODUCTION_TIME\", \"name\": \"total_production_time\", \"function\": \"sum\"}]}, \"output\": \"aggregated_production_time\", \"comment\": \"Calculating the total production time required for each product.\"}]"}]}
{"id":"ad9949e8-3b5a-4d26-87ce-e3ed7d7e6314","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer data table from the CSV file.\", \"Filter the customer data to include only those with a lifetime value greater than 1000.\", \"Aggregate the filtered data to find the average lifetime value by region.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_data\", \"location\": \"C:/data/marketing/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains information about customers, including their lifetime value and region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"lifetime_value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total value a customer has contributed over their lifetime.\"}, {\"column\": \"col_3\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"The geographical region of the customer.\"}]}, \"table_id2\": {\"name\": \"sales_data\", \"location\": \"C:/data/marketing/sales_data.csv\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"This table contains sales transactions, including product details and sales amount.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the product sold.\"}, {\"column\": \"col_3\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money generated from the sale.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/customer_data.csv\", \"type\": \"csv\"}, \"output\": \"customer_data\", \"comment\": \"Loading the customer data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_data\", \"query\": \"lifetime_value > 1000\"}, \"output\": \"high_value_customers\", \"comment\": \"Filtering customer data to include only those with a lifetime value greater than 1000.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"high_value_customers\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"lifetime_value\", \"name\": \"average_lifetime_value\", \"function\": \"mean\"}]}, \"output\": \"average_lifetime_value_by_region\", \"comment\": \"Aggregating the filtered data to find the average lifetime value by region.\"}]"}]}
{"id":"e144f2b8-50ba-4d3e-802e-279b922beaec","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'production_schedule' which contains the daily production targets.\", \"Filter the production schedule to include only entries for the current month.\", \"Load the table 'machine_status' which contains the operational status of machines.\", \"Join the filtered production schedule with machine status data on the machine ID.\", \"Aggregate the joined data to calculate the total production target for each machine.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains daily production targets for each machine in the manufacturing plant.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date for which the production target is set.\"}, {\"column\": \"col_2\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each machine.\"}, {\"column\": \"col_3\", \"column_name\": \"production_target\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The production target for the machine on the given date.\"}]}, \"table_id2\": {\"name\": \"machine_status\", \"location\": \"C:/data/manufacturing/machine_status.csv\", \"sheet_name\": null, \"label\": \"Machine Status\", \"description\": \"This table contains the operational status of machines in the manufacturing plant.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each machine.\"}, {\"column\": \"col_2\", \"column_name\": \"status\", \"column_type\": \"xsd:string\", \"column_description\": \"The operational status of the machine (e.g., operational, maintenance, offline).\"}]}, \"table_id3\": {\"name\": \"inventory_data\", \"location\": \"C:/data/manufacturing/inventory_data.csv\", \"sheet_name\": null, \"label\": \"Inventory Data\", \"description\": \"This table contains inventory levels for raw materials.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each raw material.\"}, {\"column\": \"col_2\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The quantity of the raw material available in inventory.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule containing daily production targets.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"`date`.dt.month == pd.Timestamp('now').month\"}, \"output\": \"current_month_schedule\", \"comment\": \"Filtering the production schedule to include only entries for the current month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/machine_status.csv\", \"type\": \"csv\"}, \"output\": \"machine_status\", \"comment\": \"Loading the machine status data to check operational status of machines.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"current_month_schedule\", \"machine_status\"], \"joinOn\": [\"machine_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining the filtered production schedule with machine status data on machine ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"machine_id\", \"aggregations\": [{\"column\": \"production_target\", \"name\": \"total_production_target\", \"function\": \"sum\"}]}, \"output\": \"aggregated_production_targets\", \"comment\": \"Aggregating the joined data to calculate the total production target for each machine.\"}]"}]}
{"id":"81706fbe-0072-48a3-9ff0-0878d23cea37","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains details of materials used in manufacturing.\", \"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\", \"Load the table 'production_schedule' which contains the schedule for manufacturing processes.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Aggregate the joined table to calculate the total quantity of materials required for each production line.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials\", \"description\": \"A table containing details of raw materials used in the manufacturing process, including material ID, name, and quantity.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"material_name\", \"column_name\": \"MATERIAL_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the material.\"}, {\"column\": \"quantity\", \"column_name\": \"QUANTITY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of the material available.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Production Schedule\", \"description\": \"A table containing the schedule for manufacturing processes, including material ID and production line.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"production_line\", \"column_name\": \"PRODUCTION_LINE\", \"column_type\": \"xsd:string\", \"column_description\": \"The production line where the material is used.\"}, {\"column\": \"scheduled_date\", \"column_name\": \"SCHEDULED_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the material is scheduled for use.\"}]}, \"table_id3\": {\"name\": \"inventory\", \"location\": \"C:/data/manufacturing/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory\", \"description\": \"A table containing inventory details, not relevant to the current workflow.\", \"columns\": [{\"column\": \"inventory_id\", \"column_name\": \"INVENTORY_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each inventory item.\"}, {\"column\": \"item_name\", \"column_name\": \"ITEM_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the inventory item.\"}, {\"column\": \"stock_level\", \"column_name\": \"STOCK_LEVEL\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Current stock level of the item.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"QUANTITY > 100\"}, \"output\": \"filtered_raw_materials\", \"comment\": \"Filtering raw materials to include only those with a quantity greater than 100.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.xlsx\", \"type\": \"xlsx\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule dataset from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_raw_materials\", \"production_schedule\"], \"joinOn\": [\"MATERIAL_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered raw materials with the production schedule on material ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_table\", \"groupBy\": \"PRODUCTION_LINE\", \"aggregations\": [{\"column\": \"QUANTITY\", \"name\": \"total_quantity\", \"function\": \"sum\"}]}, \"output\": \"aggregated_materials\", \"comment\": \"Aggregating the joined table to calculate the total quantity of materials required for each production line.\"}]"}]}
{"id":"deae5b5a-87fe-4ffa-9af9-bab73400aa7d","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\", \"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\", \"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\", \"Join the filtered transactions with the 'exchange_rates' table on the 'currency_code' column.\", \"Calculate a new column 'amount_in_usd' by multiplying 'transaction_amount' with 'exchange_rate'.\", \"Aggregate the joined table to find the total 'amount_in_usd' for each 'transaction_type'.\", \"Sort the aggregated data by 'total_amount_usd' in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transactions with details such as transaction amount, currency code, and transaction type.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"transaction_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code of the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"transaction_type\", \"column_type\": \"xsd:string\", \"column_description\": \"The type of transaction, such as purchase or refund.\"}]}, \"table_id2\": {\"name\": \"exchange_rates\", \"location\": \"C:/data/exchange_rates.xlsx\", \"sheet_name\": \"Rates\", \"label\": \"Exchange Rates\", \"description\": \"A table containing exchange rates for different currencies against USD.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code.\"}, {\"column\": \"col_2\", \"column_name\": \"exchange_rate\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The exchange rate of the currency against USD.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"A table containing data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/exchange_rates.xlsx\", \"type\": \"xlsx\"}, \"output\": \"exchange_rates\", \"comment\": \"Loading the exchange rates dataset from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"transaction_amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"exchange_rates\"], \"joinOn\": [\"currency_code\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with exchange rates on currency code.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"amount_in_usd\", \"formula\": \"transaction_amount * exchange_rate\"}, \"output\": \"transactions_with_usd\", \"comment\": \"Calculating amount in USD by multiplying transaction amount with exchange rate.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"transactions_with_usd\", \"groupBy\": \"transaction_type\", \"aggregations\": [{\"column\": \"amount_in_usd\", \"name\": \"total_amount_usd\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to find total amount in USD for each transaction type.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_amount_usd\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by total amount in USD in descending order.\"}]"}]}
{"id":"1a26b25d-502c-471d-a6dc-79e1784a6cbc","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'raw_materials' which contains information about the materials used in manufacturing.\", \"Load the table 'production_schedule' which contains the schedule for manufacturing different products.\", \"Filter the 'raw_materials' table to include only materials with a quantity greater than 100.\", \"Join the filtered 'raw_materials' table with the 'production_schedule' table on the 'material_id' column.\", \"Calculate a new column 'total_cost' in the joined table by multiplying 'quantity' with 'unit_price'.\", \"Aggregate the data by 'product_id' to find the total cost of materials for each product.\", \"Sort the aggregated data by 'total_cost' in descending order to identify the most expensive products to manufacture.\", \"Save the sorted data to a new table 'cost_analysis'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"raw_materials\", \"location\": \"C:/data/manufacturing/raw_materials.csv\", \"sheet_name\": null, \"label\": \"Raw Materials\", \"description\": \"This table contains information about the raw materials used in the manufacturing process, including material ID, name, quantity, and unit price.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each material.\"}, {\"column\": \"column_2_id\", \"column_name\": \"material_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the material.\"}, {\"column\": \"column_3_id\", \"column_name\": \"quantity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Quantity of the material available.\"}, {\"column\": \"column_4_id\", \"column_name\": \"unit_price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price per unit of the material.\"}]}, \"table_id2\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the schedule for manufacturing different products, including product ID, material ID, and scheduled production date.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"column_2_id\", \"column_name\": \"material_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the material used in the product.\"}, {\"column\": \"column_3_id\", \"column_name\": \"scheduled_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date when the product is scheduled for production.\"}]}, \"table_id3\": {\"name\": \"employee_data\", \"location\": \"C:/data/manufacturing/employee_data.csv\", \"sheet_name\": null, \"label\": \"Employee Data\", \"description\": \"This table contains information about employees involved in the manufacturing process, including employee ID, name, and role.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"column_2_id\", \"column_name\": \"employee_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the employee.\"}, {\"column\": \"column_3_id\", \"column_name\": \"role\", \"column_type\": \"xsd:string\", \"column_description\": \"Role of the employee in the manufacturing process.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/raw_materials.csv\", \"type\": \"csv\"}, \"output\": \"raw_materials\", \"comment\": \"Loading the raw materials dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"raw_materials\", \"query\": \"quantity > 100\"}, \"output\": \"filtered_raw_materials\", \"comment\": \"Filtering raw materials to include only those with a quantity greater than 100.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_raw_materials\", \"production_schedule\"], \"joinOn\": [\"material_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered raw materials with production schedule on material_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"total_cost\", \"formula\": \"quantity * unit_price\"}, \"output\": \"joined_data_with_cost\", \"comment\": \"Calculating total cost by multiplying quantity with unit price.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_cost\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"total_cost\", \"name\": \"total_material_cost\", \"function\": \"sum\"}]}, \"output\": \"aggregated_costs\", \"comment\": \"Aggregating data by product_id to find the total cost of materials for each product.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_costs\", \"sortBy\": \"total_material_cost\", \"order\": \"desc\"}, \"output\": \"sorted_costs\", \"comment\": \"Sorting the aggregated data by total cost in descending order.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/cost_analysis.csv\", \"type\": \"csv\"}, \"output\": \"cost_analysis\", \"comment\": \"Saving the sorted data to a new table 'cost_analysis'.\"}]"}]}
{"id":"386655a6-3573-4da3-a53a-0f549b396a4d","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the financial transactions table from the CSV file.\", \"Filter the transactions to include only those with an amount greater than 1000.\", \"Join the filtered transactions with the customer details table on the customer ID.\", \"Aggregate the joined data to calculate the total transaction amount per customer.\"]\n\n                ### Context:\n                {\"table_1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/finance/transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transactions with columns for transaction ID, customer ID, amount, and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}, {\"column\": \"col_4\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the transaction occurred.\"}]}, \"table_2\": {\"name\": \"customer_details\", \"location\": \"C:/data/finance/customers.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"A table containing customer details with columns for customer ID, name, and contact information.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_info\", \"column_type\": \"xsd:string\", \"column_description\": \"Contact information for the customer.\"}]}, \"table_3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/finance/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"A table with irrelevant data not used in the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/finance/customers.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading the customer details table from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered transactions with customer details on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_transaction_amount\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating joined data to calculate the total transaction amount per customer.\"}]"}]}
{"id":"b983fe7b-ca71-4496-8ebd-fb7846139a56","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file.\", \"Load the customer details table from the Excel file.\", \"Filter the feedback table to include only feedback with a rating of 4 or above.\", \"Join the filtered feedback with the customer details on customer ID.\", \"Aggregate the joined table to calculate the average rating per customer.\", \"Sort the aggregated data by average rating in descending order.\", \"Identify the top 10 customers based on average rating.\", \"Calculate a new column to determine the difference between the highest and lowest ratings for each customer.\", \"Save the final table with top customers and their rating differences to a new CSV file.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"Table containing customer feedback with ratings and comments.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Rating given by the customer, ranging from 1 to 5.\"}, {\"column\": \"column_3_id\", \"column_name\": \"feedback\", \"column_type\": \"xsd:string\", \"column_description\": \"Textual feedback provided by the customer.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"Table containing detailed information about customers.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"email\", \"column_type\": \"xsd:string\", \"column_description\": \"Email address of the customer.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table is not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading customer details data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"filtered_feedback\", \"comment\": \"Filtering feedback to include only ratings of 4 or above.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_feedback\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered feedback with customer details on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Calculating the average rating per customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"average_rating\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting data by average rating in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_data\", \"query\": \"index < 10\"}, \"output\": \"top_customers\", \"comment\": \"Identifying the top 10 customers based on average rating.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"top_customers\", \"columnName\": \"rating_difference\", \"formula\": \"`average_rating` - `average_rating`\"}, \"output\": \"top_customers_with_difference\", \"comment\": \"Calculating the difference between highest and lowest ratings for each customer.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/top_customers.csv\", \"type\": \"csv\"}, \"output\": \"top_customers_with_difference\", \"comment\": \"Saving the final table with top customers and their rating differences to a new CSV file.\"}]"}]}
{"id":"592f53bf-0f45-41ed-a377-a140de44ebfc","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee details table from the HR database.\", \"Filter the employees who have been with the company for more than 5 years.\", \"Join the filtered employee data with the salary information table.\", \"Calculate the average salary for employees in each department.\", \"Sort the departments by average salary in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_details\", \"location\": \"C:/HR/employee_details.csv\", \"sheet_name\": null, \"label\": \"Employee Details\", \"description\": \"Contains information about employees, including their start date and department.\", \"columns\": [{\"column\": \"emp_id\", \"column_name\": \"EMP_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"start_date\", \"column_name\": \"START_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the employee started working at the company.\"}, {\"column\": \"department\", \"column_name\": \"DEPARTMENT\", \"column_type\": \"xsd:string\", \"column_description\": \"The department in which the employee works.\"}]}, \"table_id2\": {\"name\": \"salary_information\", \"location\": \"C:/HR/salary_information.xlsx\", \"sheet_name\": \"Salaries\", \"label\": \"Salary Information\", \"description\": \"Contains salary details for employees, including their employee ID and salary amount.\", \"columns\": [{\"column\": \"emp_id\", \"column_name\": \"EMP_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"salary\", \"column_name\": \"SALARY\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The salary amount for the employee.\"}]}, \"table_id3\": {\"name\": \"employee_attendance\", \"location\": \"C:/HR/employee_attendance.csv\", \"sheet_name\": null, \"label\": \"Employee Attendance\", \"description\": \"Contains attendance records for employees, not relevant for the current workflow.\", \"columns\": [{\"column\": \"emp_id\", \"column_name\": \"EMP_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"attendance_date\", \"column_name\": \"ATTENDANCE_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of attendance record.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR/employee_details.csv\", \"type\": \"csv\"}, \"output\": \"employee_details\", \"comment\": \"Loading the employee details table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_details\", \"query\": \"(`START_DATE` <= '2018-10-01')\"}, \"output\": \"long_term_employees\", \"comment\": \"Filtering employees who have been with the company for more than 5 years.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR/salary_information.xlsx\", \"type\": \"xlsx\"}, \"output\": \"salary_information\", \"comment\": \"Loading the salary information table from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"long_term_employees\", \"salary_information\"], \"joinOn\": [\"EMP_ID\"], \"joinType\": \"inner\"}, \"output\": \"employee_salary_data\", \"comment\": \"Joining filtered employee data with salary information based on employee ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"employee_salary_data\", \"groupBy\": \"DEPARTMENT\", \"aggregations\": [{\"column\": \"SALARY\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"department_salary_averages\", \"comment\": \"Calculating the average salary for employees in each department.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"department_salary_averages\", \"sortBy\": \"average_salary\", \"order\": \"desc\"}, \"output\": \"sorted_department_salaries\", \"comment\": \"Sorting the departments by average salary in descending order.\"}]"}]}
{"id":"04e35ab6-59f5-4f2f-8cb3-c7bfa104e58d","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file located at C:/data/customer_feedback.csv.\", \"Filter the feedback data to include only entries with a rating of 4 or 5.\", \"Aggregate the filtered feedback data to count the number of positive feedbacks per product.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback Data\", \"description\": \"This table contains customer feedback including ratings and comments for various products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_3\", \"column_name\": \"rating\", \"column_type\": \"xsd:integer\", \"column_description\": \"Customer rating for the product, ranging from 1 to 5.\"}, {\"column\": \"col_4\", \"column_name\": \"feedback\", \"column_type\": \"xsd:string\", \"column_description\": \"Textual feedback provided by the customer.\"}]}, \"table_id2\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.xlsx\", \"sheet_name\": \"Products\", \"label\": \"Product Catalog\", \"description\": \"A comprehensive list of products available in the store.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating >= 4\"}, \"output\": \"positive_feedback\", \"comment\": \"Filtering feedback data to include only positive ratings (4 or 5).\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"positive_feedback\", \"groupBy\": \"product_id\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"positive_feedback_count\", \"function\": \"count\"}]}, \"output\": \"positive_feedback_summary\", \"comment\": \"Aggregating data to count the number of positive feedbacks per product.\"}]"}]}
{"id":"644fe062-1933-48e3-868b-22c74f2c2db5","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\", \"Load the table 'product_data' which contains details about products and their associated suppliers.\", \"Filter the supplier data to include only those with a sustainability rating above 80.\", \"Join the filtered supplier data with the product data on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Information\", \"description\": \"Contains information about suppliers including their sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Rating of the supplier's sustainability practices.\"}, {\"column\": \"supplier_name\", \"column_name\": \"SUPPLIER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Information\", \"description\": \"Contains details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}, {\"column\": \"product_name\", \"column_name\": \"PRODUCT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Information\", \"description\": \"Contains logistics data including transportation modes and emissions.\", \"columns\": [{\"column\": \"logistics_id\", \"column_name\": \"LOGISTICS_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each logistics entry.\"}, {\"column\": \"emissions\", \"column_name\": \"EMISSIONS\", \"column_type\": \"xsd:decimal\", \"column_description\": \"CO2 emissions associated with the logistics entry.\"}, {\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transportation used.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading supplier data containing sustainability ratings.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading product data with associated suppliers.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"sustainable_suppliers\", \"comment\": \"Filtering suppliers to include only those with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"sustainable_suppliers\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"sustainable_supplier_products\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sustainable_supplier_products\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"sustainable_supplier_aggregation\", \"comment\": \"Aggregating data to calculate the total number of products supplied by each sustainable supplier.\"}]"}]}
{"id":"3568a1b7-91f8-4cb3-af10-6015694b32b4","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'court_cases' which contains information about various legal cases.\", \"Load the table 'lawyers' which contains details about lawyers involved in the cases.\", \"Filter the 'court_cases' table to include only cases from the year 2023.\", \"Join the filtered 'court_cases' table with the 'lawyers' table on the lawyer_id.\", \"Aggregate the joined table to count the number of cases each lawyer handled in 2023.\", \"Sort the aggregated data by the number of cases in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"court_cases\", \"location\": \"C:/legal_data/court_cases.csv\", \"sheet_name\": null, \"label\": \"Court Cases\", \"description\": \"A table containing information about various legal cases, including case ID, lawyer ID, and case date.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"case_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each case.\"}, {\"column\": \"column_2_id\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the lawyer handling the case.\"}, {\"column\": \"column_3_id\", \"column_name\": \"case_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the case was filed.\"}]}, \"table_id2\": {\"name\": \"lawyers\", \"location\": \"C:/legal_data/lawyers.csv\", \"sheet_name\": null, \"label\": \"Lawyers\", \"description\": \"A table containing details about lawyers, including lawyer ID and name.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"lawyer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each lawyer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"lawyer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the lawyer.\"}]}, \"table_id3\": {\"name\": \"court_judges\", \"location\": \"C:/legal_data/court_judges.csv\", \"sheet_name\": null, \"label\": \"Court Judges\", \"description\": \"A table containing information about judges, which is not relevant to the current workflow.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"judge_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each judge.\"}, {\"column\": \"column_2_id\", \"column_name\": \"judge_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the judge.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/court_cases.csv\", \"type\": \"csv\"}, \"output\": \"court_cases\", \"comment\": \"Loading the court cases dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/legal_data/lawyers.csv\", \"type\": \"csv\"}, \"output\": \"lawyers\", \"comment\": \"Loading the lawyers dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"court_cases\", \"query\": \"`case_date`.year == 2023\"}, \"output\": \"court_cases_2023\", \"comment\": \"Filtering the court cases to include only those from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"court_cases_2023\", \"lawyers\"], \"joinOn\": [\"lawyer_id\"], \"joinType\": \"inner\"}, \"output\": \"cases_with_lawyers\", \"comment\": \"Joining the filtered court cases with the lawyers table on lawyer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"cases_with_lawyers\", \"groupBy\": \"lawyer_id\", \"aggregations\": [{\"column\": \"case_id\", \"name\": \"case_count\", \"function\": \"count\"}]}, \"output\": \"lawyer_case_counts\", \"comment\": \"Aggregating the data to count the number of cases each lawyer handled in 2023.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"lawyer_case_counts\", \"sortBy\": [\"case_count\"], \"order\": [\"desc\"]}, \"output\": \"sorted_lawyer_case_counts\", \"comment\": \"Sorting the aggregated data by the number of cases in descending order.\"}]"}]}
{"id":"e12f6e04-a70b-4c05-9cf8-17d13008233c","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'shipment_data' table from a CSV file.\", \"Load the 'warehouse_data' table from an Excel file.\", \"Filter the 'shipment_data' to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column.\", \"Calculate a new column 'shipping_cost' in the joined table by multiplying 'weight' by 'cost_per_kg'.\", \"Aggregate the data by 'warehouse_location' to find the total shipping cost per location.\", \"Sort the aggregated data by 'total_shipping_cost' in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"Contains details of shipments including weight, destination, and cost per kg.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_3\", \"column_name\": \"cost_per_kg\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Cost per kilogram for shipping.\"}, {\"column\": \"col_4\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the warehouse associated with the shipment.\"}]}, \"table_id2\": {\"name\": \"warehouse_data\", \"location\": \"C:/data/warehouse_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Warehouse Data\", \"description\": \"Details about warehouses including their locations.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each warehouse.\"}, {\"column\": \"col_2\", \"column_name\": \"warehouse_location\", \"column_type\": \"xsd:string\", \"column_description\": \"Location of the warehouse.\"}]}, \"table_id3\": {\"name\": \"fleet_data\", \"location\": \"C:/data/fleet_data.csv\", \"sheet_name\": null, \"label\": \"Fleet Data\", \"description\": \"Information about the fleet vehicles used for shipments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"vehicle_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each vehicle.\"}, {\"column\": \"col_2\", \"column_name\": \"capacity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Capacity of the vehicle in kilograms.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading shipment data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/warehouse_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"warehouse_data\", \"comment\": \"Loading warehouse data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 1000\"}, \"output\": \"filtered_shipment_data\", \"comment\": \"Filtering shipments to include only those with weight greater than 1000 kg.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipment_data\", \"warehouse_data\"], \"joinOn\": [\"warehouse_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with warehouse data on warehouse_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"shipping_cost\", \"formula\": \"weight * cost_per_kg\"}, \"output\": \"data_with_shipping_cost\", \"comment\": \"Calculating shipping cost by multiplying weight by cost per kg.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_shipping_cost\", \"groupBy\": \"warehouse_location\", \"aggregations\": [{\"column\": \"shipping_cost\", \"name\": \"total_shipping_cost\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by warehouse location to find total shipping cost per location.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_shipping_cost\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the aggregated data by total shipping cost in descending order.\"}]"}]}
{"id":"3b180100-db09-4527-81de-985ef6806462","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the CSV file.\", \"Load the medical test results table from the Excel file.\", \"Filter the patient records to include only those over 60 years old.\", \"Join the filtered patient records with the medical test results on patient ID.\", \"Calculate the BMI for each patient using weight and height columns.\", \"Aggregate the data to find the average BMI by gender.\", \"Sort the aggregated data by average BMI in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/data/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"Contains demographic information of patients including age, gender, weight, and height.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the patient.\"}, {\"column\": \"col_4\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the patient in kilograms.\"}, {\"column\": \"col_5\", \"column_name\": \"height\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Height of the patient in meters.\"}]}, \"table_id2\": {\"name\": \"medical_test_results\", \"location\": \"C:/data/medical_test_results.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Medical Test Results\", \"description\": \"Contains results of various medical tests conducted on patients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"test_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the medical test.\"}, {\"column\": \"col_3\", \"column_name\": \"test_result\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Result of the medical test.\"}]}, \"table_id3\": {\"name\": \"hospital_staff\", \"location\": \"C:/data/hospital_staff.csv\", \"sheet_name\": null, \"label\": \"Hospital Staff\", \"description\": \"Contains information about hospital staff members.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"staff_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each staff member.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the staff member.\"}, {\"column\": \"col_3\", \"column_name\": \"position\", \"column_type\": \"xsd:string\", \"column_description\": \"Position of the staff member in the hospital.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading patient demographic information from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/medical_test_results.xlsx\", \"type\": \"xlsx\"}, \"output\": \"medical_test_results\", \"comment\": \"Loading medical test results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"age > 60\"}, \"output\": \"filtered_patient_records\", \"comment\": \"Filtering patient records to include only those over 60 years old.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_patient_records\", \"medical_test_results\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered patient records with medical test results on patient ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"BMI\", \"formula\": \"weight / (height ** 2)\"}, \"output\": \"data_with_bmi\", \"comment\": \"Calculating BMI for each patient using weight and height columns.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_bmi\", \"groupBy\": \"gender\", \"aggregations\": [{\"column\": \"BMI\", \"name\": \"average_BMI\", \"function\": \"mean\"}]}, \"output\": \"average_bmi_by_gender\", \"comment\": \"Aggregating data to find the average BMI by gender.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_bmi_by_gender\", \"sortBy\": \"average_BMI\", \"order\": \"desc\"}, \"output\": \"sorted_bmi_data\", \"comment\": \"Sorting the aggregated data by average BMI in descending order.\"}]"}]}
{"id":"1c495fef-3e3c-40cb-bb4b-a9c1243cac8a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the filtered campaign results with customer demographics on customer ID.\", \"Aggregate the joined data to find the average age of customers for each campaign.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information of customers including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaign_results.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Contains results of various marketing campaigns including success status and customer IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"data_point\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data point.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaign_results.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering marketing campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"successful_campaigns\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining successful campaign results with customer demographics on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"campaign_id\", \"aggregations\": [{\"column\": \"age\", \"name\": \"average_age\", \"function\": \"mean\"}]}, \"output\": \"campaign_age_analysis\", \"comment\": \"Aggregating joined data to find the average age of customers for each campaign.\"}]"}]}
{"id":"44e7234f-01ea-44b6-97a7-d0b7542aa2cf","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'shipment_data' which contains details of shipments including dates and weights.\", \"Filter the shipment data to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered shipment data with the 'route_info' table on the route_id column.\", \"Aggregate the joined data to calculate the total weight of shipments for each route.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"Contains details of shipments including shipment dates, weights, and route IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the shipment.\"}, {\"column\": \"col_3\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_4\", \"column_name\": \"route_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the route of the shipment.\"}]}, \"table_id2\": {\"name\": \"route_info\", \"location\": \"C:/data/logistics/route_info.csv\", \"sheet_name\": null, \"label\": \"Route Information\", \"description\": \"Contains information about different routes including route IDs and descriptions.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"route_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each route.\"}, {\"column\": \"col_2\", \"column_name\": \"route_description\", \"column_type\": \"xsd:string\", \"column_description\": \"Description of the route.\"}]}, \"table_id3\": {\"name\": \"driver_info\", \"location\": \"C:/data/logistics/driver_info.csv\", \"sheet_name\": null, \"label\": \"Driver Information\", \"description\": \"Contains details about drivers including driver IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"driver_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each driver.\"}, {\"column\": \"col_2\", \"column_name\": \"driver_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the driver.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 1000\"}, \"output\": \"filtered_shipments\", \"comment\": \"Filtering shipments to include only those with a weight greater than 1000 kg.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/route_info.csv\", \"type\": \"csv\"}, \"output\": \"route_info\", \"comment\": \"Loading the route information from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipments\", \"route_info\"], \"joinOn\": [\"route_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with route information on route_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"route_id\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_weight\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate the total weight of shipments for each route.\"}]"}]}
{"id":"40521b67-21af-4245-8882-19e83a17ac11","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics data from the CSV file.\", \"Filter the data to include only customers aged between 25 and 40.\", \"Aggregate the filtered data to find the average income by city.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/marketing/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"This table contains demographic information about customers, including age, income, and city.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"income\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Annual income of the customer.\"}, {\"column\": \"col_4\", \"column_name\": \"city\", \"column_type\": \"xsd:string\", \"column_description\": \"City where the customer resides.\"}]}, \"table_id2\": {\"name\": \"customer_purchases\", \"location\": \"C:/data/marketing/customer_purchases.csv\", \"sheet_name\": null, \"label\": \"Customer Purchases\", \"description\": \"This table contains purchase history of customers, including purchase amount and date.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"purchase_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each purchase.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier linking purchase to a customer.\"}, {\"column\": \"col_3\", \"column_name\": \"purchase_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount spent on the purchase.\"}, {\"column\": \"col_4\", \"column_name\": \"purchase_date\", \"column_type\": \"xsd:date\", \"column_description\": \"Date of the purchase.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_demographics\", \"query\": \"`age` >= 25 and `age` <= 40\"}, \"output\": \"filtered_customers\", \"comment\": \"Filtering customers to include only those aged between 25 and 40.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_customers\", \"groupBy\": \"city\", \"aggregations\": [{\"column\": \"income\", \"name\": \"average_income\", \"function\": \"mean\"}]}, \"output\": \"average_income_by_city\", \"comment\": \"Aggregating data to find the average income by city for the filtered customers.\"}]"}]}
{"id":"b4accdde-4dd3-40d4-ab40-0750f2e43739","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file.\", \"Filter the data to include only records where the consumption is greater than 100 kWh.\", \"Aggregate the filtered data to calculate the total energy consumption per month.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains records of energy consumption with timestamps and consumption values in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"The timestamp of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The energy consumption in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"energy_prices\", \"location\": \"C:/data/energy_prices.csv\", \"sheet_name\": null, \"label\": \"Energy Prices Data\", \"description\": \"This table contains historical energy prices with timestamps and price values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"The timestamp of the energy price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price_per_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"consumption_kWh > 100\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering the data to include only records where the consumption is greater than 100 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_consumption\", \"groupBy\": \"timestamp\", \"aggregations\": [{\"column\": \"consumption_kWh\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the filtered data to calculate the total energy consumption per month.\"}]"}]}
{"id":"49bd4434-9c70-464b-a924-88201d127ce1","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Filter the data to include only entries from the year 2023.\", \"Aggregate the data to calculate the total energy consumption per month.\", \"Join the aggregated data with the building occupancy data on the month column.\", \"Calculate the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\", \"Sort the resulting data by energy consumption per occupant in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption\", \"description\": \"This table contains energy consumption data for the building, including timestamps and energy usage values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"building_occupancy\", \"location\": \"C:/data/building_occupancy.csv\", \"sheet_name\": null, \"label\": \"Building Occupancy Data\", \"description\": \"This table contains data on the number of occupants in the building per month.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"month\", \"column_type\": \"xsd:string\", \"column_description\": \"The month for which the occupancy data is recorded.\"}, {\"column\": \"col_2\", \"column_name\": \"occupants\", \"column_type\": \"xsd:integer\", \"column_description\": \"The number of occupants in the building for the given month.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column for demonstration purposes.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering the energy consumption data to include only entries from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_consumption\", \"groupBy\": \"timestamp.dt.month\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the data to calculate the total energy consumption per month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_occupancy.csv\", \"type\": \"csv\"}, \"output\": \"building_occupancy\", \"comment\": \"Loading the building occupancy data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"monthly_energy_consumption\", \"building_occupancy\"], \"joinOn\": [\"month\"], \"joinType\": \"inner\"}, \"output\": \"energy_occupancy_data\", \"comment\": \"Joining the aggregated energy data with the building occupancy data on the month column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"energy_occupancy_data\", \"columnName\": \"energy_per_occupant\", \"formula\": \"total_energy_consumption / occupants\"}, \"output\": \"energy_per_occupant_data\", \"comment\": \"Calculating the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"energy_per_occupant_data\", \"sortBy\": \"energy_per_occupant\", \"order\": \"desc\"}, \"output\": \"sorted_energy_per_occupant_data\", \"comment\": \"Sorting the resulting data by energy consumption per occupant in descending order.\"}]"}]}
{"id":"f8cfd6bc-0be1-41b6-ab82-2988c454762a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'financial_transactions' from the CSV file located at C:/data/financial_transactions.csv.\", \"Filter the transactions to include only those with an amount greater than 1000.\", \"Aggregate the filtered transactions by account_id to calculate the total amount for each account.\", \"Sort the aggregated data by total amount in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"A table containing financial transactions with details such as account_id, transaction_date, and amount.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"account_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each account.\"}, {\"column\": \"column_2_id\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the transaction occurred.\"}, {\"column\": \"column_3_id\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"A table containing details about customers such as customer_id, name, and address.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"address\", \"column_type\": \"xsd:string\", \"column_description\": \"The address of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_transactions\", \"groupBy\": \"account_id\", \"aggregations\": [{\"column\": \"amount\", \"name\": \"total_amount\", \"function\": \"sum\"}]}, \"output\": \"aggregated_transactions\", \"comment\": \"Aggregating filtered transactions by account_id to calculate the total amount for each account.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_transactions\", \"sortBy\": \"total_amount\", \"order\": \"desc\"}, \"output\": \"sorted_transactions\", \"comment\": \"Sorting the aggregated data by total amount in descending order.\"}]"}]}
{"id":"b0ddf5e7-31f9-4af9-8fad-a7556c4584f7","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\", \"Load the table 'product_data' which includes details about products and their associated suppliers.\", \"Filter the 'supplier_data' to include only suppliers with a sustainability rating above 80.\", \"Join the filtered supplier data with the 'product_data' on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\", \"Sort the aggregated data by the total number of products in descending order.\", \"Export the sorted data to a CSV file for reporting purposes.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supply_chain/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers, including their IDs and sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Rating of the supplier's sustainability practices.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/supply_chain/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"Includes details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/supply_chain/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Data\", \"description\": \"Contains logistics information, not relevant to the current workflow.\", \"columns\": [{\"column\": \"logistics_id\", \"column_name\": \"LOGISTICS_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each logistics entry.\"}, {\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transport used for logistics.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supply_chain/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading the supplier data containing sustainability ratings.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supply_chain/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading the product data with supplier associations.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"sustainable_suppliers\", \"comment\": \"Filtering suppliers to include only those with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"sustainable_suppliers\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate total number of products supplied by each sustainable supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_products\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the aggregated data by total number of products in descending order.\"}]"}]}
{"id":"89176afc-3a09-43be-8c27-0f8da090edee","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the CSV file.\", \"Load the treatment data from the Excel file.\", \"Filter the patient records to include only those over 50 years old.\", \"Join the filtered patient records with the treatment data on patient ID.\", \"Calculate a new column in the joined table to determine the treatment success rate.\", \"Aggregate the data to find the average treatment success rate by treatment type.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/data/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"Contains demographic and health information about patients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the patient.\"}]}, \"table_id2\": {\"name\": \"treatment_data\", \"location\": \"C:/data/treatment_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Treatment Data\", \"description\": \"Details of treatments administered to patients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"treatment_type\", \"column_type\": \"xsd:string\", \"column_description\": \"Type of treatment administered.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Success rate of the treatment.\"}]}, \"table_id3\": {\"name\": \"hospital_staff\", \"location\": \"C:/data/hospital_staff.csv\", \"sheet_name\": null, \"label\": \"Hospital Staff\", \"description\": \"Information about hospital staff members.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"staff_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each staff member.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the staff member.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading patient records from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/treatment_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"treatment_data\", \"comment\": \"Loading treatment data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"age > 50\"}, \"output\": \"filtered_patient_records\", \"comment\": \"Filtering patient records to include only those over 50 years old.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_patient_records\", \"treatment_data\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered patient records with treatment data on patient ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"treatment_success_rate\", \"formula\": \"success\"}, \"output\": \"joined_data_with_success_rate\", \"comment\": \"Calculating treatment success rate in the joined table.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_success_rate\", \"groupBy\": \"treatment_type\", \"aggregations\": [{\"column\": \"treatment_success_rate\", \"name\": \"average_success_rate\", \"function\": \"mean\"}]}, \"output\": \"average_success_rate_by_treatment\", \"comment\": \"Aggregating data to find the average treatment success rate by treatment type.\"}]"}]}
{"id":"d6f8d147-a717-4eb0-9298-b0344021c120","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'shipment_data' which contains shipment details.\", \"Load the table 'warehouse_locations' which contains warehouse location details.\", \"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered shipment data with the 'warehouse_locations' table on the 'warehouse_id' column.\", \"Calculate a new column 'shipping_cost' in the joined table using the formula 'distance * weight * 0.05'.\", \"Aggregate the data by 'warehouse_id' to find the total shipping cost for each warehouse.\", \"Sort the aggregated data by 'total_shipping_cost' in descending order.\", \"Apply a log transformation to the 'total_shipping_cost' column to stabilize variance.\", \"Forecast the future shipping costs for the next 12 months using the Holt-Winters method.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"Contains details of shipments including weight, distance, and warehouse ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_3\", \"column_name\": \"distance\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Distance the shipment needs to travel in kilometers.\"}, {\"column\": \"col_4\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the warehouse associated with the shipment.\"}]}, \"table_id2\": {\"name\": \"warehouse_locations\", \"location\": \"C:/data/logistics/warehouse_locations.csv\", \"sheet_name\": null, \"label\": \"Warehouse Locations\", \"description\": \"Contains details of warehouse locations including warehouse ID and address.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each warehouse.\"}, {\"column\": \"col_2\", \"column_name\": \"address\", \"column_type\": \"xsd:string\", \"column_description\": \"Physical address of the warehouse.\"}]}, \"table_id3\": {\"name\": \"driver_data\", \"location\": \"C:/data/logistics/driver_data.csv\", \"sheet_name\": null, \"label\": \"Driver Data\", \"description\": \"Contains details of drivers including driver ID and assigned shipments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"driver_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each driver.\"}, {\"column\": \"col_2\", \"column_name\": \"assigned_shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the shipment assigned to the driver.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading the shipment data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/warehouse_locations.csv\", \"type\": \"csv\"}, \"output\": \"warehouse_locations\", \"comment\": \"Loading the warehouse location data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 1000\"}, \"output\": \"filtered_shipment_data\", \"comment\": \"Filtering shipments to include only those with a weight greater than 1000 kg.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipment_data\", \"warehouse_locations\"], \"joinOn\": [\"warehouse_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with warehouse locations on warehouse_id.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"shipping_cost\", \"formula\": \"distance * weight * 0.05\"}, \"output\": \"data_with_shipping_cost\", \"comment\": \"Calculating shipping cost using the formula 'distance * weight * 0.05'.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"data_with_shipping_cost\", \"groupBy\": \"warehouse_id\", \"aggregations\": [{\"column\": \"shipping_cost\", \"name\": \"total_shipping_cost\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by warehouse_id to find total shipping cost for each warehouse.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_shipping_cost\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting the aggregated data by total shipping cost in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_aggregated_data\", \"transformations\": [{\"columns\": [\"total_shipping_cost\"], \"operation\": \"log_transform\"}]}, \"output\": \"log_transformed_data\", \"comment\": \"Applying a log transformation to the total shipping cost column.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"log_transformed_data\", \"columns\": [\"total_shipping_cost\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"dateColumn\": \"warehouse_id\"}}, \"output\": \"forecasted_shipping_costs\", \"comment\": \"Forecasting future shipping costs for the next 12 months using Holt-Winters method.\"}]"}]}
{"id":"2d90cd5a-3e54-4825-b2c7-b4282dffd8fd","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Calculate the total energy consumption for each building.\", \"Sort the buildings by total energy consumption in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"building_energy_data\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption Data\", \"description\": \"This table contains energy consumption data for various buildings, including timestamps and energy usage in kilowatt-hours.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_2\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_3\", \"column_name\": \"energy_usage_kwh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"building_info\", \"location\": \"C:/data/building_info.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Building Information\", \"description\": \"This table contains information about the buildings, such as their location and type.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"building_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each building.\"}, {\"column\": \"col_B\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"The location of the building.\"}, {\"column\": \"col_C\", \"column_name\": \"building_type\", \"column_type\": \"xsd:string\", \"column_description\": \"The type of building, e.g., residential, commercial.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_X\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"building_energy_data\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"building_energy_data\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"filtered_energy_data\", \"comment\": \"Filtering the data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_data\", \"groupBy\": \"building_id\", \"aggregations\": [{\"column\": \"energy_usage_kwh\", \"name\": \"total_energy_consumption\", \"function\": \"sum\"}]}, \"output\": \"total_energy_per_building\", \"comment\": \"Calculating the total energy consumption for each building.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"total_energy_per_building\", \"sortBy\": \"total_energy_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_buildings\", \"comment\": \"Sorting the buildings by total energy consumption in descending order.\"}]"}]}
{"id":"a8c5c132-dcea-4ecc-b91c-581a22eb9764","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'shipment_data' table from the CSV file located at 'C:/data/logistics/shipment_data.csv'.\", \"Load the 'warehouse_data' table from the Excel file located at 'C:/data/logistics/warehouse_data.xlsx'.\", \"Filter the 'shipment_data' table to include only shipments with a weight greater than 1000 kg.\", \"Join the filtered 'shipment_data' with 'warehouse_data' on the 'warehouse_id' column using an inner join.\", \"Aggregate the joined table to calculate the total shipment weight per warehouse.\", \"Sort the aggregated data by total shipment weight in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"shipment_data\", \"location\": \"C:/data/logistics/shipment_data.csv\", \"sheet_name\": null, \"label\": \"Shipment Data\", \"description\": \"Contains details of shipments including weight, destination, and warehouse ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"shipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each shipment.\"}, {\"column\": \"col_2\", \"column_name\": \"weight\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Weight of the shipment in kilograms.\"}, {\"column\": \"col_3\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the warehouse associated with the shipment.\"}]}, \"table_id2\": {\"name\": \"warehouse_data\", \"location\": \"C:/data/logistics/warehouse_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Warehouse Data\", \"description\": \"Contains information about warehouses including their ID and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"warehouse_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each warehouse.\"}, {\"column\": \"col_2\", \"column_name\": \"location\", \"column_type\": \"xsd:string\", \"column_description\": \"Location of the warehouse.\"}]}, \"table_id3\": {\"name\": \"transport_data\", \"location\": \"C:/data/logistics/transport_data.csv\", \"sheet_name\": null, \"label\": \"Transport Data\", \"description\": \"Contains information about transportation modes and schedules.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transport_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transport mode.\"}, {\"column\": \"col_2\", \"column_name\": \"schedule\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"Scheduled time for the transport.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/shipment_data.csv\", \"type\": \"csv\"}, \"output\": \"shipment_data\", \"comment\": \"Loading shipment data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/logistics/warehouse_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"warehouse_data\", \"comment\": \"Loading warehouse data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"shipment_data\", \"query\": \"weight > 1000\"}, \"output\": \"filtered_shipment_data\", \"comment\": \"Filtering shipments to include only those with weight greater than 1000 kg.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_shipment_data\", \"warehouse_data\"], \"joinOn\": [\"warehouse_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered shipment data with warehouse data on warehouse_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"warehouse_id\", \"aggregations\": [{\"column\": \"weight\", \"name\": \"total_shipment_weight\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to calculate total shipment weight per warehouse.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_shipment_weight\", \"order\": \"desc\"}, \"output\": \"sorted_aggregated_data\", \"comment\": \"Sorting aggregated data by total shipment weight in descending order.\"}]"}]}
{"id":"c94cb0a5-7c7a-4cb8-b97f-5b4b0dd1b599","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the energy consumption data from the CSV file located in the energy_data folder.\", \"Filter the data to include only records where the consumption is greater than 500 kWh.\", \"Aggregate the filtered data by month to calculate the total consumption for each month.\", \"Sort the aggregated data by total consumption in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/energy_data/energy_consumption.csv\", \"sheet_name\": null, \"label\": \"Energy Consumption Data\", \"description\": \"This table contains energy consumption records with timestamps and consumption values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"energy_prices\", \"location\": \"C:/energy_data/energy_prices.csv\", \"sheet_name\": null, \"label\": \"Energy Prices Data\", \"description\": \"This table contains energy prices with timestamps and price values.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:timeStamp\", \"column_description\": \"The date and time of the energy price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price_per_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/energy_data/energy_consumption.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"consumption_kWh > 500\"}, \"output\": \"filtered_energy_consumption\", \"comment\": \"Filtering the data to include only records where the consumption is greater than 500 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_consumption\", \"groupBy\": \"timestamp\", \"aggregations\": [{\"column\": \"consumption_kWh\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the filtered data by month to calculate the total consumption for each month.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"monthly_energy_consumption\", \"sortBy\": \"total_monthly_consumption\", \"order\": \"desc\"}, \"output\": \"sorted_monthly_energy_consumption\", \"comment\": \"Sorting the aggregated data by total consumption in descending order.\"}]"}]}
{"id":"9781bb6d-232e-4ffc-90af-a7649c895854","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the product specifications table from the CSV file.\", \"Load the market analysis data from the Excel sheet.\", \"Filter the market analysis data to include only entries from the last two years.\", \"Join the product specifications with the filtered market analysis data on the product ID.\", \"Calculate the potential market size by multiplying the average demand by the number of potential customers.\", \"Aggregate the joined data by product category to find the total potential market size.\", \"Sort the aggregated data by total potential market size in descending order.\", \"Filter the sorted data to include only products with a potential market size greater than 100,000 units.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"product_specifications\", \"location\": \"C:/data/product_specifications.csv\", \"sheet_name\": null, \"label\": \"Product Specifications\", \"description\": \"Contains detailed specifications of each product, including product ID, name, and category.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category to which the product belongs.\"}]}, \"table_id2\": {\"name\": \"market_analysis\", \"location\": \"C:/data/market_analysis.xlsx\", \"sheet_name\": \"Analysis\", \"label\": \"Market Analysis\", \"description\": \"Contains market analysis data including demand and potential customer base for various products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"average_demand\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Average demand for the product over the last year.\"}, {\"column\": \"col_3\", \"column_name\": \"potential_customers\", \"column_type\": \"xsd:integer\", \"column_description\": \"Estimated number of potential customers for the product.\"}, {\"column\": \"col_4\", \"column_name\": \"year\", \"column_type\": \"xsd:integer\", \"column_description\": \"Year of the market analysis data.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Irrelevant\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_specifications.csv\", \"type\": \"csv\"}, \"output\": \"product_specifications\", \"comment\": \"Loading the product specifications from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/market_analysis.xlsx\", \"type\": \"xlsx\"}, \"output\": \"market_analysis\", \"comment\": \"Loading the market analysis data from an Excel sheet.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"market_analysis\", \"query\": \"year >= 2021\"}, \"output\": \"filtered_market_analysis\", \"comment\": \"Filtering market analysis data to include only entries from the last two years.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"product_specifications\", \"filtered_market_analysis\"], \"joinOn\": [\"product_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining product specifications with filtered market analysis data on product ID.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_data\", \"columnName\": \"potential_market_size\", \"formula\": \"average_demand * potential_customers\"}, \"output\": \"joined_data_with_market_size\", \"comment\": \"Calculating potential market size by multiplying average demand by the number of potential customers.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data_with_market_size\", \"groupBy\": \"category\", \"aggregations\": [{\"column\": \"potential_market_size\", \"name\": \"total_potential_market_size\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by product category to find total potential market size.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_potential_market_size\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by total potential market size in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_data\", \"query\": \"total_potential_market_size > 100000\"}, \"output\": \"filtered_sorted_data\", \"comment\": \"Filtering sorted data to include only products with a potential market size greater than 100,000 units.\"}]"}]}
{"id":"70b6bd44-4888-4af0-b747-286ccf67f1a8","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the building energy consumption data from the CSV file.\", \"Filter the data to include only records from the year 2023.\", \"Aggregate the data to calculate the total energy consumption per month.\", \"Join the aggregated data with the building occupancy data on the month column.\", \"Calculate the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\", \"Sort the resulting data by energy consumption per occupant in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_consumption\", \"location\": \"C:/data/building_energy.csv\", \"sheet_name\": null, \"label\": \"Building Energy Consumption\", \"description\": \"This table contains energy consumption data for the building, including timestamps and energy usage in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"timestamp\", \"column_type\": \"xsd:dateTime\", \"column_description\": \"The date and time of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"energy_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kWh.\"}]}, \"table_id2\": {\"name\": \"building_occupancy\", \"location\": \"C:/data/building_occupancy.csv\", \"sheet_name\": null, \"label\": \"Building Occupancy Data\", \"description\": \"This table contains data on the number of occupants in the building per month.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"month\", \"column_type\": \"xsd:string\", \"column_description\": \"The month of the record.\"}, {\"column\": \"col_2\", \"column_name\": \"occupants\", \"column_type\": \"xsd:integer\", \"column_description\": \"The number of occupants in the building.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"An irrelevant column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_energy.csv\", \"type\": \"csv\"}, \"output\": \"energy_consumption\", \"comment\": \"Loading the building energy consumption data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_consumption\", \"query\": \"`timestamp`.dt.year == 2023\"}, \"output\": \"filtered_energy_2023\", \"comment\": \"Filtering the energy consumption data to include only records from the year 2023.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_energy_2023\", \"groupBy\": \"timestamp.dt.month\", \"aggregations\": [{\"column\": \"energy_usage\", \"name\": \"total_energy\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy\", \"comment\": \"Aggregating the data to calculate the total energy consumption per month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_occupancy.csv\", \"type\": \"csv\"}, \"output\": \"building_occupancy\", \"comment\": \"Loading the building occupancy data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"monthly_energy\", \"building_occupancy\"], \"joinOn\": [\"month\"], \"joinType\": \"inner\"}, \"output\": \"energy_occupancy\", \"comment\": \"Joining the aggregated energy data with the building occupancy data on the month column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"energy_occupancy\", \"columnName\": \"energy_per_occupant\", \"formula\": \"total_energy / occupants\"}, \"output\": \"energy_per_occupant_data\", \"comment\": \"Calculating the energy consumption per occupant by dividing the total energy consumption by the number of occupants.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"energy_per_occupant_data\", \"sortBy\": \"energy_per_occupant\", \"order\": \"desc\"}, \"output\": \"sorted_energy_per_occupant\", \"comment\": \"Sorting the resulting data by energy consumption per occupant in descending order.\"}]"}]}
{"id":"7102d152-3eae-4591-a0cf-de8faf683046","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Filter the customers who are aged between 25 and 40.\", \"Aggregate the filtered data to find the average income by city.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/marketing/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"This table contains information about customer demographics including age, income, and city.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"income\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Annual income of the customer.\"}, {\"column\": \"col_4\", \"column_name\": \"city\", \"column_type\": \"xsd:string\", \"column_description\": \"City where the customer resides.\"}]}, \"table_id2\": {\"name\": \"product_sales\", \"location\": \"C:/data/marketing/product_sales.csv\", \"sheet_name\": null, \"label\": \"Product Sales\", \"description\": \"This table contains sales data for different products.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total sales amount for the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading the customer demographics table from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_demographics\", \"query\": \"`age` >= 25 and `age` <= 40\"}, \"output\": \"filtered_customers\", \"comment\": \"Filtering customers who are aged between 25 and 40.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_customers\", \"groupBy\": \"city\", \"aggregations\": [{\"column\": \"income\", \"name\": \"average_income\", \"function\": \"mean\"}]}, \"output\": \"average_income_by_city\", \"comment\": \"Aggregating the filtered data to find the average income by city.\"}]"}]}
{"id":"86a0f1fe-30a9-4912-8547-152ccd1ceea1","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results table from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the filtered campaign results with the customer demographics table on customer ID.\", \"Aggregate the joined data to find the total number of successful campaigns per age group.\", \"Sort the aggregated data by the number of successful campaigns in descending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information of customers including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaigns.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Details of marketing campaigns including their success status.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the marketing workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"irrelevant_column\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data column.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaigns.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"successful_campaigns\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining successful campaign results with customer demographics on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"age\", \"aggregations\": [{\"column\": \"campaign_id\", \"name\": \"total_successful_campaigns\", \"function\": \"count\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data to find the total number of successful campaigns per age group.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_successful_campaigns\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting the aggregated data by the number of successful campaigns in descending order.\"}]"}]}
{"id":"4ebdac07-7476-4d3c-aef9-80b7cff031b8","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the production schedule table from the CSV file.\", \"Filter the production schedule to include only tasks scheduled for the current week.\", \"Join the filtered schedule with the inventory table to check material availability.\", \"Aggregate the joined data to calculate total material requirements for the week.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the production schedule with task details and scheduled dates.\", \"columns\": [{\"column\": \"task_id\", \"column_name\": \"TASK_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each production task.\"}, {\"column\": \"scheduled_date\", \"column_name\": \"SCHEDULED_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the task is scheduled to be performed.\"}, {\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the material required for the task.\"}]}, \"table_id2\": {\"name\": \"inventory\", \"location\": \"C:/data/manufacturing/inventory.csv\", \"sheet_name\": null, \"label\": \"Inventory\", \"description\": \"This table contains current inventory levels of materials.\", \"columns\": [{\"column\": \"material_id\", \"column_name\": \"MATERIAL_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the material in inventory.\"}, {\"column\": \"quantity_available\", \"column_name\": \"QUANTITY_AVAILABLE\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The quantity of material currently available in inventory.\"}]}, \"table_id3\": {\"name\": \"employee_schedule\", \"location\": \"C:/data/manufacturing/employee_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Employee Schedule\", \"description\": \"This table contains the work schedule for employees.\", \"columns\": [{\"column\": \"employee_id\", \"column_name\": \"EMPLOYEE_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"work_date\", \"column_name\": \"WORK_DATE\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the employee is scheduled to work.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the production schedule from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"SCHEDULED_DATE >= '2023-10-23' and SCHEDULED_DATE <= '2023-10-29'\"}, \"output\": \"current_week_schedule\", \"comment\": \"Filtering the production schedule to include only tasks for the current week.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/inventory.csv\", \"type\": \"csv\"}, \"output\": \"inventory\", \"comment\": \"Loading the inventory data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"current_week_schedule\", \"inventory\"], \"joinOn\": [\"MATERIAL_ID\"], \"joinType\": \"inner\"}, \"output\": \"schedule_with_inventory\", \"comment\": \"Joining the filtered schedule with inventory to check material availability.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"schedule_with_inventory\", \"groupBy\": \"MATERIAL_ID\", \"aggregations\": [{\"column\": \"QUANTITY_AVAILABLE\", \"name\": \"total_material_required\", \"function\": \"sum\"}]}, \"output\": \"weekly_material_requirements\", \"comment\": \"Aggregating data to calculate total material requirements for the week.\"}]"}]}
{"id":"60299080-f997-4954-b32c-8b2cbd404571","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer data from the CSV file located at C:/data/customer_data.csv.\", \"Load the sales data from the Excel file located at C:/data/sales_data.xlsx.\", \"Filter the sales data to include only transactions from the year 2023.\", \"Join the filtered sales data with the customer data on the customer_id column.\", \"Calculate the total sales for each customer by summing the sales_amount column.\", \"Sort the customers by their total sales in descending order.\", \"Identify the top 10 customers based on total sales.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"Contains information about customers including their IDs, names, and contact details.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"column_2_id\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"column_3_id\", \"column_name\": \"email\", \"column_type\": \"xsd:string\", \"column_description\": \"Email address of the customer.\"}]}, \"table_id2\": {\"name\": \"sales_data\", \"location\": \"C:/data/sales_data.xlsx\", \"sheet_name\": null, \"label\": \"Sales Data\", \"description\": \"Contains sales transactions including sales amounts and dates.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"column_2_id\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the customer involved in the transaction.\"}, {\"column\": \"column_3_id\", \"column_name\": \"sales_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of money involved in the transaction.\"}, {\"column\": \"column_4_id\", \"column_name\": \"transaction_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date when the transaction occurred.\"}]}, \"table_id3\": {\"name\": \"marketing_campaigns\", \"location\": \"C:/data/marketing_campaigns.xlsx\", \"sheet_name\": \"Campaigns\", \"label\": \"Marketing Campaigns\", \"description\": \"Details of various marketing campaigns conducted over the years.\", \"columns\": [{\"column\": \"column_1_id\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"column_2_id\", \"column_name\": \"campaign_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the marketing campaign.\"}, {\"column\": \"column_3_id\", \"column_name\": \"start_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The start date of the campaign.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_data.csv\", \"type\": \"csv\"}, \"output\": \"customer_data\", \"comment\": \"Loading customer data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_data.xlsx\", \"type\": \"xlsx\"}, \"output\": \"sales_data\", \"comment\": \"Loading sales data from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_data\", \"query\": \"`transaction_date`.dt.year == 2023\"}, \"output\": \"filtered_sales_data\", \"comment\": \"Filtering sales data to include only transactions from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_sales_data\", \"customer_data\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"sales_with_customers\", \"comment\": \"Joining filtered sales data with customer data on customer_id.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sales_with_customers\", \"groupBy\": \"customer_id\", \"aggregations\": [{\"column\": \"sales_amount\", \"name\": \"total_sales\", \"function\": \"sum\"}]}, \"output\": \"customer_sales_totals\", \"comment\": \"Calculating total sales for each customer.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"customer_sales_totals\", \"sortBy\": \"total_sales\", \"order\": \"desc\"}, \"output\": \"sorted_customer_sales\", \"comment\": \"Sorting customers by their total sales in descending order.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sorted_customer_sales\", \"query\": \"index < 10\"}, \"output\": \"top_10_customers\", \"comment\": \"Identifying the top 10 customers based on total sales.\"}]"}]}
{"id":"253fb8c5-770e-4c98-b2fb-4cad68824ba7","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'supplier_data' which contains information about suppliers and their sustainability ratings.\", \"Load the table 'product_data' which contains details about products and their associated suppliers.\", \"Filter the supplier data to include only those with a sustainability rating above 80.\", \"Join the filtered supplier data with the product data on the supplier ID.\", \"Aggregate the joined data to calculate the total number of products supplied by each sustainable supplier.\", \"Sort the aggregated data by the total number of products in descending order.\", \"Export the sorted data to a CSV file for further analysis.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"supplier_data\", \"location\": \"C:/data/supplier_data.csv\", \"sheet_name\": null, \"label\": \"Supplier Data\", \"description\": \"Contains information about suppliers, including their sustainability ratings.\", \"columns\": [{\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each supplier.\"}, {\"column\": \"sustainability_rating\", \"column_name\": \"SUSTAINABILITY_RATING\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Sustainability rating of the supplier, ranging from 0 to 100.\"}, {\"column\": \"supplier_name\", \"column_name\": \"SUPPLIER_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the supplier.\"}]}, \"table_id2\": {\"name\": \"product_data\", \"location\": \"C:/data/product_data.csv\", \"sheet_name\": null, \"label\": \"Product Data\", \"description\": \"Contains details about products and their associated suppliers.\", \"columns\": [{\"column\": \"product_id\", \"column_name\": \"PRODUCT_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"supplier_id\", \"column_name\": \"SUPPLIER_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the supplier of the product.\"}, {\"column\": \"product_name\", \"column_name\": \"PRODUCT_NAME\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}]}, \"table_id3\": {\"name\": \"logistics_data\", \"location\": \"C:/data/logistics_data.csv\", \"sheet_name\": null, \"label\": \"Logistics Data\", \"description\": \"Contains information about logistics and transportation details.\", \"columns\": [{\"column\": \"logistics_id\", \"column_name\": \"LOGISTICS_ID\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each logistics entry.\"}, {\"column\": \"transport_mode\", \"column_name\": \"TRANSPORT_MODE\", \"column_type\": \"xsd:string\", \"column_description\": \"Mode of transportation used for logistics.\"}, {\"column\": \"carbon_emission\", \"column_name\": \"CARBON_EMISSION\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Carbon emission associated with the logistics entry.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/supplier_data.csv\", \"type\": \"csv\"}, \"output\": \"supplier_data\", \"comment\": \"Loading the supplier data containing sustainability ratings.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/product_data.csv\", \"type\": \"csv\"}, \"output\": \"product_data\", \"comment\": \"Loading the product data with supplier associations.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"supplier_data\", \"query\": \"SUSTAINABILITY_RATING > 80\"}, \"output\": \"sustainable_suppliers\", \"comment\": \"Filtering suppliers with a sustainability rating above 80.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"sustainable_suppliers\", \"product_data\"], \"joinOn\": [\"SUPPLIER_ID\"], \"joinType\": \"inner\"}, \"output\": \"sustainable_supplier_products\", \"comment\": \"Joining filtered supplier data with product data on supplier ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"sustainable_supplier_products\", \"groupBy\": \"SUPPLIER_ID\", \"aggregations\": [{\"column\": \"PRODUCT_ID\", \"name\": \"total_products\", \"function\": \"count\"}]}, \"output\": \"aggregated_supplier_products\", \"comment\": \"Aggregating data to calculate total products supplied by each sustainable supplier.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_supplier_products\", \"sortBy\": \"total_products\", \"order\": \"desc\"}, \"output\": \"sorted_supplier_products\", \"comment\": \"Sorting the aggregated data by total number of products in descending order.\"}]"}]}
{"id":"e44fca4d-24b0-4ee4-8a9f-11da6f6a982a","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the sales data from the CSV file located at C:/data/sales_2023.csv.\", \"Filter the sales data to include only transactions where the total revenue is greater than 1000.\", \"Aggregate the filtered sales data by product category to calculate the total sales revenue for each category.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"sales_2023\", \"location\": \"C:/data/sales_2023.csv\", \"sheet_name\": null, \"label\": \"Sales Data 2023\", \"description\": \"This table contains sales transactions for the year 2023, including details such as transaction ID, product category, and total revenue.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"product_category\", \"column_type\": \"xsd:string\", \"column_description\": \"Category of the product sold.\"}, {\"column\": \"col_3\", \"column_name\": \"total_revenue\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total revenue generated from the transaction.\"}]}, \"table_id2\": {\"name\": \"customer_data\", \"location\": \"C:/data/customer_data.csv\", \"sheet_name\": null, \"label\": \"Customer Data\", \"description\": \"This table contains customer information, including customer ID, name, and contact details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"contact_number\", \"column_type\": \"xsd:string\", \"column_description\": \"Contact number of the customer.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/sales_2023.csv\", \"type\": \"csv\"}, \"output\": \"sales_2023\", \"comment\": \"Loading the sales data for 2023 from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"sales_2023\", \"query\": \"total_revenue > 1000\"}, \"output\": \"filtered_sales\", \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"filtered_sales\", \"groupBy\": \"product_category\", \"aggregations\": [{\"column\": \"total_revenue\", \"name\": \"total_sales_revenue\", \"function\": \"sum\"}]}, \"output\": \"category_sales\", \"comment\": \"Aggregating filtered sales data by product category to calculate total sales revenue for each category.\"}]"}]}
{"id":"791cc786-f0b9-40e7-bda3-0025aae45061","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'energy_usage' which contains daily energy consumption data.\", \"Load the table 'weather_data' which contains daily weather information.\", \"Filter the 'energy_usage' table to include only data from the year 2023.\", \"Join the 'energy_usage' table with the 'weather_data' table on the 'date' column.\", \"Calculate a new column 'adjusted_usage' by subtracting 'solar_generation' from 'total_usage'.\", \"Aggregate the joined table by 'month' to calculate the total 'adjusted_usage'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_usage\", \"location\": \"C:/data/energy_usage.csv\", \"sheet_name\": null, \"label\": \"Energy Usage Data\", \"description\": \"This table contains daily energy consumption data including total usage and solar generation.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy usage record.\"}, {\"column\": \"col_2\", \"column_name\": \"total_usage\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The total energy usage for the day.\"}, {\"column\": \"col_3\", \"column_name\": \"solar_generation\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy generated by solar panels.\"}]}, \"table_id2\": {\"name\": \"weather_data\", \"location\": \"C:/data/weather_data.csv\", \"sheet_name\": null, \"label\": \"Weather Data\", \"description\": \"This table contains daily weather information including temperature and humidity.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the weather record.\"}, {\"column\": \"col_2\", \"column_name\": \"temperature\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average temperature for the day.\"}, {\"column\": \"col_3\", \"column_name\": \"humidity\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The average humidity for the day.\"}]}, \"table_id3\": {\"name\": \"historical_energy_prices\", \"location\": \"C:/data/historical_energy_prices.csv\", \"sheet_name\": null, \"label\": \"Historical Energy Prices\", \"description\": \"This table contains historical energy prices data, which is not relevant for the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the price record.\"}, {\"column\": \"col_2\", \"column_name\": \"price_per_kwh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The price of energy per kilowatt-hour.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/energy_usage.csv\", \"type\": \"csv\"}, \"output\": \"energy_usage\", \"comment\": \"Loading the energy usage data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/weather_data.csv\", \"type\": \"csv\"}, \"output\": \"weather_data\", \"comment\": \"Loading the weather data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_usage\", \"query\": \"`date` >= '2023-01-01' and `date` <= '2023-12-31'\"}, \"output\": \"energy_usage_2023\", \"comment\": \"Filtering energy usage data to include only records from the year 2023.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"energy_usage_2023\", \"weather_data\"], \"joinOn\": [\"date\"], \"joinType\": \"inner\"}, \"output\": \"joined_energy_weather\", \"comment\": \"Joining energy usage data with weather data on the date column.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_energy_weather\", \"columnName\": \"adjusted_usage\", \"formula\": \"`total_usage` - `solar_generation`\"}, \"output\": \"energy_with_adjusted_usage\", \"comment\": \"Calculating adjusted energy usage by subtracting solar generation from total usage.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"energy_with_adjusted_usage\", \"groupBy\": \"month\", \"aggregations\": [{\"column\": \"adjusted_usage\", \"name\": \"total_adjusted_usage\", \"function\": \"sum\"}]}, \"output\": \"monthly_adjusted_usage\", \"comment\": \"Aggregating the adjusted energy usage by month to calculate total adjusted usage.\"}]"}]}
{"id":"63b2dff3-7213-4268-a14d-08c76df09a3f","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'production_schedule' which contains the planned production activities.\", \"Filter the production schedule to include only activities scheduled for the current month.\", \"Join the filtered production schedule with the 'machine_availability' table to ensure machines are available for the scheduled activities.\", \"Aggregate the joined data to calculate the total production hours required for each machine.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"production_schedule\", \"location\": \"C:/data/manufacturing/production_schedule.csv\", \"sheet_name\": null, \"label\": \"Production Schedule\", \"description\": \"This table contains the planned production activities including start and end dates, and machine requirements.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"activity_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each production activity.\"}, {\"column\": \"col_2\", \"column_name\": \"start_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The start date of the production activity.\"}, {\"column\": \"col_3\", \"column_name\": \"end_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The end date of the production activity.\"}, {\"column\": \"col_4\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the machine required for the activity.\"}]}, \"table_id2\": {\"name\": \"machine_availability\", \"location\": \"C:/data/manufacturing/machine_availability.csv\", \"sheet_name\": null, \"label\": \"Machine Availability\", \"description\": \"This table provides information on the availability of machines including machine IDs and available hours.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"machine_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the machine.\"}, {\"column\": \"col_2\", \"column_name\": \"available_hours\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Total available hours for the machine.\"}]}, \"table_id3\": {\"name\": \"employee_schedule\", \"location\": \"C:/data/manufacturing/employee_schedule.csv\", \"sheet_name\": null, \"label\": \"Employee Schedule\", \"description\": \"This table contains the work schedules of employees, which is not relevant for the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"shift_start\", \"column_type\": \"xsd:time\", \"column_description\": \"Start time of the employee's shift.\"}, {\"column\": \"col_3\", \"column_name\": \"shift_end\", \"column_type\": \"xsd:time\", \"column_description\": \"End time of the employee's shift.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/production_schedule.csv\", \"type\": \"csv\"}, \"output\": \"production_schedule\", \"comment\": \"Loading the planned production activities from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"production_schedule\", \"query\": \"`start_date`.dt.month == current_month\"}, \"output\": \"current_month_schedule\", \"comment\": \"Filtering production activities to include only those scheduled for the current month.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/manufacturing/machine_availability.csv\", \"type\": \"csv\"}, \"output\": \"machine_availability\", \"comment\": \"Loading machine availability data from a CSV file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"current_month_schedule\", \"machine_availability\"], \"joinOn\": [\"machine_id\"], \"joinType\": \"inner\"}, \"output\": \"schedule_with_availability\", \"comment\": \"Joining the filtered production schedule with machine availability to ensure machines are available.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"schedule_with_availability\", \"groupBy\": \"machine_id\", \"aggregations\": [{\"column\": \"available_hours\", \"name\": \"total_production_hours\", \"function\": \"sum\"}]}, \"output\": \"machine_production_hours\", \"comment\": \"Calculating the total production hours required for each machine.\"}]"}]}
{"id":"09d680a7-41b6-4e5e-842a-8f273625d24e","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the 'financial_transactions' table from the CSV file located at C:/data/financial_transactions.csv.\", \"Load the 'exchange_rates' table from the Excel file located at C:/data/exchange_rates.xlsx.\", \"Filter the 'financial_transactions' table to include only transactions with an amount greater than 1000.\", \"Join the filtered 'financial_transactions' table with the 'exchange_rates' table on the 'currency_code' column.\", \"Calculate a new column 'amount_in_usd' by multiplying 'amount' with 'exchange_rate' in the joined table.\", \"Aggregate the joined table by 'transaction_type' to calculate the total 'amount_in_usd' for each type.\", \"Sort the aggregated data by 'total_amount_in_usd' in descending order.\", \"Apply a log transformation to the 'total_amount_in_usd' column to stabilize variance.\", \"Forecast the 'total_amount_in_usd' for the next 12 months using the Holt-Winters method.\", \"Save the forecast results to a new table called 'forecasted_financials'.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"financial_transactions\", \"location\": \"C:/data/financial_transactions.csv\", \"sheet_name\": null, \"label\": \"Financial Transactions\", \"description\": \"This table contains records of financial transactions including transaction ID, amount, currency code, and transaction type.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"transaction_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each transaction.\"}, {\"column\": \"col_2\", \"column_name\": \"amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of the transaction.\"}, {\"column\": \"col_3\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code of the transaction amount.\"}, {\"column\": \"col_4\", \"column_name\": \"transaction_type\", \"column_type\": \"xsd:string\", \"column_description\": \"The type of transaction, e.g., sale, refund.\"}]}, \"table_id2\": {\"name\": \"exchange_rates\", \"location\": \"C:/data/exchange_rates.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Exchange Rates\", \"description\": \"This table provides exchange rates for various currencies against USD.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"currency_code\", \"column_type\": \"xsd:string\", \"column_description\": \"The currency code.\"}, {\"column\": \"col_2\", \"column_name\": \"exchange_rate\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The exchange rate of the currency against USD.\"}]}, \"table_id3\": {\"name\": \"irrelevant_data\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Irrelevant Data\", \"description\": \"This table contains data that is not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"data_id\", \"column_type\": \"xsd:string\", \"column_description\": \"An identifier for the data.\"}, {\"column\": \"col_2\", \"column_name\": \"value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"A value associated with the data.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/financial_transactions.csv\", \"type\": \"csv\"}, \"output\": \"financial_transactions\", \"comment\": \"Loading the financial transactions dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/exchange_rates.xlsx\", \"type\": \"xlsx\"}, \"output\": \"exchange_rates\", \"comment\": \"Loading the exchange rates dataset from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"financial_transactions\", \"query\": \"amount > 1000\"}, \"output\": \"filtered_transactions\", \"comment\": \"Filtering transactions to include only those with an amount greater than 1000.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_transactions\", \"exchange_rates\"], \"joinOn\": [\"currency_code\"], \"joinType\": \"inner\"}, \"output\": \"joined_table\", \"comment\": \"Joining filtered transactions with exchange rates on currency code.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"joined_table\", \"columnName\": \"amount_in_usd\", \"formula\": \"amount * exchange_rate\"}, \"output\": \"transactions_with_usd\", \"comment\": \"Calculating amount in USD by multiplying amount with exchange rate.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"transactions_with_usd\", \"groupBy\": \"transaction_type\", \"aggregations\": [{\"column\": \"amount_in_usd\", \"name\": \"total_amount_in_usd\", \"function\": \"sum\"}]}, \"output\": \"aggregated_data\", \"comment\": \"Aggregating data by transaction type to calculate total amount in USD.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"aggregated_data\", \"sortBy\": \"total_amount_in_usd\", \"order\": \"desc\"}, \"output\": \"sorted_data\", \"comment\": \"Sorting aggregated data by total amount in USD in descending order.\"}, {\"action\": \"applyTransformations\", \"parameters\": {\"table\": \"sorted_data\", \"transformations\": [{\"columns\": [\"total_amount_in_usd\"], \"operation\": \"log_transform\"}]}, \"output\": \"log_transformed_data\", \"comment\": \"Applying log transformation to stabilize variance in total amount in USD.\"}, {\"action\": \"forecastData\", \"parameters\": {\"table\": \"log_transformed_data\", \"columns\": [\"total_amount_in_usd\"], \"forecastParameters\": {\"forecastPeriod\": 12, \"frequency\": \"MS\", \"algorithm\": \"holt_winters\", \"seasonal_periods\": 12, \"dateColumn\": \"transaction_type\"}}, \"output\": \"forecast_results\", \"comment\": \"Forecasting total amount in USD for the next 12 months using Holt-Winters method.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/forecasted_financials.csv\", \"type\": \"csv\"}, \"output\": \"forecasted_financials\", \"comment\": \"Saving the forecast results to a new table called forecasted_financials.\"}]"}]}
{"id":"9c38ce4c-2a11-40fe-991a-191dc0e0442c","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results table from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the customer demographics with the filtered marketing campaign results on customer ID.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information of customers including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaigns.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Contains results of various marketing campaigns including success metrics.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"data_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for irrelevant data.\"}, {\"column\": \"col_2\", \"column_name\": \"value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Some irrelevant value.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading the customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaigns.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading the marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering the marketing campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"customer_demographics\", \"successful_campaigns\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"demographics_with_successful_campaigns\", \"comment\": \"Joining customer demographics with successful marketing campaign results on customer ID.\"}]"}]}
{"id":"ab4af8a8-0765-4b21-8e1a-b31e4cb46d75","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee records from the CSV file located in the HR directory.\", \"Filter the employee records to include only those with a salary greater than 50,000.\", \"Sort the filtered employee records by their hire date in ascending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_records\", \"location\": \"C:/HR/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"A table containing records of employees including their salary, hire date, and other personal details.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Full name of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The salary of the employee.\"}, {\"column\": \"col_4\", \"column_name\": \"hire_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date the employee was hired.\"}]}, \"table_id2\": {\"name\": \"department_info\", \"location\": \"C:/HR/department_info.xlsx\", \"sheet_name\": \"Departments\", \"label\": \"Department Information\", \"description\": \"Contains information about different departments within the company.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_B\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the department.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR/employee_records.csv\", \"type\": \"csv\"}, \"output\": \"employee_records\", \"comment\": \"Loading the employee records from the CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_records\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employee records to include only those with a salary greater than 50,000.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"high_salary_employees\", \"sortBy\": \"hire_date\", \"order\": \"asc\"}, \"output\": \"sorted_high_salary_employees\", \"comment\": \"Sorting the filtered employee records by hire date in ascending order.\"}]"}]}
{"id":"0c3a8a14-af0d-4e22-b6a7-09ff7e642e0f","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the table 'energy_usage' which contains daily energy consumption data for the building.\", \"Filter the energy usage data to include only entries where the consumption is greater than 500 kWh.\", \"Aggregate the filtered data by month to calculate the total energy consumption for each month.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"energy_usage\", \"location\": \"C:/data/building_management/energy_usage.csv\", \"sheet_name\": null, \"label\": \"Energy Usage Data\", \"description\": \"This table contains daily energy consumption data for the building, including date and consumption in kWh.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"date\", \"column_type\": \"xsd:date\", \"column_description\": \"The date of the energy consumption record.\"}, {\"column\": \"col_2\", \"column_name\": \"consumption_kWh\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The amount of energy consumed in kilowatt-hours.\"}]}, \"table_id2\": {\"name\": \"maintenance_schedule\", \"location\": \"C:/data/building_management/maintenance_schedule.xlsx\", \"sheet_name\": \"Schedule\", \"label\": \"Maintenance Schedule\", \"description\": \"This table contains the maintenance schedule for building equipment.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"equipment_id\", \"column_type\": \"xsd:string\", \"column_description\": \"The unique identifier for each piece of equipment.\"}, {\"column\": \"col_2\", \"column_name\": \"maintenance_date\", \"column_type\": \"xsd:date\", \"column_description\": \"The scheduled date for maintenance.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/building_management/energy_usage.csv\", \"type\": \"csv\"}, \"output\": \"energy_usage\", \"comment\": \"Loading the energy usage data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"energy_usage\", \"query\": \"consumption_kWh > 500\"}, \"output\": \"high_energy_usage\", \"comment\": \"Filtering the energy usage data to include only entries with consumption greater than 500 kWh.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"high_energy_usage\", \"groupBy\": \"date\", \"aggregations\": [{\"column\": \"consumption_kWh\", \"name\": \"total_monthly_consumption\", \"function\": \"sum\"}]}, \"output\": \"monthly_energy_consumption\", \"comment\": \"Aggregating the filtered data by month to calculate the total energy consumption for each month.\"}]"}]}
{"id":"f2e6619f-a2ed-4f79-a920-6e2d8ddf0cae","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the patient records table from the CSV file.\", \"Load the hospital visits table from the Excel file.\", \"Filter the patient records to include only those with age above 60.\", \"Join the filtered patient records with hospital visits on patient ID.\", \"Calculate the average length of stay for each patient.\", \"Aggregate the data to find the total number of visits per hospital.\", \"Sort the aggregated data by the total number of visits in descending order.\", \"Identify patients with more than 3 hospital visits.\", \"Calculate a risk score for each patient based on age and number of visits.\", \"Filter the data to include only high-risk patients with a risk score above 7.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"patient_records\", \"location\": \"C:/data/patient_records.csv\", \"sheet_name\": null, \"label\": \"Patient Records\", \"description\": \"Contains demographic information and medical history of patients.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each patient.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the patient.\"}, {\"column\": \"col_3\", \"column_name\": \"medical_history\", \"column_type\": \"xsd:string\", \"column_description\": \"Medical history of the patient.\"}]}, \"table_id2\": {\"name\": \"hospital_visits\", \"location\": \"C:/data/hospital_visits.xlsx\", \"sheet_name\": \"Visits\", \"label\": \"Hospital Visits\", \"description\": \"Records of hospital visits including patient ID, hospital ID, and length of stay.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"visit_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each hospital visit.\"}, {\"column\": \"col_2\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier linking to the patient records.\"}, {\"column\": \"col_3\", \"column_name\": \"hospital_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the hospital.\"}, {\"column\": \"col_4\", \"column_name\": \"length_of_stay\", \"column_type\": \"xsd:integer\", \"column_description\": \"Number of days the patient stayed in the hospital.\"}]}, \"table_id3\": {\"name\": \"insurance_claims\", \"location\": \"C:/data/insurance_claims.xlsx\", \"sheet_name\": \"Claims\", \"label\": \"Insurance Claims\", \"description\": \"Contains insurance claim information, not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"claim_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each insurance claim.\"}, {\"column\": \"col_2\", \"column_name\": \"patient_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier linking to the patient records.\"}, {\"column\": \"col_3\", \"column_name\": \"claim_amount\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Amount claimed by the patient.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/patient_records.csv\", \"type\": \"csv\"}, \"output\": \"patient_records\", \"comment\": \"Loading the patient records dataset from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/hospital_visits.xlsx\", \"type\": \"xlsx\"}, \"output\": \"hospital_visits\", \"comment\": \"Loading the hospital visits dataset from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_records\", \"query\": \"age > 60\"}, \"output\": \"filtered_patients\", \"comment\": \"Filtering patient records to include only those aged above 60.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"filtered_patients\", \"hospital_visits\"], \"joinOn\": [\"patient_id\"], \"joinType\": \"inner\"}, \"output\": \"patient_visits\", \"comment\": \"Joining filtered patient records with hospital visits on patient ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"patient_visits\", \"groupBy\": \"patient_id\", \"aggregations\": [{\"column\": \"length_of_stay\", \"name\": \"average_stay\", \"function\": \"mean\"}]}, \"output\": \"patient_visits_with_avg_stay\", \"comment\": \"Calculating the average length of stay for each patient.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"patient_visits_with_avg_stay\", \"groupBy\": \"hospital_id\", \"aggregations\": [{\"column\": \"visit_id\", \"name\": \"total_visits\", \"function\": \"count\"}]}, \"output\": \"hospital_visit_counts\", \"comment\": \"Aggregating data to find the total number of visits per hospital.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"hospital_visit_counts\", \"sortBy\": \"total_visits\", \"order\": \"desc\"}, \"output\": \"sorted_hospital_visits\", \"comment\": \"Sorting the aggregated data by the total number of visits in descending order.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"patient_visits_with_avg_stay\", \"groupBy\": \"patient_id\", \"aggregations\": [{\"column\": \"visit_id\", \"name\": \"visit_count\", \"function\": \"count\"}]}, \"output\": \"patient_visit_counts\", \"comment\": \"Counting the number of visits for each patient.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patient_visit_counts\", \"query\": \"visit_count > 3\"}, \"output\": \"frequent_patients\", \"comment\": \"Identifying patients with more than 3 hospital visits.\"}, {\"action\": \"calculateDerivedColumn\", \"parameters\": {\"table\": \"frequent_patients\", \"columnName\": \"risk_score\", \"formula\": \"age * 0.1 + visit_count * 0.5\"}, \"output\": \"patients_with_risk_score\", \"comment\": \"Calculating a risk score for each patient based on age and number of visits.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"patients_with_risk_score\", \"query\": \"risk_score > 7\"}, \"output\": \"high_risk_patients\", \"comment\": \"Filtering the data to include only high-risk patients with a risk score above 7.\"}]"}]}
{"id":"fc75e2ff-48fc-4b3c-a81f-41178ccbc908","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the employee records table from the HR database.\", \"Filter the employee records to include only those with a salary greater than $50,000.\", \"Join the filtered employee records with the department details table on the department ID.\", \"Aggregate the joined data to calculate the average salary per department.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"employee_records\", \"location\": \"C:/HR_data/employee_records.csv\", \"sheet_name\": null, \"label\": \"Employee Records\", \"description\": \"Contains detailed information about employees, including their salaries and department IDs.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"employee_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each employee.\"}, {\"column\": \"col_2\", \"column_name\": \"salary\", \"column_type\": \"xsd:decimal\", \"column_description\": \"The salary of the employee.\"}, {\"column\": \"col_3\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Identifier for the department the employee belongs to.\"}]}, \"table_id2\": {\"name\": \"department_details\", \"location\": \"C:/HR_data/department_details.xlsx\", \"sheet_name\": \"Departments\", \"label\": \"Department Details\", \"description\": \"Provides information about each department, including department IDs and names.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"department_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each department.\"}, {\"column\": \"col_2\", \"column_name\": \"department_name\", \"column_type\": \"xsd:string\", \"column_description\": \"The name of the department.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/HR_data/irrelevant_data.csv\", \"sheet_name\": null, \"label\": \"Irrelevant Data\", \"description\": \"Contains data not relevant to the current workflow.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"random_id\", \"column_type\": \"xsd:string\", \"column_description\": \"A random identifier.\"}, {\"column\": \"col_2\", \"column_name\": \"random_value\", \"column_type\": \"xsd:decimal\", \"column_description\": \"A random value.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR_data/employee_records.csv\", \"type\": \"csv\"}, \"output\": \"employee_records\", \"comment\": \"Loading the employee records dataset from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"employee_records\", \"query\": \"salary > 50000\"}, \"output\": \"high_salary_employees\", \"comment\": \"Filtering employee records to include only those with a salary greater than $50,000.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/HR_data/department_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"department_details\", \"comment\": \"Loading the department details dataset from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"high_salary_employees\", \"department_details\"], \"joinOn\": [\"department_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining filtered employee records with department details on department ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"department_name\", \"aggregations\": [{\"column\": \"salary\", \"name\": \"average_salary\", \"function\": \"mean\"}]}, \"output\": \"average_salary_per_department\", \"comment\": \"Aggregating the joined data to calculate the average salary per department.\"}]"}]}
{"id":"c7d2634b-c978-4b7c-9723-a02b520de8fa","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer feedback table from the CSV file.\", \"Filter the feedback to include only entries with a rating below 3.\", \"Load the customer details table from the Excel file.\", \"Join the filtered feedback with customer details using the customer ID.\", \"Aggregate the data to find the average rating per region.\", \"Sort the aggregated data by average rating in ascending order.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_feedback\", \"location\": \"C:/data/customer_feedback.csv\", \"sheet_name\": null, \"label\": \"Customer Feedback\", \"description\": \"Table containing customer feedback with ratings and comments.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"rating\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Customer rating from 1 to 5.\"}, {\"column\": \"col_3\", \"column_name\": \"comment\", \"column_type\": \"xsd:string\", \"column_description\": \"Feedback comment from the customer.\"}]}, \"table_id2\": {\"name\": \"customer_details\", \"location\": \"C:/data/customer_details.xlsx\", \"sheet_name\": \"Sheet1\", \"label\": \"Customer Details\", \"description\": \"Table containing detailed information about customers including their region.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"region\", \"column_type\": \"xsd:string\", \"column_description\": \"Region where the customer is located.\"}, {\"column\": \"col_3\", \"column_name\": \"name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the customer.\"}]}, \"table_id3\": {\"name\": \"product_catalog\", \"location\": \"C:/data/product_catalog.csv\", \"sheet_name\": null, \"label\": \"Product Catalog\", \"description\": \"Table containing information about products offered by the company.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"product_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each product.\"}, {\"column\": \"col_2\", \"column_name\": \"product_name\", \"column_type\": \"xsd:string\", \"column_description\": \"Name of the product.\"}, {\"column\": \"col_3\", \"column_name\": \"price\", \"column_type\": \"xsd:decimal\", \"column_description\": \"Price of the product.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_feedback.csv\", \"type\": \"csv\"}, \"output\": \"customer_feedback\", \"comment\": \"Loading customer feedback data from a CSV file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"customer_feedback\", \"query\": \"rating < 3\"}, \"output\": \"low_rating_feedback\", \"comment\": \"Filtering feedback to include only entries with a rating below 3.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_details.xlsx\", \"type\": \"xlsx\"}, \"output\": \"customer_details\", \"comment\": \"Loading customer details from an Excel file.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"low_rating_feedback\", \"customer_details\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"feedback_with_details\", \"comment\": \"Joining filtered feedback with customer details using the customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"feedback_with_details\", \"groupBy\": \"region\", \"aggregations\": [{\"column\": \"rating\", \"name\": \"average_rating\", \"function\": \"mean\"}]}, \"output\": \"average_rating_by_region\", \"comment\": \"Aggregating data to find the average rating per region.\"}, {\"action\": \"sortData\", \"parameters\": {\"table\": \"average_rating_by_region\", \"sortBy\": \"average_rating\", \"order\": \"asc\"}, \"output\": \"sorted_average_rating_by_region\", \"comment\": \"Sorting the aggregated data by average rating in ascending order.\"}]"}]}
{"id":"5e98f079-87fe-4b64-9ea0-8ebeb9d2bf12","messages":[{"role":"system","content":"You are an AI that converts English data processing instructions into structured JSON workflows.\n\n## Task:\n\nYour task is to generate **diverse pairs of instructions, metadata and corresponding workflows** to train a small machine learning model.\n\n## Expected Output Format:\n\nReturn **only a valid JSON object** containing the keys \"instructions\", \"metadata\", \"workflow\" and structured as following:\n\n```json\n{\n\n    \"instructions\": [\n        \"Step 1 in natural language.\",\n        \"Step 2 in natural language.\",\n        \"... (continue up to 10 steps)\"\n    ],\n    \"metadata\": {\n        {\n          \"table_id1\": {\n            \"name\": \"table_name\",\n            \"location\": \"C:/path/to/table/table_name.csv\",\n            \"sheet_name\": null,\n            \"label\": \"A human readeable label\",\n            \"description\": \"A brief description of the table and its content to allow LLM to decide whether this table is relevant to answer the query\",\n            \"columns\": [\n              {\n                \"column\": \"column_1_id\",\n                \"column_name\": \"COLUMN_NAME_1\",\n                \"column_type\": \"xsd:decimal\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_2_id\",\n                \"column_name\": \"COLUMN_NAME_2\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              },\n              {\n                \"column\": \"column_3_id\",\n                \"column_name\": \"COLUMN_NAME_3\",\n                \"column_type\": \"xsd:timeStamp\",\n                \"column_description\": \"The column description, useful for knowing whether the column should be included\"\n              }\n            ]\n          },\n          \"table_id2\": {\n            \"name\": \"another_table\",\n            \"location\": \"c:/another/path/for/this/sheet.xslx:sheet=Relevant&cols=1:10,rows=32:78\",\n            \"sheet_name\": \"Relevant\",\n            \"label\": \"Some sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_1\",\n                \"column_name\": \"Col_Name\",\n                \"column_type\": \"xsd:string\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n          }, \n          {\n          \"table_id3\": {\n            \"name\": \"some_not_relevant_table\",\n            \"location\": \"c:/another/path/for/this/unused_sheet.xslx:sheet=NotRelevant&cols=1:30,rows=132:178\",\n            \"sheet_name\": \"NotRelevant\",\n            \"label\": \"A sheet in a spreadsheet\",\n            \"description\": \"A useful description of hte table, to be used for retrieval.\",\n            \"columns\": [\n              {\n                \"column\": \"col_A\",\n                \"column_name\": \"Col_Foo\",\n                \"column_type\": \"xsd:date\",\n                \"column_description\": \"a description\"\n              },\n              {\n                ...\n              },\n              {\n                ...\n              }\n\n          }\n          { \n            ... \n          }\n        }\n    }\n    \"workflow\": [\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"table_name\"\n        },\n        {\n            \"action\": \"identifyTable\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"type\": \"csv\"\n            },\n            \"output\": \"another_table_name\"\n        },\n        {\n            \"action\": \"filterData\",\n            \"parameters\": {\n                \"table\": \"another_table_name\",\n                \"caseInsensitiveColumns\": [ \"array of columns in the table for which the filter condition must be applied ignoring case\" ],\n                \"query\": \"filter_condition\"\n            },\n            \"output\": \"filtered_table\"\n        },\n        {\n            \"action\": \"joinTables\",\n            \"parameters\": {\n                \"tables\": [\"table_name\", \"another_table_name\"],\n                \"joinOn\": [\"key_column\", \"key_column_2\"],\n                \"joinType\": \"inner\"\n            },\n            \"output\": \"joined_table\"\n        },\n        ...\n    ]\n\n}\n```\n\n## Important Notes:\n\n* Ensure that each action in the workflow matches the relevant instruction.\n* Ensure that instructions and workflow actions refer to the appropriate and correct table and columns in the metadata section.\n* Ensure that instructions always refer to a specific table or columns.\n* Ensure that you have also some metadata tables, in the same domain, that are not relevant to the workflow to demonstrate accurate retrieval.\n* Do not include explanations, commentary, or any additional text\u2014only return a valid JSON object.\n* Ensure that table names, column names, and conditions are meaningful within the chosen domain.\n\n## LLM Workflow Generation Guidelines\n\n### ** 1\ufe0f - General Workflow Structure**\n\n* **Realism & Logical Ordering**: Workflows must be **realistic, logically structured**, and adhere to real-world **data processing patterns**.\n* **Workflow Composition**: Each workflow consists of **3 to 10 sequential steps**.\n* **Dataset Identification First**: Every workflow **must begin** with one or more **`identifyTable`** actions to load datasets.\n* **Coverage of All Actions**: Generated workflows **must include** examples of all **predefined actions**, ensuring diversity in training data.\n\n---\n\n### ** 2\ufe0f - Dataset Identification Rules**\n\n* **Explicit Table Definitions**:  \n  + Always assume the **exact file name**, **file type**, and **location** when loading datasets.\n  + Whenever possible, **generate realistic column names** that reflect the dataset\u2019s purpose.\n\n* **Example**:  \n  + **Instruction**:  \n `\"Load the table 'distances' which contains transportation modes and distances.\"`\n\n  + **Action**:\n    \n\n```json\n    {\n      \"action\": \"identifyTable\",\n      \"parameters\": {\n        \"location\": \"C:\\\\path\\\\to\\\\file\\\\distances.csv\",\n        \"type\": \"csv\"\n      },\n      \"output\": \"distances\",\n      \"comment\": \"Loading a dataset containing transportation distances from a CSV file.\"\n    }\n```\n\n---\n\n### ** 3\ufe0f - Deterministic & Strict Mapping Rules**\n\n1. **Direct Action Mapping**:  \n   - Every instruction **must map explicitly** to **one or more valid workflow actions**.\n   - No implicit transformations or assumptions allowed.\n2. **Strict Action Definitions**:  \n   - Use **only predefined actions**.\n   - Do **not introduce new actions or modify existing ones**.\n3. **Valid Parameter Selection**:  \n   - **All enumerated parameters must be chosen from the official list of valid values**.\n   - If an instruction references an invalid parameter, return an **error** instead of making assumptions.\n\n---\n\n### ** 4 - Column Names & Expression Rules**\n\n* **Correct Formatting**:\n  + If a column name contains **special characters** (`-`,       `.`,       `(`,       `)`, etc.), **enclose it in backticks (`\"`).\n  + Example:\n    \n\n```json\n    {\n      \"action\": \"filterData\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"query\": \"`total-revenue` > 1000\"\n      },\n      \"output\": \"high_value_sales\",\n      \"comment\": \"Filtering sales data to include only transactions with total revenue greater than 1000.\"\n    }\n```\n\n---\n\n### ** 5 - Workflow Structure & Data Dependencies**\n\n* **Table Dependencies**:\n  + **Ensure tables exist before being referenced**.\n  + If a step requires a dataset that hasn\u2019t been loaded, return an **error** instead of generating invalid workflows.\n* **Order Preservation**:\n  + **Preserve correct step ordering** to ensure logical execution.\n\n---\n\n### ** 6\ufe0f - Error Handling & Validation**\n\n* **Incremental Fixes**:  \n  + If an error is detected, **modify only the affected step** while preserving valid parts of the workflow.\n* **Error Reporting**:  \n  + If an instruction **cannot be mapped** to a valid action, return an **explicit error** rather than a faulty workflow.\n\n---\n\n### ** 7\ufe0f - Mathematical Formula Generation**\n\n* **Formula Execution Context**:  \n  + Any generated mathematical formulas **must be valid for execution** using `pandas.eval()` with the `\"python\"` engine.\n\n* **Safe Functions & Operators**:\n  + **Allowed functions**: `abs(x)`,   `round(x)`,   `min(x, y)`,   `max(x, y)`,   `np.sqrt(x)`,   `np.log(x)`,   `np.exp(x)`,   `np.sin(x)`,   `np.cos(x)`,   `np.tan(x)`,   `sum(x)`,   `mean(x)`,   `std(x)`,   `var(x)`,   `median(x)`,   `cumsum(x)`,   `cumprod(x)`,   `diff(x)`.\n  + **Allowed operators**: `+`, `-`,   `*`,   `/`,   `**`,   `%`,   `(`,   `)`.\n  + **Column Naming**:\n    - Use **column names as variables** (e.g.,   `total_23Q1 + total_23Q2`).\n    - If applying `max()` or `min()` to multiple columns, use the **pandas syntax**:\n\n```python\ndf[['col1', 'col2']].max(axis=1)\n```\n\n  + **Aggregation Functions**:\n    - Ensure functions like `sum()`, `mean()`, `std()` are **applied to column lists**, not individual values.\n  + **Example Action**:\n    \n\n```json\n    {\n      \"action\": \"calculateDerivedColumn\",\n      \"parameters\": {\n        \"table\": \"sales_data\",\n        \"columnName\": \"profit_margin\",\n        \"formula\": \"`total_revenue` - `total_cost`\"\n      },\n      \"output\": \"sales_data_with_profit\",\n      \"comment\": \"Calculating profit margin by subtracting total cost from total revenue.\"\n    }\n```\n\n---\n\n### ** 8\ufe0f - Workflow Action Schema**\n\n* **Workflow Representation**:\n  + A workflow is **an ordered JSON array** of **action objects**.\n* **Mandatory Keys for Each Action**:\n  + **`action`** (*string*): Name of the action.\n  + **`parameters`** (*object*): Contains the **action-specific parameters**.\n  + **`output`** (*string*): Name of the **resulting table** after the action.\n  + **`comment`** (*string*): Explains **the reasoning behind parameter choices**.\n\nThe key `output` MUST NOT be generated as a key of `parameter` . \n\n#### **Example Workflow**\n\n```json\n[\n  {\n\n    \"action\": \"identifyTable\",\n    \"parameters\": {\n      \"location\": \"C:\\\\data\\\\sales_2024.xlsx\",\n      \"type\": \"xlsx\"\n    },\n    \"output\": \"sales_data\",\n    \"comment\": \"Loading the main sales dataset from an Excel file.\"\n\n  }, \n  {\n\n    \"action\": \"filterData\",\n    \"parameters\": {\n      \"table\": \"sales_data\",\n      \"query\": \" `total_revenue` > 5000\"\n    },\n    \"output\": \"high_revenue_sales\",\n    \"comment\": \"Filtering sales data to keep transactions with total revenue above 5000.\"\n\n  }, \n  {\n\n    \"action\": \"joinTables\",\n    \"parameters\": {\n      \"tables\": [\"high_revenue_sales\", \"customer_data\"],\n      \"joinOn\": [\"customer_id\"],\n      \"joinType\": \"inner\"\n    },\n    \"output\": \"sales_with_customers\",\n    \"comment\": \"Joining high revenue sales with customer data based on customer_id.\"\n\n  }, \n  {\n\n    \"action\": \"aggregateData\",\n    \"parameters\": {\n      \"table\": \"sales_with_customers\",\n      \"groupBy\": \"region\",\n      \"aggregations\": [\n        {\n          \"column\": \"total_revenue\",\n          \"name\": \"total_sales\",\n          \"function\": \"sum\"\n        }\n      ]\n    },\n    \"output\": \"regional_sales\",\n    \"comment\": \"Aggregating sales data by region to compute total revenue.\"\n\n  }\n]\n```\n\n\n## Workflow Language Specification\n\n### 1. Overview\n\nThis specification defines a structured language for describing data transformation workflows. The language consists of a list of actions that process tables through various operations such as filtering, joining, aggregation, transformation, and forecasting. Each action is strictly defined to ensure unambiguous interpretation and deterministic execution.\n\n### 2. Syntax\n\nEach workflow consists of a list of actions, where each action has:\n* A **name** identifying the operation to be performed.\n* A **list of parameters**, including input tables and transformation options.\n* An **output table name**, where the result is stored.\n\nDatasets are identified by unique names and must exist before being referenced by an action.\n\n### 3. Actions and Their Definitions\n\n#### `identifyTable`\n\n**Description:** Loads a table into memory from a specified source.\n\n**Parameters:**\n\n* `location` (*string*): URL or file path specifying the table location. It may optionally include the query string to precisely identify the table.\n* `type` (*enum: `csv`,  `xlsx`*): Format of the table.\n\n**Behavior**\n\n* If the table does not exist, an error is raised.\n* If the format is unsupported, an error is raised.\n* If the table is already loaded, it is overwritten unless explicitly prevented.\n* The query string follows HTTP query string rules and contains parameters used by the interpreter to locate the desired table.\n\n**Examples**\n\n* Identifying a Table\n  \n\n```json\n  {{ \"name\": \"identifyTable\",\n    \"parameters\": {{ \"location\": \"https://example.com/data.csv\",\n      \"type\": \"csv\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n* Unsupported Data Type\n\n  \n\n```json\n  {{\n    \"name\": \"identifyTable\",\n    \"parameters\": {{\n      \"location\": \"https://example.com/data.xml\",\n      \"type\": \"xml\",\n      \"table\": \"sales_data\"\n    }},\n    \"output\": \"sales_data\"\n  }}\n  ```\n\n  **Expected Error:** Unsupported file type.\n\n#### `filterData`\n\n**Description:** Filters a table based on a query condition.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table to filter.\n* `query` (*string*): A Pandas-compatible query expression.\n* `caseInsensitiveColumns` (*list[string]*, default=`[]`): A list of columns in the table where filters in the query must be case insensitive. If supplied, the columns must be in the table.\n\n**Behavior**\n\n* Columns referenced in the query must exist; otherwise, an error is raised.\n* If the table does not exist, an error is raised.\n* If the query is invalid, an error is raised.\n\n**Example**\n\n* Filtering Data\n\n  \n\n```json\n  {{\n    \"name\": \"filterData\",\n    \"parameters\": {{\n      \"table\": \"sales_data\",\n      \"query\": \"revenue > 1000\"\n    }},\n    \"output\": \"filtered_sales\"\n  }}\n  ```\n\n#### `joinTables`\n\n**Description:** Joins two tables on specified key columns.\n\n**Parameters:**\n\n* `tables` (*list[string]*): Names of tables to join.\n* `joinOn` (*list[string]*): Columns used for joining.\n* `joinType` (*enum: `inner`,  `outer`,  `left`,  `right`*): Join type.\n\n**Behavior:**\n\n* If join columns do not exist in both tables, an error is raised.\n* Duplicate column names are automatically renamed, the prefix is the table name followed by `_`.\n* Null values are permitted in join keys.\n\n#### `aggregateData`\n\n**Description:** Aggregates a table based on grouping and aggregation functions.\n\n**Parameters:**\n* `table` (*string*): The name of the input table to be aggregated.\n* `groupBy` (*list[string] | string*): Column(s) used for grouping. If provided as a string, it will be treated as a single column. If an empty list or null, no grouping will be applied, and aggregation will be performed on the entire table.\n* `aggregations` (*list[dict]*): A list of aggregation instructions, where each dictionary contains the following parameters.\n  + `column`: (*string*) The name of the column on which the aggregation function will be applied.\n  + `name`: (*string*): The output column name for the aggregation result. If `groupBy` is empty (null or []), this field is ignored. If `groupBy` is set,  `name` is recommended to avoid column name conflicts.\n  + `function` (*enum: `sum`,  `mean`,  `median`,  `min`,  `max`,  `count`,  `std`,  `var`*): The aggregation function to apply to the specified column.\n\n**Behavior:**\n\n* If `groupBy` is empty onlu if the aggregation must apply to the entire table.\n* Non-numeric columns in aggregation are ignored.\n* Always choose a new unique name for the aggregation that is clear and intuitive. This name may be needed later in the workflow.\n\n#### `sortData`\n\n**Description:** Sorts a table by specified columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `sortBy` (*list[string] | string*): Columns to sort by.\n* `order` (*list[string] | *string, enum: `asc`,  `desc`*): Sorting direction.\n\n**Behavior:**\n\n* If `sortBy` columns do not exist, sorting is skipped.\n* Null values are sorted based on the underlying sorting algorithm.\n\n#### `calculateDerivedColumn`\n\n**Description:** Creates a new column based on a formula applied to existing columns.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `columnName` (*string*): Name of the new column.\n* `formula` (*string*): Pandas-compatible formula.\n\n**Behavior:**\n\n* Missing columns in the formula are treated as `null`.\n* Division by zero results in `null`.\n* Only a safe subset of operations is allowed.\n* If a formula is invalid, an error is raised.\n* Safe functions: `abs`,  `round`,  `min`,  `max`,  `sqrt`,  `log`,  `exp`,  `sin`,  `cos`,  `tan`,  `sum`,  `mean`,  `std`,  `var`,  `median`,  `cumsum`,  `cumprod`,  `diff`\n* Safe operators: `+`,  `-`,  `*`,  `/`,  `**`,  `%`,  `(`,  `)`\n\n#### `applyTransformations`\n\n**Description:** Applies general transformations to a dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed general transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed General Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `interpolate` | Fills missing values using interpolation. Requires `parameters.method` (e.g., `linear` , `spline` ). |\n| `fillna` | Fills missing values with a specified strategy. Requires `parameters.method` ( `zero` , `ffill` , `bfill` , `mean` ). |\n| `dropna` | Removes rows with missing values. |\n| `log_transform` | Applies the natural logarithm to stabilize variance. |\n| `exp_transform` | Applies an exponential function. |\n| `sin_transform` | Applies a sine transformation for cyclical data. |\n| `cos_transform` | Applies a cosine transformation for cyclical data. |\n| `tan_transform` | Applies a tangent transformation for cyclical data. |\n| `min_max_scale` | Scales data between a specified range (e.g., `[0,1]` ). Requires `parameters.range` . |\n| `standardize` | Standardizes data by subtracting mean and dividing by standard deviation. |\n| `normalize` | Normalizes data by dividing by its maximum absolute value. |\n| `power_transform` | Applies a power transform (e.g., Box-Cox) for variance stabilization. |\n| `rolling_quantile` | Computes rolling quantiles over a given window. Requires `parameters.window` and `parameters.q` . |\n| `rolling_skew` | Computes rolling skewness over a given window. Requires `parameters.window` . |\n| `rolling_kurtosis` | Computes rolling kurtosis over a given window. Requires `parameters.window` . |\n\n#### `applyTimeSeriesTransformations`\n\n**Description:** Applies a sequence of transformations to a time series dataset.\n\n**Parameters:**\n\n* `table` (*string*): Name of the table.\n* `transformations` (*list[dict]*): A list of transformations to be applied in sequence. Each transformation includes:\n  + `columns` (*list[string]*): Column names involved in the operation.\n  + `operation` (*enum*): One of the allowed time series transformations.\n  + `parameters` (*dict*, *optional*): Additional parameters required for the transformation.\n\n**Behavior:**\n\n* The transformations are applied sequentially in the order they appear in the `transformations` list.\n* If an operation requires a column that does not exist, an **error is raised**.\n* If a transformation fails (e.g., invalid parameters), an **error is raised**.\n* No filtering operations are allowed.\n* The transformed table is stored with the name specified in `output`.\n\n**Allowed Time Series Operations:**\n\n| **Operation**               | **Description** |\n|-----------------------------|------------------------------------------------|\n| `set_index` | Sets the specified column as the index (must be datetime). |\n| `reset_index` | Resets the index and converts it back to a column. |\n| `resample` | Resamples data at a specified frequency (e.g., daily, weekly). Requires `parameters.frequency` . |\n| `rolling_mean` | Computes a rolling mean over a given window size. Requires `parameters.window` . |\n| `rolling_median` | Computes a rolling median over a given window size. Requires `parameters.window` . |\n| `rolling_std` | Computes a rolling standard deviation over a given window size. Requires `parameters.window` . |\n| `rolling_min` | Computes a rolling minimum over a given window size. Requires `parameters.window` . |\n| `rolling_max` | Computes a rolling maximum over a given window size. Requires `parameters.window` . |\n| `expanding_sum` | Computes a cumulative sum over time. |\n| `expanding_mean` | Computes a cumulative mean over time. |\n| `expanding_std` | Computes a cumulative standard deviation over time. |\n| `expanding_min` | Computes a cumulative minimum over time. |\n| `expanding_max` | Computes a cumulative maximum over time. |\n| `lag` | Shifts column values by a given period. Requires `parameters.periods` . |\n| `diff` | Computes the difference over a given period. Requires `parameters.periods` . |\n| `pct_change` | Computes the percentage change between consecutive values. |\n| `cumsum` | Computes the cumulative sum. |\n| `cumprod` | Computes the cumulative product. |\n| `ewm_mean` | Computes an exponentially weighted moving average. Requires `parameters.span` . |\n| `ewm_std` | Computes an exponentially weighted moving standard deviation. Requires `parameters.span` . |\n| `ewm_var` | Computes an exponentially weighted moving variance. Requires `parameters.span` . |\n| `decompose` | Decomposes a time series into trend, seasonality, and residuals. Requires `parameters.period` . |\n| `detect_outliers` | Identifies outliers using a z-score threshold. Requires `parameters.threshold` . |\n| `convert_timezone` | Converts the time column to a specified timezone. Requires `parameters.timezone` . |\n| `extract_datetime_features` | Extracts year, month, day, hour, day of week, week of year, etc., as new columns. |\n| `fourier_transform` | Applies a Fast Fourier Transform (FFT) to extract frequency components. |\n| `inverse_fourier_transform` | Converts frequency components back to the time domain. |\n| `detrend` | Removes linear trends from the time series. |\n| `seasonal_adjustment` | Removes seasonal components from the time series. Requires `parameters.method` ( `multiplicative` or `additive` ). |\n\n#### `forecastData`\n\n**Description:** Performs time-series forecasting.\n\n**Parameters:**\n\n* **`table`** (*string*): Name of the table containing the time series data.\n* **`columns`** (*array of strings*): Columns to forecast (must be numeric).\n* **`forecastParameters`** (*object*): Configuration for the forecasting process, including:\n  + **`forecastPeriod`** (*integer*): Number of future time steps to predict (must be greater than 0).\n  + **`frequency`** (*enum*, optional): Frequency at which data points are forecasted (must be one of the predefined values).\n  + **`seasonal_periods`** (*integer*, required for Holt-Winters): The seasonal period (must be at least 2).\n  + **`n_neighbors`** (*integer*, required for Local Outlier Factor): The number of neighbors (must be at least 1).\n  + **`nu`** (*float*, required for One-Class SVM): The `nu` parameter (must be greater than 0 and lower or equal to 1).\n  + **`dateColumn`** (*string*, optional): Name of the datetime column. If unspecified, the system attempts to infer it only if a single datetime column exists.\n  + **`algorithm`** (*string*, required): Forecasting model to use (must be one of the allowed algorithms).\n  + **`confidenceInterval`** (*float*, optional, default=`95`): Confidence interval percentage for the forecast (must be between 0 and 100).\n* **`output`** (*string*): Name of the table to store forecasting results.\n\n**Behavior:**\n\n* **\ud83d\udd39 Date Column Handling**\n  + If `dateColumn` **is not specified**, the system:\n    - **Infers it** if exactly **one datetime column** exists.\n    - **Raises an error** if multiple or no datetime columns are found.\n  + If `dateColumn` is **specified but missing from the dataset**, an **error is raised**.\n  + If `dateColumn` is **not in ascending order**, the system **automatically sorts it** before forecasting.\n\n* **\ud83d\udd39 Target Column Handling**\n  + `columns` **must be a list of numeric columns** (otherwise, an error is raised).\n  + If any column in `columns` contains **missing values**, they are **interpolated (default: linear interpolation)**.\n  + If interpolation **fails**, an **error is raised**.\n  + If any column in `columns` is **entirely missing**, an **error is raised**.\n\n* **\ud83d\udd39 Forecast Period Validation**\n  + `forecastPeriod` **must be an integer \u2265 1**.\n  + If the dataset has **fewer than `2 \u00d7 forecastPeriod` observations**, an **error is raised**.\n\n* **\ud83d\udd39 Time Series Continuity**\n  + If **gaps exist**, they are **forward-filled**.\n  + If timestamps are **not uniformly spaced**, **resampling** is applied.\n\n* **\ud83d\udd39 Forecast Output**\n  + The `output` table contains:\n    - **For forecasting models (`holt_winters`, `xgboost`)**:\n      - A **`dateColumn` with future timestamps**.\n      - A **`forecast` column** with predicted values.\n      - **Optional**: `lower_bound` and `upper_bound` for confidence intervals.\n    - **For anomaly detection models (`isolation_forest`, `local_outlier_factor`, `one_class_svm`, `gmm`, `z-score`)**:\n      - **Does not generate future timestamps**.\n      - Contains an **`outlier` column** (binary flag for anomaly detection).\n\n**Allowed Forecasting Algorithms:**\n\n| **Algorithm**           | **Parameters**                  | **Description**                                                             |\n| ----------------------- |-------------------------------- |---------------------------------------------------------------------------- |\n| `\"holt_winters\"` | `frequency` , `seasonal_periods` | Holt-Winters Exponential Smoothing, good for trend and seasonality.         |\n| `\"xgboost\"` | `frequency` | Gradient Boosting for time series prediction.                               |\n| `\"isolation_forest\"` |                                 | Detects anomalies using Isolation Forest algorithm.                         |\n| `\"local_outlier_factor\"` | `n_neighbors` | Identifies outliers based on Local Outlier Factor (LOF).                    |\n| `\"one_class_svm\"` | `nu` | Uses One-Class Support Vector Machine (SVM) for anomaly detection.          |\n| `\"gmm\"` |                                 | Gaussian Mixture Model (GMM) for probabilistic clustering-based detection.  |\n| `\"z-score\"` |                                 | Identifies anomalies using Z-score thresholding.                            |\n\n**List of Valid Forecast Frequencies**\n\n| Code   | Description                                     |\n|--------|-------------------------------------------------|\n| `B` | business day frequency                          |\n| `C` | custom business day frequency                   |\n| `D` | calendar day frequency                          |\n| `W` | weekly frequency                                |\n| `ME` | month end frequency                             |\n| `SME` | semi-month end frequency (15th and end of month)|\n| `BME` | business month end frequency                    |\n| `CBME` | custom business month end frequency             |\n| `MS` | month start frequency                           |\n| `SMS` | semi-month start frequency (1st and 15th)       |\n| `BMS` | business month start frequency                  |\n| `CBMS` | custom business month start frequency           |\n| `QE` | quarter end frequency                           |\n| `BQE` | business quarter end frequency                  |\n| `QS` | quarter start frequency                         |\n| `BQS` | business quarter start frequency                |\n| `YE` | year end frequency                              |\n| `BYE` | business year end frequency                     |\n| `YS` | year start frequency                            |\n| `BYS` | business year start frequency                   |\n| `h` | hourly frequency                                |\n| `bh` | business hour frequency                         |\n| `cbh` | custom business hour frequency                  |\n| `min` | minutely frequency                              |\n| `s` | secondly frequency                              |\n| `ms` | milliseconds                                    |\n| `us` | microseconds                                    |\n| `ns` | nanoseconds                                     |\n\n**Examples JSON Input:**\n\n```json\n{\n  \"table\": \"sales_data\",\n  \"columns\": [\"revenue\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 30,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"date\",\n    \"algorithm\": \"xgboost\",\n    \"confidenceInterval\": 95\n  },\n  \"output\": \"forecast_results\"\n}\n```\n\n```json\n{\n  \"table\": \"energy_consumption\",\n  \"columns\": [\"usage\"],\n  \"forecastParameters\": {\n    \"forecastPeriod\": 15,\n    \"frequency\": \"D\",\n    \"dateColumn\": \"timestamp\",\n    \"algorithm\": \"holt_winters\",\n    \"seasonal_periods\": 7,\n    \"confidenceInterval\": 90\n  },\n  \"output\": \"energy_forecast\"\n}\n```"},{"role":"user","content":"\n                ### Input:\n                [\"Load the customer demographics table from the CSV file.\", \"Load the marketing campaign results table from the Excel file.\", \"Filter the marketing campaign results to include only successful campaigns.\", \"Join the filtered marketing campaign results with the customer demographics table on customer ID.\", \"Aggregate the joined table to calculate the total number of successful campaigns per age group.\"]\n\n                ### Context:\n                {\"table_id1\": {\"name\": \"customer_demographics\", \"location\": \"C:/data/customer_demographics.csv\", \"sheet_name\": null, \"label\": \"Customer Demographics\", \"description\": \"Contains demographic information of customers including age, gender, and location.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_2\", \"column_name\": \"age\", \"column_type\": \"xsd:integer\", \"column_description\": \"Age of the customer.\"}, {\"column\": \"col_3\", \"column_name\": \"gender\", \"column_type\": \"xsd:string\", \"column_description\": \"Gender of the customer.\"}]}, \"table_id2\": {\"name\": \"marketing_campaign_results\", \"location\": \"C:/data/marketing_campaign_results.xlsx\", \"sheet_name\": \"Results\", \"label\": \"Marketing Campaign Results\", \"description\": \"Contains results of various marketing campaigns including success status and customer ID.\", \"columns\": [{\"column\": \"col_1\", \"column_name\": \"campaign_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each marketing campaign.\"}, {\"column\": \"col_2\", \"column_name\": \"customer_id\", \"column_type\": \"xsd:string\", \"column_description\": \"Unique identifier for each customer.\"}, {\"column\": \"col_3\", \"column_name\": \"success\", \"column_type\": \"xsd:boolean\", \"column_description\": \"Indicates whether the campaign was successful.\"}]}, \"table_id3\": {\"name\": \"irrelevant_table\", \"location\": \"C:/data/irrelevant_data.xlsx\", \"sheet_name\": \"Data\", \"label\": \"Irrelevant Data\", \"description\": \"Contains data not related to the marketing workflow.\", \"columns\": [{\"column\": \"col_A\", \"column_name\": \"data_point\", \"column_type\": \"xsd:string\", \"column_description\": \"Irrelevant data point.\"}]}}\n\n                ### Response:\n                "},{"role":"assistant","content":"[{\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/customer_demographics.csv\", \"type\": \"csv\"}, \"output\": \"customer_demographics\", \"comment\": \"Loading customer demographics data from a CSV file.\"}, {\"action\": \"identifyTable\", \"parameters\": {\"location\": \"C:/data/marketing_campaign_results.xlsx\", \"type\": \"xlsx\"}, \"output\": \"marketing_campaign_results\", \"comment\": \"Loading marketing campaign results from an Excel file.\"}, {\"action\": \"filterData\", \"parameters\": {\"table\": \"marketing_campaign_results\", \"query\": \"success == True\"}, \"output\": \"successful_campaigns\", \"comment\": \"Filtering marketing campaign results to include only successful campaigns.\"}, {\"action\": \"joinTables\", \"parameters\": {\"tables\": [\"successful_campaigns\", \"customer_demographics\"], \"joinOn\": [\"customer_id\"], \"joinType\": \"inner\"}, \"output\": \"joined_data\", \"comment\": \"Joining successful campaigns with customer demographics on customer ID.\"}, {\"action\": \"aggregateData\", \"parameters\": {\"table\": \"joined_data\", \"groupBy\": \"age\", \"aggregations\": [{\"column\": \"campaign_id\", \"name\": \"total_successful_campaigns\", \"function\": \"count\"}]}, \"output\": \"age_group_campaigns\", \"comment\": \"Aggregating data to calculate the total number of successful campaigns per age group.\"}]"}]}
